<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US"><head profile="http://gmpg.org/xfn/11">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="apple-touch-icon" sizes="144x144" href="https://insidebigdata.com/wp-content/themes/news-bigdata/images/apple-touch-icon-144x144.png">
<!-- For iPhone with high-resolution Retina display running iOS ≥ 7: -->
<link rel="apple-touch-icon" sizes="120x120" href="https://insidebigdata.com/wp-content/themes/news-bigdata/images/apple-touch-icon-120x120.png">
<!-- For iPhone with high-resolution Retina display running iOS ≤ 6: -->
<link rel="apple-touch-icon" sizes="114x114" href="https://insidebigdata.com/wp-content/themes/news-bigdata/images/apple-touch-icon-114x114.png">
<!-- For first- and second-generation iPad: -->
<link rel="apple-touch-icon" sizes="72x72" href="https://insidebigdata.com/wp-content/themes/news-bigdata/images/apple-touch-icon-72x72.png">
<link rel="apple-touch-icon" sizes="57x57" href="https://insidebigdata.com/wp-content/themes/news-bigdata/images/apple-touch-icon-57x57.png">
<!-- For non-Retina iPhone, iPod Touch, and Android 2.1+ devices: -->
<link rel="apple-touch-icon" href="https://insidebigdata.com/wp-content/themes/news-bigdata/images/apple-touch-icon.png">
<link rel="shortcut icon" href="https://insidebigdata.com/wp-content/themes/news-bigdata/images/favicon.png" type="image/x-icon"> <meta name="viewport" content="width=device-width, initial-scale=1.0">


        
	<!-- This site is optimized with the Yoast SEO plugin v15.4 - https://yoast.com/wordpress/plugins/seo/ -->
	<title>Best of arXiv.org for AI, Machine Learning, and Deep Learning – April 2018 - insideBIGDATA</title>
	<meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
	<link rel="canonical" href="https://insidebigdata.com/2018/05/07/best-arxiv-org-ai-machine-learning-deep-learning-april-2018/">
	<meta property="og:locale" content="en_US">
	<meta property="og:type" content="article">
	<meta property="og:title" content="Best of arXiv.org for AI, Machine Learning, and Deep Learning – April 2018 - insideBIGDATA">
	<meta property="og:description" content="In this recurring monthly feature, we will filter all the recent research papers appearing in the arXiv.org preprint server for subjects relating to AI, machine learning and deep learning – from disciplines including statistics, mathematics and computer science – and provide you with a useful “best of” list for the month.">
	<meta property="og:url" content="https://insidebigdata.com/2018/05/07/best-arxiv-org-ai-machine-learning-deep-learning-april-2018/">
	<meta property="og:site_name" content="insideBIGDATA">
	<meta property="article:publisher" content="http://www.facebook.com/insidebigdata">
	<meta property="article:published_time" content="2018-05-07T15:30:20+00:00">
	<meta property="article:modified_time" content="2018-05-08T15:40:08+00:00">
	<meta property="og:image" content="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg">
	<meta property="og:image:width" content="450">
	<meta property="og:image:height" content="380">
	<meta name="twitter:card" content="summary">
	<meta name="twitter:creator" content="@AMULETAnalytics">
	<meta name="twitter:site" content="@insideBigData">
	<meta name="twitter:label1" content="Written by">
	<meta name="twitter:data1" content="Daniel Gutierrez">
	<meta name="twitter:label2" content="Est. reading time">
	<meta name="twitter:data2" content="3 minutes">
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel="stylesheet" id="news-theme-for-inside-big-data-css" href="https://insidebigdata.com/wp-content/themes/news-bigdata/style.css?b-modified=1614726832&amp;ver=2.7.3" type="text/css" media="all">
<link rel="stylesheet" id="wp-block-library-css" href="https://insidebigdata.com/wp-includes/css/dist/block-library/style.min.css?ver=5.6.2" type="text/css" media="all">
<style id="wp-block-library-inline-css" type="text/css">
.has-text-align-justify{text-align:justify;}
</style>
<link rel="stylesheet" id="tm_clicktotweet-css" href="https://insidebigdata.com/wp-content/plugins/click-to-tweet-by-todaymade/assets/css/styles.css?ver=5.6.2" type="text/css" media="all">
<link rel="stylesheet" id="contact-form-7-css" href="https://insidebigdata.com/wp-content/plugins/contact-form-7/includes/css/styles.css?ver=5.3" type="text/css" media="all">
<link rel="stylesheet" id="wpdm-font-awesome-css" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css?ver=5.6.2" type="text/css" media="all">
<link rel="stylesheet" id="wpdm-bootstrap-css" href="https://insidebigdata.com/wp-content/plugins/download-manager/assets/bootstrap/css/bootstrap.min.css?ver=5.6.2" type="text/css" media="all">
<link rel="stylesheet" id="wpdm-front-css" href="https://insidebigdata.com/wp-content/plugins/download-manager/assets/css/front.css?ver=5.6.2" type="text/css" media="all">
<link rel="stylesheet" id="social_icons_style-css" href="https://insidebigdata.com/wp-content/themes/news-bigdata/css/social-style.css?ver=5.6.2" type="text/css" media="all">
<link rel="stylesheet" id="social_icons_app-css" href="https://insidebigdata.com/wp-content/themes/news-bigdata/css/social-app.css?ver=5.6.2" type="text/css" media="all">
<link rel="stylesheet" id="__EPYT__style-css" href="https://insidebigdata.com/wp-content/plugins/youtube-embed-plus/styles/ytprefs.min.css?ver=13.4.1.2" type="text/css" media="all">
<style id="__EPYT__style-inline-css" type="text/css">

                .epyt-gallery-thumb {
                        width: 33.333%;
                }
                
</style>
<link rel="stylesheet" id="jetpack_css-css" href="https://insidebigdata.com/wp-content/plugins/jetpack/css/jetpack.css?ver=9.2" type="text/css" media="all">
<link rel="stylesheet" id="simplr-forms-style-css" href="https://insidebigdata.com/wp-content/plugins/simplr-registration-form/assets/skins/default.css?ver=5.6.2" type="text/css" media="all">
<!--[if lt IE 9]>
	<link rel="stylesheet" href="https://insidebigdata.com/wp-content/plugins/google-mp3-audio-player/style.css" />
	
	<style type="text/css">
	.codeart-google-mp3-player .download-link{
		display:block;
		padding: 0 5px 0 5px;
		float: left;
	}
	.codeart-google-mp3-player embed{
		float: left;
	}
	.codeart-google-mp3-player{
		overflow: hidden;
	}
	.codeart-google-mp3-player object{
		float: left;
	}
	</style>
	<!--[if gte IE 9]>
		<style type="text/css">
			.gradient {
				filter: none;
			}
		</style>
	<![endif]-->

<!-- Twitter Cards Meta - V 2.5.4 -->
<meta name="twitter:card" content="photo">
<meta name="twitter:site" content="@insidebigdata">
<meta name="twitter:creator" content="@insidebigdata">
<meta name="twitter:url" content="https://insidebigdata.com/2018/05/07/best-arxiv-org-ai-machine-learning-deep-learning-april-2018/">
<meta name="twitter:title" content="Best of arXiv.org for AI, Machine Learning, and Deep Learning – April 2018">
<meta name="twitter:description" content="In this recurring monthly feature, we will filter all the recent research papers appearing in the arXiv.org preprint server for subjects relating to AI, [...]">
<meta name="twitter:image" content="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg">
<!-- Twitter Cards Meta By WPDeveloper.net -->

<link rel="icon" href="https://insidebigdata.com/wp-content/themes/news-bigdata/images/favicon.ico">
<!-- Global site tag (gtag.js) - Google Analytics -->





		</head><body><div class="post-14538 post type-post status-publish format-standard hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-education category-uncategorized tag-artificial-intelligence tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – November 2017</h1>
<div class="post-info"><span class="date published time" title="2017-11-27T08:30:28-08:00">November 27, 2017</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2017/11/27/best-of-arxiv-for-machine-learning-february-2016/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1711.08141" target="_blank" rel="noopener">Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions</a></p>
<p>Neural networks rely on convolutions to aggregate spatial information. However, spatial convolutions are expensive in terms of model size and computation, both of which grow quadratically with respect to kernel size. This paper presents a parameter-free, FLOP-free “shift” operation as an alternative to spatial convolutions to increase accuracy by up to 8% with the same FLOPs &amp; recovers accuracy with 1/3 of FLOPs for ResNet’s.</p><!--<div style="float:left; width:310px; height:250px;">	-->

<div id="desktop-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle -->
<div id="div-gpt-ad-1439400881943-0" style="height:250px; width:300px;">
</div>
</div>

<div id="mobile-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle 
<div id='div-gpt-ad-1439400881943-0' style='height:250px; width:300px;'>
</div>-->
</div>

<!--</div>-->
<p><a href="https://arxiv.org/abs/1711.06788" target="_blank" rel="noopener">MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks</a></p>
<p>This article introduces MinimalRNN, a new recurrent neural network architecture that achieves comparable performance as the popular gated RNNs with a simplified structure. It employs minimal updates within RNN, which not only leads to efficient learning and testing but more importantly better interpretability and trainability, plus captures longer range dependencies than existing RNN.</p>
<p><a href="https://arxiv.org/abs/1711.07607" target="_blank" rel="noopener">Knowledge Concentration: Learning 100K Object Classifiers in a Single CNN</a></p>
<p>To address the challenges of many computer vision applications, this article proposes a Knowledge Concentration method, which effectively transfers the knowledge from dozens of specialists (multiple teacher networks) into one single model (one student network) to classify 100K object categories.</p>
<p><a href="https://arxiv.org/abs/1711.07618" target="_blank" rel="noopener">S4Net: Single Stage Salient-Instance Segmentation</a></p>
<p>This paper considers an interesting vision problem—salient instance segmentation. Other than producing approximate bounding boxes, the described network also outputs high-quality instance-level segments. Taking into account the category-independent property of each target, the authors design a single stage salient instance segmentation framework, with a novel segmentation branch. Their new branch regards not only local context inside each detection window but also its surrounding context, enabling us to distinguish the instances in the same scope even with obstruction.</p>
<p><a href="https://arxiv.org/abs/1711.06929" target="_blank" rel="noopener">Deep Gaussian Mixture Models</a></p>
<p>Deep learning is a hierarchical inference method formed by subsequent multiple layers of learning able to more efficiently describe complex relationships. In this paper, Deep Gaussian Mixture Models are introduced and discussed. A Deep Gaussian Mixture model (DGMM) is a network of multiple layers of latent variables, where, at each layer, the variables follow a mixture of Gaussian distributions.</p>
<p><a href="https://arxiv.org/abs/1711.06178" target="_blank" rel="noopener"> Beyond Sparsity: Tree Regularization of Deep Models for Interpretability</a></p>
<p>The lack of interpretability remains a key barrier to the adoption of deep models in many applications. In this paper, the authors show how to explicitly regularize deep models so human users might step through the process behind their predictions in little time. Specifically, they train deep time-series models so their class-probability predictions have high accuracy while being closely modeled by decision trees with few nodes.</p>
<p><a href="https://arxiv.org/abs/1711.04325" target="_blank" rel="noopener">Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes</a></p>
<p>This article demonstrates that training ResNet-50 on ImageNet for 90 epochs can be achieved in 15 minutes with 1024 Tesla P100 GPUs. This was made possible by using a large minibatch size of 32k. To maintain accuracy with this large minibatch size, the authors employed several techniques such as RMSprop warm-up, batch normalization without moving averages, and a slow-start learning rate schedule.</p>
<p><a href="https://arxiv.org/abs/1711.04340" target="_blank" rel="noopener">Data Augmentation Generative Adversarial Networks</a></p>
<p>This paper shows that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. It also shows a DAGAN can enhance few-shot learning systems such as Matching Networks.</p>
<p><a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener">Fixing Weight Decay Regularization in Adam</a></p>
<p>This paper notes that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. The authors propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function.</p>
<p><a href="https://arxiv.org/abs/1711.00489" target="_blank" rel="noopener">Don’t Decay the Learning Rate, Increase the Batch Size</a></p>
<p>It is common practice to decay the learning rate. This paper shows how one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam.</p>
<p>&nbsp;</p>
<p><em>Sign up for the free insideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
<p>&nbsp;</p>
</div><p></p></div><!-- end .after-post-ad --></div><div class="post-19716 post type-post status-publish format-standard hentry category-ai-deep-learning category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-artificial-intelligence tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – December 2017</h1>
<div class="post-info"><span class="date published time" title="2018-01-08T08:30:45-08:00">January 8, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/01/08/best-arxiv-org-ai-machine-learning-deep-learning-december-2017/#comments">1 Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1712.00714" target="_blank" rel="noopener">Spatial PixelCNN: Generating Images from Patches</a></p>
<p>This is a very cool paper in the computer vision space that proposes <em>Spatial PixelCNN</em>, a conditional autoregressive model that generates images from small patches. By conditioning on a grid of pixel coordinates and global features extracted from a Variational Autoencoder (VAE), they’re able to train on patches of images, and reproduce the full-sized image. They show that the technique not only allows for generating high quality samples at the same resolution as the underlying data set, but is also capable of up-scaling images to arbitrary resolutions (tested at resolutions up to <span id="MathJax-Element-1-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-1" class="math"><span id="MathJax-Span-2" class="mrow"><span id="MathJax-Span-3" class="mn">50</span><span id="MathJax-Span-4" class="mo">×</span></span></span></span>) on the MNIST dataset. Compared to a PixelCNN++ baseline, Spatial PixelCNN quantitatively and qualitatively achieves similar performance on the MNIST data set.</p><!--<div style="float:left; width:310px; height:250px;">	-->

<div id="desktop-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle -->
<div id="div-gpt-ad-1439400881943-0" style="height:250px; width:300px;">
</div>
</div>

<div id="mobile-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle 
<div id='div-gpt-ad-1439400881943-0' style='height:250px; width:300px;'>
</div>-->
</div>

<!--</div>-->
<p><a href="https://arxiv.org/abs/1712.09665" target="_blank" rel="noopener">Adversarial Patch</a></p>
<p>A group of Google researchers presents a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class. These adversarial patches can be printed, added to any scene, photographed, and presented to image classifiers; even when the patches are small, they cause the classifiers to ignore the other items in the scene and report a chosen target class.</p>
<p><iframe id="_ytid_57033" width="430" height="323" data-origwidth="430" data-origheight="323" src="https://www.youtube.com/embed/i1sp4X57TL4?enablejsapi=1&amp;autoplay=0&amp;cc_load_policy=0&amp;cc_lang_pref=&amp;iv_load_policy=1&amp;loop=0&amp;modestbranding=0&amp;rel=1&amp;fs=1&amp;playsinline=0&amp;autohide=2&amp;theme=dark&amp;color=red&amp;controls=1&amp;" class="__youtube_prefs__  epyt-is-override  no-lazyload" title="YouTube player" allow="autoplay; encrypted-media" allowfullscreen="" data-no-lazy="1" data-skipgform_ajax_framebjll=""></iframe></p>
<p><a href="https://arxiv.org/abs/1712.09913" target="_blank" rel="noopener">Visualizing the Loss Landscape of Neural Nets</a></p>
<p>Neural network training relies on our ability to find “good” minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. This paper explores the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods.</p>
<p><a href="https://arxiv.org/abs/1712.09381" target="_blank" rel="noopener">Ray RLLib: A Composable and Scalable Reinforcement Learning Library</a></p>
<p>Reinforcement learning (RL) algorithms involve the deep nesting of distinct components, where each component typically exhibits opportunities for distributed computation. Current RL libraries offer parallelism at the level of the entire program, coupling all the components together and making existing implementations difficult to extend, combine, and reuse. This paper argues for building composable RL components by encapsulating parallelism and resource requirements within individual components, which can be achieved by building on top of a flexible task-based programming model. The authors demonstrate this principle by building Ray RLLib on top of Ray and show how to implement a wide range of state-of-the-art algorithms by composing and reusing a handful of standard components. Ray RLLib is available as part of Ray on <a href="https://github.com/ray-project/ray/" target="_blank" rel="noopener">GitHub</a>.</p>
<p><a href="https://arxiv.org/abs/1712.05577" target="_blank" rel="noopener">Gradients explode – Deep Networks are shallow – ResNet explained</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities “solve” the exploding gradient problem, this paper shows that this is not the case in general and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. The authors explain why exploding gradients occur and highlight the <em>collapsing domain problem</em>, which can arise in architectures that avoid exploding gradients. ResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which they show is a consequence of a surprising mathematical property. By noticing that any neural network is a residual network, this new research devises the <em>residual trick</em>, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.</p>
<p><a href="https://arxiv.org/abs/1711.09081" target="_blank" rel="noopener">Deep Extreme Cut: From Extreme Points to Object Segmentation</a></p>
<p>This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. The authors do so by adding an extra channel to the image in the input of a convolutional neural network (CNN), which contains a Gaussian centered in each of the extreme points. The CNN learns to transform this information into a segmentation of an object that matches those extreme points. The paper demonstrates the usefulness of this approach for guided segmentation (grabcut-style), interactive segmentation, video object segmentation, and dense segmentation annotation.</p>
<p><a href="https://arxiv.org/abs/1705.09558" target="_blank" rel="noopener">Bayesian GAN</a></p>
<p>Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. This paper presents a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, the authors use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching, or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.</p>
<p><a href="https://arxiv.org/abs/1712.07788" target="_blank" rel="noopener">Deep Unsupervised Clustering Using Mixture of Autoencoders</a></p>
<p>Unsupervised clustering is one of the most fundamental challenges in machine learning. A popular hypothesis is that data are generated from a union of low-dimensional nonlinear manifolds; thus an approach to clustering is identifying and separating these manifolds. This paper presents a novel approach to solve this problem by using a mixture of autoencoders. The model consists of two parts: 1) a collection of autoencoders where each autoencoder learns the underlying manifold of a group of similar objects, and 2) a mixture assignment neural network, which takes the concatenated latent vectors from the autoencoders as input and infers the distribution over clusters. By jointly optimizing the two parts, the authors simultaneously assign data to clusters and learn the underlying manifolds of each cluster.</p>
<p><a href="https://arxiv.org/abs/1712.07897" target="_blank" rel="noopener">Non-convex Optimization for Machine Learning</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">A vast majority of machine learning algorithms train their models and perform inference by solving optimization problems. In order to capture the learning and prediction problems accurately, structural constraints such as sparsity or low rank are frequently imposed or else the objective itself is designed to be a non-convex function. This is especially true of algorithms that operate in high-dimensional spaces or that train non-linear models such as tensor models and deep networks. This paper leads the reader through several widely used non-convex optimization techniques, as well as applications thereof. The goal is to both, introduce the rich literature in this area, as well as equip the reader with the tools and techniques needed to analyze these simple procedures for non-convex problems.</p>
<p><a href="https://arxiv.org/abs/1712.07628" target="_blank" rel="noopener">Improving Generalization Performance by Switching from Adam to SGD</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">Despite superior training outcomes, adaptive optimization methods such as Adam, Adagrad or RMSprop have been found to generalize poorly compared to Stochastic gradient descent (SGD). These methods tend to perform well in the initial portion of training but are outperformed by SGD at later stages of training. This paper investigates a hybrid strategy that begins training with an adaptive method and switches to SGD when appropriate. Concretely, the authors propose SWATS, a simple strategy which switches from Adam to SGD when a triggering condition is satisfied. The condition proposed relates to the projection of Adam steps on the gradient subspace. By design, the monitoring process for this condition adds very little overhead and does not increase the number of hyperparameters in the optimizer.</p>
<p>&nbsp;</p>
<p><em>Sign up for the free insideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</div><p></p></div><!-- end .after-post-ad --></div><div class="post-19891 post type-post status-publish format-standard hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis tag-artificial-intelligence tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – January 2018</h1>
<div class="post-info"><span class="date published time" title="2018-02-09T08:30:57-08:00">February 9, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/02/09/best-arxiv-org-ai-machine-learning-deep-learning-january-2018/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1802.00027" target="_blank" rel="noopener">A New Backpropagation Algorithm without Gradient Descent</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">We’ve all been taught that the backpropagation algorithm, which had been originally introduced in the 1970s, is the workhorse of learning in neural networks. This backpropagation algorithm makes use of the famous machine learning algorithm known as Gradient Descent, which is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. In this paper, PES University researcher develop an alternative to the backpropagation without the use of the Gradient Descent Algorithm, but instead they devise a new algorithm to find the error in the weights and biases of an artificial neuron using Moore-Penrose Pseudo Inverse. The numerical studies and the experiments performed on various data sets are used to verify the working of this alternative algorithm.</p><!--<div style="float:left; width:310px; height:250px;">	-->

<div id="desktop-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle -->
<div id="div-gpt-ad-1439400881943-0" style="height:250px; width:300px;">
</div>
</div>

<div id="mobile-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle 
<div id='div-gpt-ad-1439400881943-0' style='height:250px; width:300px;'>
</div>-->
</div>

<!--</div>-->
<p><a href="https://arxiv.org/abs/1801.10247" target="_blank" rel="noopener">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</a></p>
<p>This paper comes to us from researchers at IBM Research. The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, the authors interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work—FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. The paper includes a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.</p>
<p><a href="https://arxiv.org/abs/1711.03953" target="_blank" rel="noopener">Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</a></p>
<p>Researchers from the CMU Computer Science department formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. The authors propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.</p>
<p><a href="https://arxiv.org/abs/1708.06742" target="_blank" rel="noopener">Twin Networks: Matching the Future for Sequence Generation</a></p>
<p>A group of researchers including Yoshua Bengio propose a simple technique for encouraging generative RNNs to plan ahead. They train a “backward” recurrent network to generate a given sequence in reverse order, and they encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. The authors hypothesize that the approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). The paper shows empirically that the approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.</p>
<p><a href="https://arxiv.org/abs/1801.07883" target="_blank" rel="noopener">Deep Learning for Sentiment Analysis : A Survey</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many other application domains, deep learning is also popularly used in sentiment analysis in recent years. This paper first gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.</p>
<p><a href="https://arxiv.org/abs/1801.05894" target="_blank" rel="noopener">Deep Learning: An Introduction for Applied Mathematicians</a></p>
<p>Multilayered artificial neural networks are becoming a pervasive tool in a host of application fields. At the heart of this deep learning revolution are familiar concepts from applied and computational mathematics; notably, in calculus, approximation theory, optimization and linear algebra. This article provides a very brief introduction to the basic ideas that underlie deep learning from an applied mathematics perspective.</p>
<p><a href="https://arxiv.org/abs/1711.11561" target="_blank" rel="noopener">Measuring the tendency of CNNs to Learn Surface Statistical Regularities</a></p>
<p>Deep CNNs are known to exhibit the following peculiarity: on the one hand they generalize extremely well to a test set, while on the other hand they are extremely sensitive to so-called adversarial perturbations. The extreme sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are learning high level abstractions in the data set. This paper is concerned with the following question: How can a deep CNN that does not learn any high level semantics of the data set manage to generalize so well? The goal of this article is to measure the tendency of CNNs to learn surface statistical regularities of the data set.</p>
<p><a href="https://arxiv.org/abs/1702.07800" target="_blank" rel="noopener">On the Origin of Deep Learning</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning.</p>
<p><a href="https://arxiv.org/abs/1801.01078" target="_blank" rel="noopener">Recent Advances in Recurrent Neural Networks</a></p>
<p>Recurrent neural networks (RNNs) are capable of learning features and long term dependencies from sequential and time-series data. The RNNs have a stack of non-linear units where at least one connection between units forms a directed cycle. A well-trained RNN can model any dynamical system; however, training RNNs is mostly plagued by issues in learning long-term dependencies. This paper presents a survey on RNNs and several new advances for newcomers and professionals in the field. The fundamentals and recent advances are explained and the research challenges are introduced.</p>
<p><a href="https://arxiv.org/abs/1801.00631" target="_blank" rel="noopener">Deep Learning: A Critical Appraisal</a></p>
<p>Although deep learning has historical roots going back decades, neither the term “deep learning” nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton’s now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, AI contrarian Gary Marcus presents ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.</p>
<p>&nbsp;</p>
<p><em>Sign up for the free insideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</div><p></p></div><!-- end .after-post-ad --></div><div class="post-20047 post type-post status-publish format-standard hentry category-ai-deep-learning category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-artificial-intelligence tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – February 2018</h1>
<div class="post-info"><span class="date published time" title="2018-03-16T08:30:37-07:00">March 16, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/03/16/best-arxiv-org-ai-machine-learning-deep-learning-february-2018/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1802.01528" target="_blank" rel="noopener">The Matrix Calculus You Need For Deep Learning</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">This paper is a wonderful resource that explains all the linear algebra you need in order to understand the training of deep neural networks. It assumes little math knowledge beyond what you learned in freshman calculus, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math.</p><!--<div style="float:left; width:310px; height:250px;">	-->

<div id="desktop-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle -->
<div id="div-gpt-ad-1439400881943-0" style="height:250px; width:300px;">
</div>
</div>

<div id="mobile-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle 
<div id='div-gpt-ad-1439400881943-0' style='height:250px; width:300px;'>
</div>-->
</div>

<!--</div>-->
<p><a href="https://arxiv.org/abs/1802.08760" target="_blank" rel="noopener">Sensitivity and Generalization in Neural Networks: an Empirical Study</a></p>
<p>In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. This paper investigates this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. The experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets.</p>
<p><a href="https://arxiv.org/abs/1802.06509" target="_blank" rel="noopener">On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization</a></p>
<p>Conventional wisdom in deep learning states that increasing depth improves expressiveness but complicates optimization. This paper suggests that, sometimes, increasing depth can speed up optimization. The effect of depth on optimization is decoupled from expressiveness by focusing on settings where additional layers amount to overparameterization – linear neural networks, a well-studied model. Theoretical analysis, as well as experiments, show that here depth acts as a preconditioner which may accelerate convergence. Even on simple convex problems such as linear regression with <span id="MathJax-Element-1-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-1" class="math"><span id="MathJax-Span-2" class="mrow"><span id="MathJax-Span-3" class="msubsup"><span id="MathJax-Span-4" class="mi">ℓ</span><span id="MathJax-Span-5" class="mi">p</span></span></span></span></span> loss, <span id="MathJax-Element-2-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-6" class="math"><span id="MathJax-Span-7" class="mrow"><span id="MathJax-Span-8" class="mi">p</span><span id="MathJax-Span-9" class="mo">&gt;</span><span id="MathJax-Span-10" class="mn">2</span></span></span></span>, gradient descent can benefit from transitioning to a non-convex overparameterized objective, more than it would from some common acceleration schemes. The paper also shows that it is mathematically impossible to obtain the acceleration effect of overparametrization via gradients of any regularizer.</p>
<p><a href="https://arxiv.org/abs/1802.05296" target="_blank" rel="noopener">Stronger generalization bounds for deep nets via a compression approach</a></p>
<p>Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. This paper shows generalization bounds that are orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net — a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. The results also provide some theoretical justification for widespread empirical success in compressing deep nets.</p>
<p><a href="https://arxiv.org/abs/1802.02226" target="_blank" rel="noopener">Generative Adversarial Networks using Adaptive Convolution</a></p>
<p>Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator. This paper argues that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances. The paper proposes a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem. The authors modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator. Experiments on CIFAR-10 dataset show that the modified models improve the baseline model by a large margin. Furthermore, the models achieve state-of-the-art performance on CIFAR-10 and STL-10 data sets in the unsupervised setting.</p>
<p><a href="https://arxiv.org/abs/1801.07729" target="_blank" rel="noopener">The Shape of Art History in the Eyes of the Machine</a></p>
<p>How does the machine classify styles in art? And how does it relate to art historians’ methods for analyzing style? Several studies have shown the ability of the machine to learn and predict style categories, such as Renaissance, Baroque, Impressionism, etc., from images of paintings. This implies that the machine can learn an internal representation encoding discriminative features through its visual analysis. However, such a representation is not necessarily interpretable. The authors of this paper conducted a comprehensive study of several of the state-of-the-art convolutional neural networks applied to the task of style classification on 77K images of paintings, and analyzed the learned representation through correlation analysis with concepts derived from art history. Surprisingly, the networks could place the works of art in a smooth temporal arrangement mainly based on learning style labels, without any a priori knowledge of time of creation, the historical time and context of styles, or relations between styles.</p>
<p><a href="https://arxiv.org/abs/1802.02871" target="_blank" rel="noopener">Online Learning: A Comprehensive Survey</a></p>
<p>Online learning represents an important family of machine learning algorithms, in which a learner attempts to resolve an online prediction (or any type of decision-making) task by learning a model/hypothesis from a sequence of data instances one at a time. The goal of online learning is to ensure that the online learner would make a sequence of accurate predictions (or correct decisions) given the knowledge of correct answers to previous prediction or learning tasks and possibly additional information. This is in contrast to many traditional batch learning or offline machine learning algorithms that are often designed to train a model in batch from a given collection of training data instances. This survey aims to provide a comprehensive survey of the online machine learning literatures through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques.</p>
<p><a href="https://arxiv.org/abs/1711.00811" target="_blank" rel="noopener">Expressive power of recurrent neural networks</a></p>
<p>Deep neural networks are surprisingly efficient at solving practical tasks, but the theory behind this phenomenon is only starting to catch up with the practice. Numerous works show that depth is the key to this efficiency. A certain class of deep convolutional networks — namely those that correspond to the Hierarchical Tucker (HT) tensor decomposition — has been proven to have exponentially higher expressive power than shallow networks. I.e. a shallow network of exponential width is required to realize the same score function as computed by the deep architecture. This paper proves the expressive power theorem (an exponential lower bound on the width of the equivalent shallow network) for a class of recurrent neural networks — ones that correspond to the Tensor Train (TT) decomposition. This means that even processing an image patch by patch with an RNN can be exponentially more efficient than a (shallow) convolutional network with one hidden layer.</p>
<p>&nbsp;</p>
<p><em>Sign up for the free insideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div><p></p></div><!-- end .after-post-ad --></div><div class="post-20228 post type-post status-publish format-standard hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-artificial-intelligence tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – March 2018</h1>
<div class="post-info"><span class="date published time" title="2018-04-13T08:30:48-07:00">April 13, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/editorial/" rel="author">Editorial Team</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/04/13/best-arxiv-org-ai-machine-learning-deep-learning-march-2018/#comments">1 Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1803.10750" target="_blank" rel="noopener">Adversarial Network Compression</a></p>
<p>Neural network compression has recently received much attention due to the computational requirements of modern deep models. In this paper, the objective is to transfer knowledge from a deep and accurate model to a smaller one. The research’s contributions are threefold: (i) propose an adversarial network compression approach to train the small student network to mimic the large teacher, without the need for labels during training; (ii) introduce a regularization scheme to prevent a trivially-strong discriminator without reducing the network capacity and (iii) the approach generalizes on different teacher-student models.</p><!--<div style="float:left; width:310px; height:250px;">	-->

<div id="desktop-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle -->
<div id="div-gpt-ad-1439400881943-0" style="height:250px; width:300px;">
</div>
</div>

<div id="mobile-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle 
<div id='div-gpt-ad-1439400881943-0' style='height:250px; width:300px;'>
</div>-->
</div>

<!--</div>-->
<p><a href="https://arxiv.org/abs/1804.00222" target="_blank" rel="noopener">Learning Unsupervised Learning Rules</a></p>
<p>A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this goal is approached by minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. This paper proposes instead to directly target a later desired task by meta-learning an unsupervised learning rule, which leads to representations useful for that task.</p>
<p><a href="https://arxiv.org/abs/1803.09473" target="_blank" rel="noopener">code2vec: Learning Distributed Representations of Code</a></p>
<p>This paper presents a neural model for representing snippets of code as continuous distributed vectors. The main idea is to represent code as a collection of paths in its abstract syntax tree, and aggregate these paths, in a smart and scalable way, into a single fixed-length <span id="MathJax-Element-1-Frame" class="MathJax" tabindex="0"><span id="MathJax-Span-1" class="math"><span id="MathJax-Span-2" class="mrow"><span id="MathJax-Span-3" class="texatom"><span id="MathJax-Span-4" class="mrow"><span id="MathJax-Span-5" class="mtext">code vector</span></span></span></span></span></span>, which can be used to predict semantic properties of the snippet.</p>
<p><a href="https://arxiv.org/abs/1803.08494" target="_blank" rel="noopener">Group Normalization</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems — BN’s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN’s usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. This paper, by Facebook AI Researchers (FAIR), presents <em>Group Normalization (GN)</em> as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN’s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes.</p>
<p><a href="https://arxiv.org/abs/1703.06211" target="_blank" rel="noopener">Deformable Convolutional Networks</a></p>
<p>Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. This paper introduces two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The code can be found <a href="https://github.com/felixlaumon/deform-conv" target="_blank" rel="noopener">HERE</a>.</p>
<p><a href="https://arxiv.org/abs/1803.06199" target="_blank" rel="noopener">Complex-YOLO: Real-time 3D Object Detection on Point Clouds</a></p>
<p>Lidar based 3D object detection is inevitable for autonomous driving, because it directly links to environmental understanding and therefore builds the base for prediction and motion planning. The capacity of inferencing highly sparse 3D data in real-time is an ill-posed problem for lots of other application areas besides automated vehicles, e.g. augmented reality, personal robotics or industrial automation. This paper introduces Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. The work described is a network that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space.</p>
<p><a href="https://arxiv.org/abs/1803.05407" target="_blank" rel="noopener">Averaging Weights Leads to Wider Optima and Better Generalization</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. This paper shows that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. The paper also shows that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model.</p>
<p><a href="https://arxiv.org/abs/1803.02323" target="_blank" rel="noopener">Deep Super Learner: A Deep Ensemble for Classification Problems</a></p>
<p>Deep learning has become very popular for tasks such as predictive modeling and pattern recognition in handling big data. Deep learning is a powerful machine learning method that extracts lower level features and feeds them forward for the next layer to identify higher level features that improve performance. However, deep neural networks have drawbacks, which include many hyper-parameters and infinite architectures, opaqueness into results, and relatively slower convergence on smaller data sets. While traditional machine learning algorithms can address these drawbacks, they are not typically capable of the performance levels achieved by deep neural networks. To improve performance, ensemble methods are used to combine multiple base learners. Super learning is an ensemble that finds the optimal combination of diverse learning algorithms. This paper proposes deep super learning as an approach which achieves log loss and accuracy results competitive to deep neural networks while employing traditional machine learning algorithms in a hierarchical structure.</p>
<p><a href="https://arxiv.org/abs/1803.00684" target="_blank" rel="noopener">Autostacker: A Compositional Evolutionary Learning System</a></p>
<p>This paper introduces an automatic machine learning (AutoML) modeling architecture called Autostacker, which combines an innovative hierarchical stacking architecture and an Evolutionary Algorithm (EA) to perform efficient parameter search. Neither prior domain knowledge about the data nor feature preprocessing is needed. Using EA, Autostacker quickly evolves candidate pipelines with high predictive accuracy. These pipelines can be used as is or as a starting point for human experts to build on.</p>
<p>&nbsp;</p>
<p><em>Sign up for the free insideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div><p></p></div><!-- end .after-post-ad --></div><div class="post-20335 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-uncategorized tag-artificial-intelligence tag-data-science tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – April 2018</h1>
<div class="post-info"><span class="date published time" title="2018-05-07T08:30:20-07:00">May 7, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/05/07/best-arxiv-org-ai-machine-learning-deep-learning-april-2018/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1804.07723" target="_blank" rel="noopener">Image Inpainting for Irregular Holes Using Partial Convolutions</a></p>
<p>Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. This paper by a team of NVIDIA AI researchers includes a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. The model outperforms other methods for irregular masks. A demo video is included below:</p>
<p><iframe id="_ytid_78943" width="430" height="242" data-origwidth="430" data-origheight="242" src="https://www.youtube.com/embed/gg0F5JjKmhA?enablejsapi=1&amp;autoplay=0&amp;cc_load_policy=0&amp;cc_lang_pref=&amp;iv_load_policy=1&amp;loop=0&amp;modestbranding=0&amp;rel=1&amp;fs=1&amp;playsinline=0&amp;autohide=2&amp;theme=dark&amp;color=red&amp;controls=1&amp;" class="__youtube_prefs__  epyt-is-override  no-lazyload" title="YouTube player" allow="autoplay; encrypted-media" allowfullscreen="" data-no-lazy="1" data-skipgform_ajax_framebjll=""></iframe></p>
<p><a href="https://arxiv.org/abs/1804.04589" target="_blank" rel="noopener">A Survey on Neural Network-Based Summarization Methods</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">Automatic text summarization, the automated process of shortening a text while reserving the main ideas of the document(s), is a critical research area in natural language processing. The aim of this literature review is to survey the recent work on neural-based models in automatic text summarization. The author examines in detail ten state-of-the-art neural-based summarizers: five abstractive models and five extractive models. In addition, the paper discusses the related techniques that can be applied to the summarization tasks and present promising paths for future research in neural-based summarization.</p>
<p><a href="https://arxiv.org/abs/1705.04058" target="_blank" rel="noopener">Neural Style Transfer: A Review</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNN) in creating artistic imagery by separating and recombining image content and style. This process of using CNN to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. This review aims to provide an overview of the current progress towards NST, as well as discussing its various applications and open problems for future research.</p>
<p><a href="https://arxiv.org/abs/1801.10130" target="_blank" rel="noopener">Spherical CNNs</a></p>
<p>Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective. This paper introduces the building blocks for constructing spherical CNNs.</p>
<p><a href="https://arxiv.org/abs/1804.00222" target="_blank" rel="noopener">Learning Unsupervised Learning Rules</a></p>
<p>A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this goal is approached by minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. This paper proposes instead to directly target a later desired task by meta-learning an unsupervised learning rule, which leads to representations useful for that task.</p>
<p><em>Sign up for the free insideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-20518 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-artificial-intelligence tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – May 2018</h1>
<div class="post-info"><span class="date published time" title="2018-06-04T08:30:24-07:00">June 4, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/06/04/best-arxiv-org-ai-machine-learning-deep-learning-may-2018/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1805.08308" target="_blank" rel="noopener">geomstats: a Python Package for Riemannian Geometry in Machine Learning</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">There is a growing interest in using Riemannian geometry in machine learning. This paper introduces <em>geomstats</em>, a python package that performs computations on manifolds such as hyperspheres, hyperbolic spaces, spaces of symmetric positive definite matrices and Lie groups of transformations. Also provided is efficient and extensively unit-tested implementations of these manifolds, together with useful Riemannian metrics and associated Exponential and Logarithm maps. The corresponding geodesic distances provide a range of intuitive choices of Machine Learning loss functions. The authors also give the corresponding Riemannian gradients. The operations implemented in geomstats are available with different computing backends such as numpy, tensorflow and keras. The authors have enabled GPU implementation and integrated geomstats manifold computations into keras deep learning framework. This paper also presents a review of manifolds in machine learning and an overview of the geomstats package with examples demonstrating its use for efficient and user-friendly Riemannian geometry.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20519" src="https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic1.jpg" alt="" width="700" height="454" srcset="https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic1.jpg 700w, https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic1-300x195.jpg 300w, https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic1-150x97.jpg 150w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p><a href="https://arxiv.org/abs/1710.03370" target="_blank" rel="noopener">iVQA: Inverse Visual Question Answering</a></p>
<p>This paper proposes the inverse problem of Visual question answering (iVQA), and explore its suitability as a benchmark for visuo-linguistic understanding. The iVQA task is to generate a question that corresponds to a given image and answer pair. Since the answers are less informative than the questions, and the questions have less learnable bias, an iVQA model needs to better understand the image to be successful than a VQA model. The authors pose question generation as a multi-modal dynamic inference process and propose an iVQA model that can gradually adjust its focus of attention guided by both a partially generated question and the answer. For evaluation, apart from existing linguistic metrics, we propose a new ranking metric. This metric compares the ground truth question’s rank among a list of distractors, which allows the drawbacks of different algorithms and sources of error to be studied. Experimental results show that this model can generate diverse, grammatically correct and content correlated questions that match the given answer.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20520" src="https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic2.jpg" alt="" width="700" height="518" srcset="https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic2.jpg 700w, https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic2-300x222.jpg 300w, https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic2-150x111.jpg 150w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p><a href="https://arxiv.org/abs/1805.06576" target="_blank" rel="noopener">A Spline Theory of Deep Networks</a></p>
<p>This paper builds a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. The key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20521" src="https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic3.jpg" alt="" width="654" height="808" srcset="https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic3.jpg 654w, https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic3-243x300.jpg 243w, https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic3-121x150.jpg 121w" sizes="(max-width: 654px) 100vw, 654px"></p>
<p><a href="https://arxiv.org/abs/1804.04849" target="_blank" rel="noopener">The unreasonable effectiveness of the forget gate</a></p>
<p>Given the success of the gated recurrent unit, a natural question is whether all the gates of the long short-term memory (LSTM) network are necessary. Previous research has shown that the forget gate is one of the most important gates in the LSTM. This paper shows that a forget-gate-only version of the LSTM with chrono-initialized biases, not only provides computational savings but outperforms the standard LSTM on multiple benchmark datasets and competes with some of the best contemporary models. The proposed network, the JANET, achieves accuracies of 99% and 92.5% on the MNIST and pMNIST data sets, outperforming the standard LSTM which yields accuracies of 98.5% and 91%.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20522" src="https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic4.jpg" alt="" width="700" height="575" srcset="https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic4.jpg 700w, https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic4-300x246.jpg 300w, https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic4-150x123.jpg 150w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p><a href="https://arxiv.org/abs/1802.05957" target="_blank" rel="noopener">Spectral Normalization for Generative Adversarial Networks</a></p>
<p>One of the challenges in the study of generative adversarial networks is the instability of its training. This paper proposes a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. A new normalization technique is computationally light and easy to incorporate into existing implementations. The authors tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 data set, and experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20523" src="https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic5.jpg" alt="" width="700" height="533" srcset="https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic5.jpg 700w, https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic5-300x228.jpg 300w, https://insidebigdata.com/wp-content/uploads/2018/06/arXiv_pic5-150x114.jpg 150w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p>&nbsp;</p>
<p><em>Sign up for the free insideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-20709 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – June 2018</h1>
<div class="post-info"><span class="date published time" title="2018-07-10T08:30:52-07:00">July 10, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/07/10/best-arxiv-org-ai-machine-learning-deep-learning-june-2018/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1703.08966" target="_blank" rel="noopener">Mastering Sketching: Adversarial Augmentation for Structured Prediction</a></p>
<p>This paper presents an integral framework for training sketch simplification networks that convert challenging rough sketches into clean line drawings. The approach augments a simplification network with a discriminator network, training both networks jointly so that the discriminator network discerns whether a line drawing is a real training data or the output of the simplification network, which in turn tries to fool it.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20710" src="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic1.png" alt="" width="700" height="259"></p>
<p><a href="https://arxiv.org/abs/1806.05393" target="_blank" rel="noopener">Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks</a></p>
<p>In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. This paper demonstrates that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. The authors derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix.<br>
<img loading="lazy" class="aligncenter size-full wp-image-20711" src="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic2.png" alt="" width="659" height="649" srcset="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic2.png 659w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic2-300x295.png 300w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic2-50x50.png 50w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic2-150x148.png 150w" sizes="(max-width: 659px) 100vw, 659px"></p>
<p><a href="https://arxiv.org/abs/1806.04558" target="_blank" rel="noopener">Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</a></p>
<p>We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20713" src="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic3.png" alt="" width="700" height="150" srcset="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic3.png 700w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic3-300x64.png 300w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic3-150x32.png 150w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p><a href="https://arxiv.org/abs/1701.03077" target="_blank" rel="noopener">A More General Robust Loss Function</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">This paper presents a two-parameter loss function which can be viewed as a generalization of many popular loss functions used in robust statistics: the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, and generalized Charbonnier loss functions (and by transitivity the L2, L1, L1-L2, and pseudo-Huber/Charbonnier loss functions). If this penalty is viewed as a negative log-likelihood, it yields a general probability distribution that includes normal and Cauchy distributions as special cases. The author describe and visualize this loss and its corresponding distribution, and document several of their useful properties.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20714" src="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic4.png" alt="" width="584" height="586" srcset="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic4.png 584w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic4-150x150.png 150w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic4-300x300.png 300w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic4-110x110.png 110w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic4-50x50.png 50w" sizes="(max-width: 584px) 100vw, 584px"><a href="https://arxiv.org/abs/1805.11714" target="_blank" rel="noopener">Deep Video Portraits</a></p>
<p>This paper presents a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted to manipulations of facial expressions only, this is the first approach to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor. The core of the approach is a generative neural network with a novel space-time architecture.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20715" src="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic5.png" alt="" width="700" height="200" srcset="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic5.png 700w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic5-300x86.png 300w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic5-150x43.png 150w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p><a href="https://arxiv.org/abs/1805.06485" target="_blank" rel="noopener">QuaterNet: A Quaternion-based Recurrent Model for Human Motion</a></p>
<p>Deep learning for predicting or generating 3D human pose sequences is an active research area. Previous work regresses either joint rotations or joint positions. The former strategy is prone to error accumulation along the kinematic chain, as well as discontinuities when using Euler angle or exponential map parameterizations. The latter requires re-projection onto skeleton constraints to avoid bone stretching and invalid configurations. This work addresses both limitations. The proposed recurrent network, QuaterNet, represents rotations with quaternions and the loss function performs forward kinematics on a skeleton to penalize absolute position errors instead of angle errors.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20716" src="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic6.png" alt="" width="803" height="172" srcset="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic6.png 803w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic6-300x64.png 300w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic6-768x165.png 768w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic6-150x32.png 150w" sizes="(max-width: 803px) 100vw, 803px"><a href="https://arxiv.org/abs/1806.01337" target="_blank" rel="noopener">Backdrop: Stochastic Backpropagation</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">This paper introduces <em>backdrop</em>, a flexible and simple-to-implement method, intuitively described as dropout acting only along the backpropagation pipeline. Backdrop is implemented via one or more masking layers which are inserted at specific points along the network. Each backdrop masking layer acts as the identity in the forward pass, but randomly masks parts of the backward gradient propagation. Intuitively, inserting a backdrop layer after any convolutional layer leads to stochastic gradients corresponding to features of that scale. Therefore, backdrop is well suited for problems in which the data have a multi-scale, hierarchical structure.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20717" src="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic7.png" alt="" width="700" height="220" srcset="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic7.png 700w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic7-300x94.png 300w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic7-150x47.png 150w" sizes="(max-width: 700px) 100vw, 700px"></p>
<p><a href="https://arxiv.org/abs/1806.01830" target="_blank" rel="noopener">Relational Deep Reinforcement Learning</a></p>
<p>This paper introduces an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. The results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20718" src="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic8.png" alt="" width="837" height="370" srcset="https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic8.png 837w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic8-300x133.png 300w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic8-768x339.png 768w, https://insidebigdata.com/wp-content/uploads/2018/07/arXiv_June_pic8-150x66.png 150w" sizes="(max-width: 837px) 100vw, 837px"><br>
<em>Sign up for the free ins</em><em>ideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-20910 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-artificial-intelligence tag-deep-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – July 2018</h1>
<div class="post-info"><span class="date published time" title="2018-08-13T08:30:44-07:00">August 13, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/08/13/best-arxiv-org-ai-machine-learning-deep-learning-july-2018/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1804.08711" target="_blank" rel="noopener">All-Optical Machine Learning Using Diffractive Deep Neural Networks</a></p>
<p>This paper introduces an <em>all-optical</em> Diffractive Deep Neural Network (D2NN) architecture that can learn to implement various functions after deep learning-based design of passive diffractive layers that work collectively. The UCLA researchers experimentally demonstrated the success of this framework by creating 3D-printed D2NNs that learned to implement handwritten digit classification and the function of an imaging lens at terahertz spectrum. With the existing plethora of 3D-printing and other lithographic fabrication methods as well as spatial-light-modulators, this all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can implement.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20913" src="https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic1.png" alt="" width="602" height="274" srcset="https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic1.png 602w, https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic1-300x137.png 300w, https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic1-150x68.png 150w" sizes="(max-width: 602px) 100vw, 602px"></p>
<p><a href="https://arxiv.org/abs/1712.00559" target="_blank" rel="noopener">Progressive Neural Architecture Search</a></p>
<p>This paper propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. The approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20914" src="https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic2.png" alt="" width="567" height="355" srcset="https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic2.png 567w, https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic2-300x188.png 300w, https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic2-150x94.png 150w" sizes="(max-width: 567px) 100vw, 567px"></p>
<p><a href="https://arxiv.org/abs/1806.08730" target="_blank" rel="noopener">The Natural Language Decathlon: Multitask Learning as Question Answering</a></p>
<p>Deep learning has improved performance on many natural language processing (NLP) tasks individually. However, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, data set, and task. This paper introduces the <em>Natural Language Decathlon</em> (decaNLP), a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20915" src="https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic3.png" alt="" width="649" height="358" srcset="https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic3.png 649w, https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic3-300x165.png 300w, https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic3-150x83.png 150w" sizes="(max-width: 649px) 100vw, 649px"><br>
<a href="https://arxiv.org/abs/1805.12076" target="_blank" rel="noopener">Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks</a></p>
<p>Despite existing work on ensuring generalization of neural networks in terms of scale sensitive complexity measures, such as norms, margin and sharpness, these complexity measures do not offer an explanation of why neural networks generalize better with over-parametrization. This work suggests a novel complexity measure based on unit-wise capacities resulting in a tighter generalization bound for two layer ReLU networks.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20916" src="https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic4.png" alt="" width="718" height="223" srcset="https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic4.png 718w, https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic4-300x93.png 300w, https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic4-150x47.png 150w" sizes="(max-width: 718px) 100vw, 718px"></p>
<p><a href="https://arxiv.org/abs/1806.07832" target="_blank" rel="noopener">StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing</a></p>
<p>Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. This paper introduces StructVAE, a variational auto-encoding model for semi-supervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-20917" src="https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic5.png" alt="" width="722" height="251" srcset="https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic5.png 722w, https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic5-300x104.png 300w, https://insidebigdata.com/wp-content/uploads/2018/08/arXiv_July_pic5-150x52.png 150w" sizes="(max-width: 722px) 100vw, 722px"></p>
<p>&nbsp;</p>
<p><em>Sign up for the free ins</em><em>ideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-21082 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-artificial-intelligence tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – August 2018</h1>
<div class="post-info"><span class="date published time" title="2018-09-13T08:30:20-07:00">September 13, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/09/13/best-arxiv-org-ai-machine-learning-deep-learning-august-2018/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1809.00946" target="_blank" rel="noopener">Twin-GAN — Unpaired Cross-Domain Image Translation with Weight-Sharing GANs</a></p>
<p>This paper presents a framework for translating unlabeled images from one domain into analog images in another domain. It employs a progressively growing skip-connected encoder-generator structure and train it with a GAN loss for realistic output, a cycle consistency loss for maintaining same-domain translation identity, and a semantic consistency loss that encourages the network to keep the input semantic features in the output. The author from Google applies the framework on the task of translating face images, and show that it is capable of learning semantic mappings for face images with no supervised one-to-one image mapping.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21083" src="https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic1.png" alt="" width="652" height="280" srcset="https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic1.png 652w, https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic1-300x129.png 300w, https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic1-150x64.png 150w" sizes="(max-width: 652px) 100vw, 652px"></p>
<p><a href="https://arxiv.org/abs/1808.08946v2" target="_blank" rel="noopener">Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures</a></p>
<p>Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. This paper hypothesizes that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and the authors evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21084" src="https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic2.png" alt="" width="684" height="224" srcset="https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic2.png 684w, https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic2-300x98.png 300w, https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic2-150x49.png 150w" sizes="(max-width: 684px) 100vw, 684px"></p>
<p><a href="https://arxiv.org/abs/1807.03247" target="_blank" rel="noopener">An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. This paper shows a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and one-hot pixel space. Although convolutional networks would seem appropriate for this task, the authors from Uber show that they fail spectacularly. The paper demonstrates and carefully analyzes the failure first on a toy problem, at which point a simple fix becomes obvious. This solution is called CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21085" src="https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic3.png" alt="" width="660" height="305" srcset="https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic3.png 660w, https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic3-300x139.png 300w, https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic3-150x69.png 150w" sizes="(max-width: 660px) 100vw, 660px"></p>
<p><a href="https://arxiv.org/abs/1808.02822" target="_blank" rel="noopener">Backprop Evolution</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">The back-propagation algorithm is the cornerstone of deep learning. Despite its importance, few variations of the algorithm have been attempted. This work presents an approach to discover new variations of the back-propagation equation. The authors from Google use a domain specific lan- guage to describe update equations as a list of primitive functions. An evolution-based method is used to discover new propagation rules that maximize the generalization per- formance after a few epochs of training. The research finds several update equations that can train faster with short training times than standard back-propagation, and perform similar as standard back-propagation at convergence.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21087" src="https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic4.png" alt="" width="695" height="329" srcset="https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic4.png 695w, https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic4-300x142.png 300w, https://insidebigdata.com/wp-content/uploads/2018/09/arXiv_Aug_pic4-150x71.png 150w" sizes="(max-width: 695px) 100vw, 695px"></p>
<p>&nbsp;</p>
<p><em>Sign up for the free ins</em><em>ideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
<p>&nbsp;</p>
</div></div><!-- end .after-post-ad --></div><div class="post-21252 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis category-uncategorized tag-artificial-intelligence tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – September 2018</h1>
<div class="post-info"><span class="date published time" title="2018-10-17T08:30:31-07:00">October 17, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/10/17/best-arxiv-org-ai-machine-learning-deep-learning-september-2018/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1809.10756" target="_blank" rel="noopener">An Introduction to Probabilistic Programming</a></p>
<p>This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21253" src="https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic1.png" alt="" width="552" height="289" srcset="https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic1.png 552w, https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic1-300x157.png 300w, https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic1-150x79.png 150w" sizes="(max-width: 552px) 100vw, 552px"></p>
<p><a href="https://arxiv.org/abs/1809.03193" target="_blank" rel="noopener">Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">Object detection-the computer vision task dealing with detecting instances of objects of a certain class (e.g., ‘car’, ‘plane’, etc.) in images-attracted a lot of attention from the community during the last 5 years. This strong interest can be explained not only by the importance this task has for many applications but also by the phenomenal advances in this area since the arrival of deep convolutional neural networks (DCNN). This article reviews the recent literature on object detection with deep CNN, in a comprehensive way, and provides an in-depth view of these recent advances.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21254" src="https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic2.png" alt="" width="613" height="315" srcset="https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic2.png 613w, https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic2-300x154.png 300w, https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic2-150x77.png 150w" sizes="(max-width: 613px) 100vw, 613px"></p>
<p><a href="https://arxiv.org/abs/1809.08267" target="_blank" rel="noopener">Neural Approaches to Conversational AI</a></p>
<p>This paper surveys neural approaches to conversational AI that have been developed in the last few years. Conversational systems are grouped into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) chatbots. For each category, the paper presents a review of state-of-the-art neural approaches, draw the connection between them and traditional approaches, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21255" src="https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic3.png" alt="" width="675" height="385" srcset="https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic3.png 675w, https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic3-300x171.png 300w, https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic3-150x86.png 150w" sizes="(max-width: 675px) 100vw, 675px"></p>
<p><a href="https://arxiv.org/abs/1809.01984" target="_blank" rel="noopener">Training Millions of Personalized Dialogue Agents</a></p>
<p>Current dialogue systems are not very engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the data set used in Zhang et al. (2018) is synthetic and of limited size as it contains around 1k different personas. In this paper we introduce a new data set providing 5 million personas and 700 million persona-based dialogues. The experiments described in this paper show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, it is shown that other tasks benefit from the wide coverage of the data set by fine-tuning the model on the data from Zhang et al. (2018) and achieving state-of-the-art results.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21256" src="https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic4.png" alt="" width="560" height="205" srcset="https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic4.png 560w, https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic4-300x110.png 300w, https://insidebigdata.com/wp-content/uploads/2018/10/arXiv_Sept_pic4-150x55.png 150w" sizes="(max-width: 560px) 100vw, 560px"></p>
<p>&nbsp;</p>
<p><em>Sign up for the free ins</em><em>ideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-21456 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis category-uncategorized tag-artificial-intelligence tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – October 2018</h1>
<div class="post-info"><span class="date published time" title="2018-11-15T08:30:19-08:00">November 15, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/11/15/best-arxiv-org-ai-machine-learning-deep-learning-october-2018/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1811.00002" target="_blank" rel="noopener">WaveGlow: A Flow-based Generative Network for Speech Synthesis</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">This paper proposes <em>WaveGlow</em>: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. The authors, all from NVIDIA, provide a PyTorch implementation that produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. WaveGlow code is available on <a href="https://github.com/NVIDIA/waveglow" target="_blank" rel="noopener">GitHub</a>.</p><!--<div style="float:left; width:310px; height:250px;">	-->

<div id="desktop-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle -->
<div id="div-gpt-ad-1439400881943-0" style="height:250px; width:300px;">
</div>
</div>

<div id="mobile-inarticle-ad">
<!-- /1061671/IBD_300x250_InArticle 
<div id='div-gpt-ad-1439400881943-0' style='height:250px; width:300px;'>
</div>-->
</div>

<!--</div>-->
<p><img loading="lazy" class="aligncenter size-full wp-image-21458" src="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic1.png" alt="" width="469" height="419" srcset="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic1.png 469w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic1-300x268.png 300w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic1-150x134.png 150w" sizes="(max-width: 469px) 100vw, 469px"><a href="https://arxiv.org/abs/1810.10999" target="_blank" rel="noopener">Reversible Recurrent Neural Networks</a></p>
<p>Recurrent neural networks (RNNs) provide state-of-the-art performance in processing sequential data but are memory intensive to train, limiting the flexibility of RNN models which can be trained. Reversible RNNs—RNNs for which the hidden-to-hidden transition can be reversed—offer a path to reduce the memory requirements of training, as hidden states need not be stored and instead can be recomputed during backpropagation. This paper firsts show that perfectly reversible RNNs, which require no storage of the hidden activations, are fundamentally limited because they cannot forget information from their hidden state. The paper then provides a scheme for storing a small number of bits in order to allow perfect reversal with forgetting. The author’s method achieves comparable performance to traditional models while reducing the activation memory cost by a factor of 10–15.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21459" src="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic2.png" alt="" width="687" height="340" srcset="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic2.png 687w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic2-300x148.png 300w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic2-150x74.png 150w" sizes="(max-width: 687px) 100vw, 687px"></p>
<p><a href="https://arxiv.org/abs/1810.09202" target="_blank" rel="noopener">Graph Convolutional Reinforcement Learning for Multi-Agent Cooperation</a></p>
<p>Learning to cooperate is crucially important in multi-agent reinforcement learning. The key is to take the influence of other agents into consideration when performing distributed decision making. However, multi-agent environment is highly dynamic, which makes it hard to learn abstract representations of influences between agents by only low-order features that existing methods exploit. This paper proposes a graph convolutional model for multi-agent cooperation. The graph convolution architecture adapts to the dynamics of the underlying graph of the multi-agent environment, where the influence among agents is captured by their abstract relation representations. The code for this paper is available on <a href="https://github.com/PKU-AI-Edge/GraphConv4MARL/" target="_blank" rel="noopener">GitHub</a>.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21460" src="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic3.png" alt="" width="624" height="416" srcset="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic3.png 624w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic3-300x200.png 300w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic3-150x100.png 150w" sizes="(max-width: 624px) 100vw, 624px"></p>
<p><a href="https://arxiv.org/abs/1810.05369" target="_blank" rel="noopener">On the Margin Theory of Feedforward Neural Networks</a></p>
<p>Past works have shown that, somewhat surprisingly, over-parametrization can help generalization in neural networks. Towards explaining this phenomenon, this paper adopts a margin-based perspective. This paper establishes: 1) for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks, 2) as a result, increasing the over-parametrization improves the normalized margin and generalization error bounds for two-layer networks. In particular, an infinite-size neural network enjoys the best generalization guarantees.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21461" src="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic4.png" alt="" width="778" height="300" srcset="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic4.png 778w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic4-300x116.png 300w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic4-768x296.png 768w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic4-150x58.png 150w" sizes="(max-width: 778px) 100vw, 778px"></p>
<p><a href="https://arxiv.org/abs/1810.03599" target="_blank" rel="noopener">SFV: Reinforcement Learning of Physical Skills from Videos</a></p>
<p>Data-driven character animation based on motion capture can produce highly naturalistic behaviors and, when combined with physics simulation, can provide for natural procedural responses to physical perturbations, environmental changes, and morphological discrepancies. Motion capture remains the most popular source of motion data, but collecting mocap data typically requires heavily instrumented environments and actors. This paper proposes a method that enables physically simulated characters to learn skills from videos (SFV). The approach, based on deep pose estimation and deep reinforcement learning, allows data-driven animation to leverage the abundance of publicly available video clips from the web, such as those from YouTube.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21462" src="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic5.png" alt="" width="416" height="458" srcset="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic5.png 416w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic5-272x300.png 272w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic5-136x150.png 136w" sizes="(max-width: 416px) 100vw, 416px"></p>
<p><a href="https://arxiv.org/abs/1810.01993" target="_blank" rel="noopener">Exascale Deep Learning for Climate Analytics</a></p>
<p>In this paper, the authors extract pixel-level masks of extreme weather patterns using variants of Tiramisu and DeepLabv3+ neural networks. They describe improvements to the software frameworks, input pipeline, and the&nbsp; network training algorithms necessary to efficiently scale deep learning on the Piz Daint and Summit systems.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21463" src="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic6.png" alt="" width="522" height="607" srcset="https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic6.png 522w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic6-258x300.png 258w, https://insidebigdata.com/wp-content/uploads/2018/11/arXiv_Oct_pic6-129x150.png 129w" sizes="(max-width: 522px) 100vw, 522px"></p>
<p>&nbsp;</p>
<p><em>Sign up for the free ins</em><em>ideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div><p></p></div><!-- end .after-post-ad --></div><div class="post-21709 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-uncategorized tag-artificial-intelligence tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – November 2018</h1>
<div class="post-info"><span class="date published time" title="2018-12-19T08:30:59-08:00">December 19, 2018</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2018/12/19/best-arxiv-org-ai-machine-learning-deep-learning-november-2018/#comments">1 Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1811.00143" target="_blank" rel="noopener">Democratizing Production-Scale Distributed Deep Learning</a></p>
<p>The interest and demand for training deep neural networks have been experiencing rapid growth, spanning a wide range of applications in both academia and industry. However, training them distributed and at scale remains difficult due to the complex ecosystem of tools and hardware involved. One consequence is that the responsibility of orchestrating these complex components is often left to one-off scripts and glue code customized for specific problems. To address these restrictions, this paper introduces <em>Alchemist</em> – an internal service built at Apple from the ground up for easy, fast, and scalable distributed training.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21710" src="https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic1.png" alt="" width="566" height="474" srcset="https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic1.png 566w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic1-300x251.png 300w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic1-150x126.png 150w" sizes="(max-width: 566px) 100vw, 566px"></p>
<p><a href="https://arxiv.org/abs/1810.06339" target="_blank" rel="noopener">Deep Reinforcement Learning</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">This paper discusses deep reinforcement learning in an overview style with six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. The author starts with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next, the discuss turns to RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21712" src="https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic2.png" alt="" width="648" height="599" srcset="https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic2.png 648w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic2-300x277.png 300w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic2-150x139.png 150w" sizes="(max-width: 648px) 100vw, 648px"></p>
<p><a href="https://arxiv.org/abs/1712.09913" target="_blank" rel="noopener">Visualizing the Loss Landscape of Neural Nets</a></p>
<p>Neural network training relies on our ability to find “good” minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. This paper explores the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21713" src="https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic3.png" alt="" width="642" height="323" srcset="https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic3.png 642w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic3-300x151.png 300w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic3-150x75.png 150w" sizes="(max-width: 642px) 100vw, 642px"></p>
<p><a href="https://arxiv.org/abs/1803.09473" target="_blank" rel="noopener">code2vec: Learning Distributed Representations of Code</a></p>
<p>This paper presents a neural model for representing snippets of code as continuous distributed vectors (“code embeddings”). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. This is performed by decomposing code to a collection of paths in its abstract syntax tree, and learning the atomic representation of each path simultaneously with learning how to aggregate a set of them. The authors demonstrate the effectiveness of our approach by using it to predict a method’s name from the vector representation of its body. The authors evaluate their approach by training a model on a data set of 14M methods. The paper shows that code vectors trained on this data set can predict method names from files that were completely unobserved during training. A GitHub repo for the paper can be found <a href="https://github.com/tech-srl/code2vec" target="_blank" rel="noopener">HERE</a>.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21714" src="https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic4.png" alt="" width="898" height="417" srcset="https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic4.png 898w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic4-300x139.png 300w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic4-768x357.png 768w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic4-150x70.png 150w" sizes="(max-width: 898px) 100vw, 898px"></p>
<p><a href="https://arxiv.org/abs/1812.04948" target="_blank" rel="noopener">A Style-Based Generator Architecture for Generative Adversarial Networks</a></p>
<p>This paper from NVIDIA researchers proposes an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21715" src="https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic5.png" alt="" width="432" height="611" srcset="https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic5.png 432w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic5-212x300.png 212w, https://insidebigdata.com/wp-content/uploads/2018/12/arXiv_Nov_pic5-106x150.png 106w" sizes="(max-width: 432px) 100vw, 432px"></p>
<p>&nbsp;</p>
<p><em>Sign up for the free ins</em><em>ideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-21994 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-featured category-google-news-feed category-machine-learning category-news-analysis category-uncategorized tag-artificial-intelligence tag-big-data tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – December 2018</h1>
<div class="post-info"><span class="date published time" title="2019-01-16T08:30:30-08:00">January 16, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/01/16/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-december-2018/#comments">2 Comments</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1811.12560" target="_blank" rel="noopener">An Introduction to Deep Reinforcement Learning</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This paper provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. The reader is assumed to be familiar with basic machine learning concepts.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21995" src="https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic1.png" alt="" width="550" height="431" srcset="https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic1.png 550w, https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic1-150x118.png 150w, https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic1-300x235.png 300w" sizes="(max-width: 550px) 100vw, 550px"></p>
<p><a href="https://arxiv.org/abs/1806.04418" target="_blank" rel="noopener">Quaternion Recurrent Neural Networks</a></p>
<p>Recurrent neural networks (RNNs) are powerful architectures to model sequential data, due to their capability to learn short and long-term dependencies between the basic elements of a sequence. Nonetheless, popular tasks such as speech or images recognition, involve multi-dimensional input features that are characterized by strong internal dependencies between the dimensions of the input vector. This paper proposes a novel quaternion recurrent neural network (QRNN), alongside with a quaternion long-short term memory neural network (QLSTM), that take into account both the external relations and these internal structural dependencies with the quaternion algebra. Visit the GitHub repo for this paper: <a href="https://github.com/Orkis-Research/Pytorch-Quaternion-Neural-Networks" target="_blank" rel="noopener">Pytorch-Quaternion-Neural-Networks</a>.<br>
<img loading="lazy" class="aligncenter size-full wp-image-21996" src="https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic2.png" alt="" width="635" height="327" srcset="https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic2.png 635w, https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic2-150x77.png 150w, https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic2-300x154.png 300w" sizes="(max-width: 635px) 100vw, 635px"></p>
<p><a href="https://arxiv.org/abs/1812.04202" target="_blank" rel="noopener">Deep Learning on Graphs: A Survey</a></p>
<p>Deep learning has been shown successful in a number of domains, ranging from acoustics, images to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, a significant amount of research efforts have been devoted to this area, greatly advancing graph analyzing techniques. This survey provide a comprehensive review of different kinds of deep learning methods applied to graphs.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21997" src="https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic3.png" alt="" width="820" height="356" srcset="https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic3.png 820w, https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic3-150x65.png 150w, https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic3-300x130.png 300w, https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic3-768x333.png 768w" sizes="(max-width: 820px) 100vw, 820px"><br>
<a href="https://arxiv.org/abs/1805.10451" target="_blank" rel="noopener">Geometric Understanding of Deep Learning</a></p>
<p>Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, and so on. It has outperformed conventional methods in various fields and achieved great successes. Unfortunately, the understanding on how it works remains unclear. It has the central importance to lay down the theoretic foundation for deep learning. This paper gives a geometric view to understand deep learning: showing that the fundamental principle attributing to the success is the manifold structure in data, namely natural high dimensional data concentrates close to a low-dimensional manifold, deep learning learns the manifold and the probability distribution on it.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-21998" src="https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic4.png" alt="" width="464" height="374" srcset="https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic4.png 464w, https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic4-150x121.png 150w, https://insidebigdata.com/wp-content/uploads/2019/01/arXiv_Dec_pic4-300x242.png 300w" sizes="(max-width: 464px) 100vw, 464px"></p>
<p>&nbsp;</p>
<p><em>Sign up for the free ins</em><em>ideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</div></div><!-- end .after-post-ad --></div><div class="post-22155 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-artificial-intelligence tag-big-data tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – January 2019</h1>
<div class="post-info"><span class="date published time" title="2019-02-20T08:30:55-08:00">February 20, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/02/20/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-january-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1901.10995" target="_blank" rel="noopener">Go-Explore: a New Approach for Hard-Exploration Problems</a></p>
<p>A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma’s Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, this paper introduces a new algorithm called <em>Go-Explore</em>.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-22157" src="https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic1.png" alt="" width="659" height="171" srcset="https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic1.png 659w, https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic1-150x39.png 150w, https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic1-300x78.png 300w" sizes="(max-width: 659px) 100vw, 659px"></p>
<p><a href="https://arxiv.org/abs/1901.06955" target="_blank" rel="noopener">Deep Neural Network Approximation for Custom Hardware: Where We’ve Been, Where We’re Going</a></p>
<p>Deep neural networks have proven to be particularly effective in visual and audio recognition tasks. Existing models tend to be computationally expensive and memory intensive, however, and so methods for hardware-oriented approximation have become a hot topic. Research has shown that custom hardware-based neural network accelerators can surpass their general-purpose processor equivalents in terms of both throughput and energy efficiency. Application-tailored accelerators, when co-designed with approximation-based network training methods, transform large, dense and computationally expensive networks into small, sparse and hardware-efficient alternatives, increasing the feasibility of network deployment. This article provides a comprehensive evaluation of approximation methods for high-performance network inference along with in-depth discussion of their effectiveness for custom hardware implementation.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-22158" src="https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic2.png" alt="" width="632" height="307" srcset="https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic2.png 632w, https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic2-150x73.png 150w, https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic2-300x146.png 300w" sizes="(max-width: 632px) 100vw, 632px"></p>
<p><a href="https://arxiv.org/abs/1901.06796" target="_blank" rel="noopener">Generating Textual Adversarial Examples for Deep Learning Models: A Survey</a></p>
<p>With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs were vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples for image DNNs, research efforts on attacking DNNs for textual applications emerges in recent years. However, existing perturbation methods for images cannotbe directly applied to texts as text data is discrete. This article reviews research that addresses this difference and generatetextual adversarial examples on DNNs.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-22159" src="https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic3.png" alt="" width="402" height="266" srcset="https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic3.png 402w, https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic3-150x99.png 150w, https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic3-300x199.png 300w" sizes="(max-width: 402px) 100vw, 402px"></p>
<p><a href="https://arxiv.org/abs/1901.09005" target="_blank" rel="noopener">Revisiting Self-Supervised Visual Representation Learning</a></p>
<p>Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal attention. Therefore, this paper revisits numerous previously proposed self-supervised models, conducts a thorough large scale study and, as a result, uncover multiple crucial insights. The code accompanying this paper can be found <a href="https://github.com/google/revisiting-self-supervised" target="_blank" rel="noopener">HERE</a>.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-22160" src="https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic4.png" alt="" width="434" height="416" srcset="https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic4.png 434w, https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic4-150x144.png 150w, https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic4-300x288.png 300w" sizes="(max-width: 434px) 100vw, 434px"><br>
<a href="https://arxiv.org/abs/1901.04407" target="_blank" rel="noopener">Self-Driving Cars: A Survey</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">This paper surveys research on self-driving cars published in the literature focusing on autonomous cars developed since the DARPA challenges, which are equipped with an autonomy system that can be categorized as SAE level 3 or higher. The architecture of the autonomy system of self-driving cars is typically organized into the perception system and the decision-making system. The perception system is generally divided into many subsystems responsible for tasks such as self-driving-car localization, static obstacles mapping, moving obstacles detection and tracking, road mapping, traffic signalization detection and recognition, among others. The decision-making system is commonly partitioned as well into many subsystems responsible for tasks such as route planning, path planning, behavior selection, motion planning, and control. The survey presents the typical architecture of the autonomy system of self-driving cars, and also reviews research on relevant methods for perception and decision making.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-22161" src="https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic5.png" alt="" width="397" height="622" srcset="https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic5.png 397w, https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic5-96x150.png 96w, https://insidebigdata.com/wp-content/uploads/2019/02/arXiv_Jan19_pic5-191x300.png 191w" sizes="(max-width: 397px) 100vw, 397px"></p>
<p>&nbsp;</p>
<p><em>Sign up for the free ins</em><em>ideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-22263 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-artificial-intelligence tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – February 2019</h1>
<div class="post-info"><span class="date published time" title="2019-03-15T08:30:05-07:00">March 15, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/03/15/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-february-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<p><a href="https://arxiv.org/abs/1902.04522v2" target="_blank" rel="noopener">ELF OpenGo: An Analysis and Open Reimplementation of AlphaZero</a></p>
<p>The AlphaGo, AlphaGo Zero, and AlphaZero series of algorithms are a remarkable demonstration of deep reinforcement learning’s capabilities, achieving superhuman performance in the complex game of Go with progressively increasing autonomy. However, many obstacles remain in the understanding of and usability of these promising approaches by the research community. Toward elucidating unresolved mysteries and facilitating future research, the authors propose ELF OpenGo, an open-source reimplementation of the AlphaZero algorithm. ELF OpenGo is the first open-source Go AI to convincingly demonstrate superhuman performance with a perfect (20:0) record against global top professionals.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-22265" src="https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic1.png" alt="" width="585" height="430" srcset="https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic1.png 585w, https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic1-150x110.png 150w, https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic1-300x221.png 300w" sizes="(max-width: 585px) 100vw, 585px"></p>
<p><a href="https://arxiv.org/abs/1902.04259" target="_blank" rel="noopener">NAIL: A General Interactive Fiction Agent</a></p>
<p>Interactive Fiction (IF) games are complex textual decision making problems. This paper introduces NAIL, an autonomous agent for general parser-based IF games. NAIL won the 2018 Text Adventure AI Competition, where it was evaluated on twenty unseen games. This paper from Microsoft Research AI describes the architecture, development, and insights underpinning NAIL’s performance.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-22266" src="https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic2.png" alt="" width="663" height="332" srcset="https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic2.png 663w, https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic2-150x75.png 150w, https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic2-300x150.png 300w" sizes="(max-width: 663px) 100vw, 663px"></p>
<p><a href="https://arxiv.org/abs/1902.08605" target="_blank" rel="noopener">Centroid Networks for Few-Shot Clustering and Unsupervised Few-Shot Classification</a></p>
<p><img loading="lazy" class="alignleft size-full wp-image-19718" src="https://insidebigdata.com/wp-content/uploads/2018/01/Thumbs-Up-Circuit-Board.png" alt="" width="80" height="82">Traditional clustering algorithms such as K-means rely heavily on the nature of the chosen metric or data representation. To get meaningful clusters, these representations need to be tailored to the downstream task (e.g. cluster photos by object category, cluster faces by identity). Therefore, this paper frames clustering as a meta-learning task, few-shot clustering, which allows us to specify how to cluster the data at the meta-training level, despite the clustering algorithm itself being unsupervised. The paper proposes Centroid Networks, a simple and efficient few-shot clustering method based on learning representations which are tailored both to the task to solve and to its internal clustering module.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-22269" src="https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic3a.png" alt="" width="483" height="614" srcset="https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic3a.png 483w, https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic3a-118x150.png 118w, https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic3a-236x300.png 236w" sizes="(max-width: 483px) 100vw, 483px"><br>
<a href="https://arxiv.org/abs/1902.06815" target="_blank" rel="noopener">The State of the Art in Multilayer Network Visualization</a></p>
<p>Modelling relationships between entities in real-world systems with a simple graph is a standard approach. However, reality is better embraced as several interdependent subsystems (or layers). Recently the concept of a multilayer network model has emerged from the field of complex systems. This model can be applied to a wide range of real-world data sets. Examples of multilayer networks can be found in the domains of life sciences, sociology, digital humanities and more. Within the domain of graph visualization there are many systems which visualize data sets having many characteristics of multilayer graphs. This report provides a state of the art and a structured analysis of contemporary multilayer network visualization, not only for researchers in visualization, but also for those who aim to visualize multilayer networks in the domain of complex systems, as well as those developing systems across application domains.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-22270" src="https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic4.png" alt="" width="465" height="509" srcset="https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic4.png 465w, https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic4-137x150.png 137w, https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic4-274x300.png 274w" sizes="(max-width: 465px) 100vw, 465px"></p>
<p><a href="https://arxiv.org/abs/1902.06068" target="_blank" rel="noopener">Deep Learning for Image Super-resolution: A Survey</a></p>
<p>Image Super-Resolution (SR) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. This survey aims to give a survey on recent advances of image super-resolution techniques using deep learning approaches in a systematic way.</p>
<p><img loading="lazy" class="aligncenter size-full wp-image-22271" src="https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic5.png" alt="" width="374" height="720" srcset="https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic5.png 374w, https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic5-78x150.png 78w, https://insidebigdata.com/wp-content/uploads/2019/03/arXiv_Feb_pic5-156x300.png 156w" sizes="(max-width: 374px) 100vw, 374px"></p>
<p>&nbsp;</p>
<p><em>Sign up for the free ins</em><em>ideBIGDATA&nbsp;<a href="http://inside-bigdata.com/newsletter/" target="_blank" rel="noopener">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-22431 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis tag-ai tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – March 2019</h1>
<div class="post-info"><span class="date published time" title="2019-04-09T08:30:35-07:00">April 9, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/04/09/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-march-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="226" height="191" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 226px) 100vw, 226px"></figure></div>







<p><a href="https://arxiv.org/abs/1903.08114" target="_blank" rel="noreferrer noopener" aria-label="Exact Gaussian Processes on a Million Data Points (opens in a new tab)">Exact Gaussian Processes on a Million Data Points</a></p>



<p>Gaussian processes (GPs) are flexible models with state-of-the-art performance on many impactful applications. However, computational constraints with standard inference procedures have limited exact GPs to problems with fewer than about ten thousand training points, necessitating approximations for larger datasets. This paper develops a scalable approach for exact GPs that leverages multi-GPU parallelization and methods like linear conjugate gradients, accessing the kernel matrix only through matrix multiplication.</p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="385" height="507" src="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic1.png" alt="" class="wp-image-22436" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic1.png 385w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic1-114x150.png 114w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic1-228x300.png 228w" sizes="(max-width: 385px) 100vw, 385px"></figure></div>



<p><a href="https://arxiv.org/abs/1903.12650" target="_blank" rel="noreferrer noopener" aria-label="Yet Another Accelerated SGD: ResNet-50 Training on ImageNet in 74.7 seconds (opens in a new tab)">Yet Another Accelerated SGD: ResNet-50 Training on ImageNet in 74.7 seconds</a></p>



<p>There has been a strong demand for algorithms that can execute machine learning as faster as possible and the speed of deep learning has accelerated by 30 times only in the past two years. Distributed deep learning  using the large mini-batch is a key technology to address the demand  and is a great challenge as it is difficult to achieve high scalability  on large clusters without compromising accuracy. This paper  introduces optimization methods applied to this challenge. The authors achieved the training time of 74.7 seconds using 2,048 GPUs on ABCI  cluster applying these methods. The training throughput is over 1.73  million images/sec and the top-1 validation accuracy is 75.08%. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="640" height="239" src="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic2.png" alt="" class="wp-image-22438" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic2.png 640w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic2-150x56.png 150w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic2-300x112.png 300w" sizes="(max-width: 640px) 100vw, 640px"></figure></div>



<p><a href="https://arxiv.org/abs/1903.12520" target="_blank" rel="noreferrer noopener" aria-label="Multimodal Emotion Classification (opens in a new tab)">Multimodal Emotion Classification</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Most NLP and Computer Vision tasks are limited to scarcity of  labelled data. In social media emotion classification and other related  tasks, hashtags have been used as indicators to label data. With the  rapid increase in emoji usage of social media, emojis are used as an  additional feature for major social NLP tasks. However, this is less  explored in case of multimedia posts on social media where posts are  composed of both image and text. At the same time, w.e have seen a surge  in the interest to incorporate domain knowledge to improve machine  understanding of text. In this paper, we investigate whether domain  knowledge for emoji can improve the accuracy of emotion classification  task.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="393" height="402" src="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic3.png" alt="" class="wp-image-22439" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic3.png 393w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic3-147x150.png 147w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic3-293x300.png 293w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic3-50x50.png 50w" sizes="(max-width: 393px) 100vw, 393px"></figure></div>



<p><a href="https://arxiv.org/abs/1903.12266" target="_blank" rel="noreferrer noopener" aria-label="Generative Adversarial Networks: recent developments (opens in a new tab)">Generative Adversarial Networks: recent developments</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>In traditional generative modeling, good data representation is very often a base for a good machine learning model. It can be linked to good representations encoding more explanatory factors that are hidden in the original data. With the invention of Generative Adversarial Networks (GANs), a subclass of generative models that are able to learn representations in an unsupervised and semi-supervised fashion, we are now able to adversarially learn good mappings from a simple prior distribution to a target data distribution. This paper presents an overview of recent developments in GANs with a focus on learning latent space representations. </p>



<p><a href="https://arxiv.org/abs/1903.09732" target="_blank" rel="noreferrer noopener" aria-label="Time Series Imputation (opens in a new tab)">Time Series Imputation</a></p>



<p> Multivariate time series is a very active topic in the research community and many machine learning tasks are being used in order to extract information from this type of data. However, in real-world problems data has missing values, which may difficult the application of machine learning techniques to extract information. This paper focuses on the task of imputation of time series. Many imputation methods for time series are based on regression methods. Unfortunately, these methods perform poorly when the variables are categorical. To address this case, the authors propose a new imputation method based on Expectation Maximization over dynamic Bayesian networks. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="697" height="397" src="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic4.png" alt="" class="wp-image-22443" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic4.png 697w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic4-150x85.png 150w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic4-300x171.png 300w" sizes="(max-width: 697px) 100vw, 697px"></figure></div>



<p><a href="https://arxiv.org/abs/1903.08708" target="_blank" rel="noreferrer noopener" aria-label="Accelerating Gradient Boosting Machine (opens in a new tab)">Accelerating Gradient Boosting Machine</a></p>



<p>Gradient Boosting Machine (GBM) is an extremely powerful supervised learning algorithm that is widely used in practice. GBM routinely features as a leading algorithm in machine learning competitions such as Kaggle and the KDDCup. This work proposes Accelerated Gradient Boosting Machine (AGBM) by incorporating Nesterov’s acceleration techniques into the design of GBM.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="780" height="343" src="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic5.png" alt="" class="wp-image-22444" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic5.png 780w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic5-150x66.png 150w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic5-300x132.png 300w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic5-768x338.png 768w" sizes="(max-width: 780px) 100vw, 780px"></figure></div>



<p><a href="https://arxiv.org/abs/1903.04361" target="_blank" rel="noreferrer noopener" aria-label="Solving the Black Box Problem: A General-Purpose Recipe for Explainable Artificial Intelligence (opens in a new tab)">Solving the Black Box Problem: A General-Purpose Recipe for Explainable Artificial Intelligence</a></p>



<p>Many of the computing systems developed using machine learning are opaque: it is difficult to explain why they do what they do, or how they work. The Explainable AI research program aims to develop analytic techniques for rendering such systems transparent, but lacks a general understanding of what it actually takes to do so. The aim of this paper is to provide a general-purpose recipe for Explainable AI: A series of steps that should be taken to render an opaque computing system transparent. </p>



<p><a href="https://arxiv.org/abs/1903.04192" target="_blank" rel="noreferrer noopener" aria-label="Accelerating Minibatch Stochastic Gradient Descent using Typicality Sampling (opens in a new tab)">Accelerating Minibatch Stochastic Gradient Descent using Typicality Sampling</a></p>



<p>Machine learning, especially deep neural networks, has been rapidly developed in fields including computer vision, speech recognition and reinforcement learning. Although Mini-batch SGD is one of the most popular stochastic optimization methods in training deep networks, it shows a slow convergence rate due to the large noise in gradient approximation. In this paper, we attempt to remedy this problem by building more efficient batch selection method based on typicality sampling, which reduces the error of gradient estimation in conventional Minibatch SGD.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="500" height="529" src="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic6.png" alt="" class="wp-image-22446" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic6.png 500w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic6-142x150.png 142w, https://insidebigdata.com/wp-content/uploads/2019/04/arXiv_Mar_pic6-284x300.png 284w" sizes="(max-width: 500px) 100vw, 500px"></figure></div>



<p><a href="https://arxiv.org/abs/1903.02831" target="_blank" rel="noreferrer noopener" aria-label="Predicting Research Trends From Arxiv (opens in a new tab)">Predicting Research Trends From Arxiv</a></p>



<p>Researchers perform trend detection on two data sets of Arxiv papers, derived from its machine learning (cs.LG) and natural language processing (cs.CL) categories. The approach is bottom-up: first rank papers by their normalized citation counts, then group top-ranked papers into different categories based on the tasks that they pursue and the methods they use. Then analyze these resulting topics. The paper describes that the dominating paradigm in cs.CL revolves around natural language generation problems and those in cs.LG revolve around reinforcement learning and adversarial principles. By extrapolation, it is predicted that these topics will remain lead problems/approaches in their fields in the short- and mid-term. </p>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-22682 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-artificial-intelligence tag-arxiv tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – April 2019</h1>
<div class="post-info"><span class="date published time" title="2019-05-22T08:30:28-07:00">May 22, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/05/22/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-april-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="198" height="167" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 198px) 100vw, 198px"></figure></div>







<p><a rel="noreferrer noopener" aria-label="Meta-Learning with Differentiable Convex Optimization (opens in a new tab)" href="https://arxiv.org/abs/1904.03758" target="_blank">Meta-Learning with Differentiable Convex Optimization</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Many meta-learning approaches for few-shot learning rely on simple base learners such as nearest-neighbor classifiers. However, even in the few-shot regime, discriminatively trained linear predictors can offer better generalization. This paper proposes to use these predictors as base learners to learn representations for few-shot learning and show they offer better trade-offs between feature size and performance across a range of few-shot recognition benchmarks. The objective is to learn feature embeddings that generalize well under a linear classification rule for novel categories. To efficiently solve the objective, the authors exploit two properties of linear classifiers: implicit differentiation of the optimality conditions of the convex problem and the dual formulation of the optimization problem. This allows the use of high-dimensional embeddings with improved generalization at a modest increase in computational overhead.  The code for this paper can be found <a href="https://github.com/kjunelee/MetaOptNet" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>.</p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="713" height="278" src="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic1.png" alt="" class="wp-image-22683" srcset="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic1.png 713w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic1-150x58.png 150w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic1-300x117.png 300w" sizes="(max-width: 713px) 100vw, 713px"><figcaption><br></figcaption></figure></div>



<p><a rel="noreferrer noopener" aria-label="Kervolutional Neural Networks (opens in a new tab)" href="https://arxiv.org/abs/1904.03955" target="_blank">Kervolutional Neural Networks</a></p>



<p>Convolutional neural networks (CNNs) have enabled the state-of-the-art performance in many computer vision tasks. However, little effort has been devoted to establishing convolution in non-linear space. Existing works mainly leverage on the activation layers, which can only provide point-wise non-linearity. To solve this problem, a new operation, kervolution (kernel convolution), is introduced to approximate complex behaviors of human perception systems leveraging on the kernel trick. It generalizes convolution, enhances the model capacity, and captures higher order interactions of features, via patch-wise kernel functions, but without introducing additional parameters.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="780" height="296" src="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic2.png" alt="" class="wp-image-22685" srcset="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic2.png 780w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic2-150x57.png 150w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic2-300x114.png 300w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic2-768x291.png 768w" sizes="(max-width: 780px) 100vw, 780px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Fashion++: Minimal Edits for Outfit Improvement (opens in a new tab)" href="https://arxiv.org/abs/1904.09261" target="_blank">Fashion++: Minimal Edits for Outfit Improvement</a></p>



<p>Given an outfit, what small changes would most improve its fashionability? This question presents an intriguing new vision challenge. This paper introduces Fashion++, an approach that proposes minimal adjustments to a full-body clothing outfit that will have maximal impact on its fashionability. The model consists of a deep image generation neural network that learns to synthesize clothing conditioned on learned per-garment encodings. The latent encodings are explicitly factorized according to shape and texture, thereby allowing direct edits for both fit/presentation and color/patterns/material, respectively. It is shown how to bootstrap Web photos to automatically train a fashionability model, and develop an activation maximization-style approach to transform the input image into its more fashionable self. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="588" height="380" src="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic3.png" alt="" class="wp-image-22686" srcset="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic3.png 588w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic3-150x97.png 150w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic3-300x194.png 300w" sizes="(max-width: 588px) 100vw, 588px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="MixMatch: A Holistic Approach to Semi-Supervised Learning (opens in a new tab)" href="https://arxiv.org/abs/1905.02249v1" target="_blank">MixMatch: A Holistic Approach to Semi-Supervised Learning</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. This paper unifies the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. It’s shown that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. The code for this paper can be found <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://github.com/google-research/mixmatch" target="_blank">HERE</a>.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="638" height="228" src="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic4.png" alt="" class="wp-image-22687" srcset="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic4.png 638w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic4-150x54.png 150w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic4-300x107.png 300w" sizes="(max-width: 638px) 100vw, 638px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Real numbers, data science and chaos: How to fit any dataset with a single parameter (opens in a new tab)" href="https://arxiv.org/abs/1904.12320v1" target="_blank">Real numbers, data science and chaos: How to fit any dataset with a single parameter</a></p>



<p>This paper shows how any data set of any modality (time-series, images, sound…) can be approximated by a well-behaved (continuous, differentiable…) scalar function with a single real-valued parameter. Building upon elementary concepts from chaos theory, the paper adopts a pedagogical approach demonstrating how to adjust this parameter in order to achieve arbitrary precision fit to all samples of the data. Targeting an audience of data scientists with a taste for the curious and unusual, the results presented here expand on previous similar observations regarding expressiveness power and generalization of machine learning models. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="782" height="297" src="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic5.png" alt="" class="wp-image-22689" srcset="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic5.png 782w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic5-150x57.png 150w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic5-300x114.png 300w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic5-768x292.png 768w" sizes="(max-width: 782px) 100vw, 782px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Text Classification Algorithms: A Survey (opens in a new tab)" href="https://arxiv.org/abs/1904.08067v3" target="_blank">Text Classification Algorithms: A Survey</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in the real-world problem are discussed. The code for this paper can be found <a href="https://github.com/kk7nc/Text_Classification" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">HERE</a>.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="682" height="193" src="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic6.png" alt="" class="wp-image-22691" srcset="https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic6.png 682w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic6-150x42.png 150w, https://insidebigdata.com/wp-content/uploads/2019/05/arXiv_Apr19_pic6-300x85.png 300w" sizes="(max-width: 682px) 100vw, 682px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-22845 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-artificial-intelligence tag-arxiv tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – May 2019</h1>
<div class="post-info"><span class="date published time" title="2019-06-26T08:00:25-07:00">June 26, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/06/26/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-may-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="214" height="181" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 214px) 100vw, 214px"></figure></div>







<p><a rel="noreferrer noopener" aria-label="Luck Matters: Understanding Training Dynamics of Deep ReLU Networks (opens in a new tab)" href="https://arxiv.org/pdf/1905.13405.pdf" target="_blank">Luck Matters: Understanding Training Dynamics of Deep ReLU Networks</a></p>



<p>This paper analyzes the dynamics of training deep ReLU networks and their implications on generalization capability. Using a teacher-student setting, the researchers discovered a novel relationship between the gradient received by hidden student nodes and the activations of teacher nodes for deep ReLU networks. With this relationship and the assumption of small overlapping teacher node activations, it is proven that (1) student nodes whose weights are initialized to be close to teacher nodes converge to them at a faster rate, and (2) in over-parameterized regimes and 2-layer case, while a small set of lucky nodes do converge to the teacher nodes, the fan-out weights of other nodes converge to zero. This framework provides insight into multiple puzzling phenomena in deep learning like over-parameterization, implicit regularization, lottery tickets, etc. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="630" height="202" src="https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic1.png" alt="" class="wp-image-22857" srcset="https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic1.png 630w, https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic1-150x48.png 150w, https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic1-300x96.png 300w" sizes="(max-width: 630px) 100vw, 630px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Deep Convolutional Networks as shallow Gaussian Processes (opens in a new tab)" href="https://arxiv.org/pdf/1808.05587.pdf" target="_blank">Deep Convolutional Networks as shallow Gaussian Processes</a></p>



<p>This paper shows that the output of a (residual) convolutional neural network (CNN) with an appropriate prior over the weights and biases is a Gaussian process (GP) in the limit of infinitely many convolutional filters, extending similar results for dense networks. For a CNN, the equivalent kernel can be computed exactly and, unlike “deep kernels”, has very few parameters: only the hyperparameters of the original CNN. Further, the paper shows that this kernel has two properties that allow it to be computed efficiently; the cost of evaluating the kernel for a pair of images is similar to a single forward pass through the original CNN with only one filter per layer. The kernel equivalent to a 32-layer ResNet obtains 0.84% classification error on MNIST, a new record for GPs with a comparable number of parameters. The code for this paper can be found HERE.</p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="665" height="377" src="https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic2.png" alt="" class="wp-image-22858" srcset="https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic2.png 665w, https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic2-150x85.png 150w, https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic2-300x170.png 300w" sizes="(max-width: 665px) 100vw, 665px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Pre-training Graph Neural Networks (opens in a new tab)" href="https://arxiv.org/pdf/1905.12265.pdf" target="_blank">Pre-training Graph Neural Networks</a></p>



<p>Many applications of machine learning in science and medicine, including molecular property and protein function prediction, can be cast as problems of predicting some properties of graphs, where having good graph representations is critical. However, two key challenges in these domains are (1) extreme scarcity of labeled data due to expensive lab experiments, and (2) needing to extrapolate to test graphs that are structurally different from those seen during training. This paper explores pre-training to address both of these challenges. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="658" height="322" src="https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic3.png" alt="" class="wp-image-22860" srcset="https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic3.png 658w, https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic3-150x73.png 150w, https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic3-300x147.png 300w" sizes="(max-width: 658px) 100vw, 658px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Speech2Face: Learning the Face Behind a Voice (opens in a new tab)" href="https://arxiv.org/pdf/1905.09773.pdf" target="_blank">Speech2Face: Learning the Face Behind a Voice</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>How much can we infer about a person’s looks from the way they speak? This paper studies the task of reconstructing a facial image of a person from a short audio recording of that person speaking. The researchers design and train a deep neural network to perform this task using millions of natural Internet/YouTube videos of people speaking. During training, our model learns voice-face correlations that allow it to produce images that capture various physical attributes of the speakers such as age, gender and ethnicity.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="391" height="543" src="https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic4.png" alt="" class="wp-image-22861" srcset="https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic4.png 391w, https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic4-108x150.png 108w, https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic4-216x300.png 216w" sizes="(max-width: 391px) 100vw, 391px"></figure></div>



<p><a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noreferrer noopener" aria-label="Distilling the Knowledge in a Neural Network (opens in a new tab)">Distilling the Knowledge in a Neural Network</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and this paper develops this approach further using a different compression technique. The reseachers achieve some surprising results on MNIST and show how to significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. The paper also introduces a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel. </p>



<p><a rel="noreferrer noopener" aria-label="MixMatch: A Holistic Approach to Semi-Supervised Learning (opens in a new tab)" href="https://arxiv.org/pdf/1905.02249.pdf" target="_blank">MixMatch: A Holistic Approach to Semi-Supervised Learning</a></p>



<p>Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="646" height="242" src="https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic5.png" alt="" class="wp-image-22862" srcset="https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic5.png 646w, https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic5-150x56.png 150w, https://insidebigdata.com/wp-content/uploads/2019/06/arXiv_May19_pic5-300x112.png 300w" sizes="(max-width: 646px) 100vw, 646px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-22953 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-storage category-featured category-google-news-feed category-machine-learning category-news-analysis category-uncategorized tag-ai tag-artificial-intelligence tag-arxiv tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – June 2019</h1>
<div class="post-info"><span class="date published time" title="2019-07-18T08:30:07-07:00">July 18, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/07/18/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-june-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="223" height="188" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 223px) 100vw, 223px"></figure></div>







<p><a rel="noreferrer noopener" aria-label="Monte Carlo Gradient Estimation in Machine Learning (opens in a new tab)" href="https://arxiv.org/pdf/1906.10652" target="_blank">Monte Carlo Gradient Estimation in Machine Learning</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. The Google researchers generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analyzed. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="711" height="255" src="https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic1.png" alt="" class="wp-image-22958" srcset="https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic1.png 711w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic1-150x54.png 150w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic1-300x108.png 300w" sizes="(max-width: 711px) 100vw, 711px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="An Introduction to Variational Autoencoders (opens in a new tab)" href="https://arxiv.org/pdf/1906.02691v1.pdf" target="_blank">An Introduction to Variational Autoencoders</a></p>



<p>Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. This paper provides an introduction to variational autoencoders and some important extensions. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="436" height="582" src="https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic2.png" alt="" class="wp-image-22959" srcset="https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic2.png 436w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic2-112x150.png 112w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic2-225x300.png 225w" sizes="(max-width: 436px) 100vw, 436px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Generative Adversarial Networks: A Survey and Taxonomy (opens in a new tab)" href="https://arxiv.org/pdf/1906.01529v1.pdf" target="_blank">Generative Adversarial Networks: A Survey and Taxonomy</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably the revolutionary techniques are in the area of computer vision such as plausible image generation, image to image translation, facial attribute manipulation and similar domains. Despite the significant success achieved in computer vision field, applying GANs over real-world problems still have three main challenges: (1) High quality image generation; (2) Diverse image generation; and (3) Stable training. Considering numerous GAN-related research in the literature, this paper provides a study on the architecture-variants and loss-variants, which are proposed to handle these three challenges from two perspectives. The authors propose loss and architecture-variants for classifying most popular GANs, and discuss the potential improvements with focusing on these two aspects. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="824" height="409" src="https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic3.png" alt="" class="wp-image-22961" srcset="https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic3.png 824w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic3-150x74.png 150w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic3-300x149.png 300w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic3-768x381.png 768w" sizes="(max-width: 824px) 100vw, 824px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Learning Causal State Representations of Partially Observable Environments (opens in a new tab)" href="https://arxiv.org/pdf/1906.10437.pdf" target="_blank">Learning Causal State Representations of Partially Observable Environments</a></p>



<p>Intelligent agents can cope with sensory-rich environments by learning task-agnostic state abstractions. This paper proposes mechanisms to approximate causal states, which optimally compress the joint history of actions and observations in partially-observable Markov decision processes. The proposed algorithm extracts causal state representations from RNNs that are trained to predict subsequent observations given the history. The authors demonstrate that these learned task-agnostic state abstractions can be used to efficiently learn policies for reinforcement learning problems with rich observation spaces. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="402" height="362" src="https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic4.png" alt="" class="wp-image-22963" srcset="https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic4.png 402w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic4-150x135.png 150w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic4-300x270.png 300w" sizes="(max-width: 402px) 100vw, 402px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="The Functional Neural Process (opens in a new tab)" href="https://arxiv.org/pdf/1906.08324.pdf" target="_blank">The Functional Neural Process</a></p>



<p>This paper presents a new family of exchangeable stochastic processes, the Functional Neural Processes (FNPs). FNPs model distributions over functions by learning a graph of dependencies on top of latent representations of the points in the given data set. In doing so, they define a Bayesian model without explicitly positing a prior distribution over latent global parameters; they instead adopt priors over the relational structure of the given data set, a task that is much simpler. The authors show how we can learn such models from data, demonstrate that they are scalable to large data sets through mini-batch optimization and describe how we can make predictions for new points via their posterior predictive distribution.</p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="547" height="473" src="https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic5.png" alt="" class="wp-image-22965" srcset="https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic5.png 547w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic5-150x130.png 150w, https://insidebigdata.com/wp-content/uploads/2019/07/arXiv_Jun_pic5-300x259.png 300w" sizes="(max-width: 547px) 100vw, 547px"></figure></div>



<p> </p>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-23163 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-artificial-intelligence tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – July 2019</h1>
<div class="post-info"><span class="date published time" title="2019-08-28T08:30:46-07:00">August 28, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/08/28/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-july-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="205" height="173" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 205px) 100vw, 205px"></figure></div>







<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p><a href="https://arxiv.org/pdf/1907.04840.pdf" target="_blank" rel="noreferrer noopener" aria-label="Sparse Networks from Scratch: Faster Training without Losing Performance (opens in a new tab)">Sparse Networks from Scratch: Faster Training without Losing Performance</a> </p>



<p>This paper demonstrates the possibility of what is called <em>sparse learning</em>: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving dense performance levels. This paper accomplishs this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. The paper demonstrates state-of-the-art sparse performance on MNIST, CIFAR-10, and ImageNet, decreasing the mean error by a relative 8%, 15%, and 6% compared to other sparse algorithms.  The PyTorch code for this paper can be found <a href="https://github.com/TimDettmers/sparse_learning" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>.</p>



<figure class="wp-block-image"><img loading="lazy" width="644" height="408" src="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic2.png" alt="" class="wp-image-23166" srcset="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic2.png 644w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic2-150x95.png 150w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic2-300x190.png 300w" sizes="(max-width: 644px) 100vw, 644px"></figure>



<p><a rel="noreferrer noopener" aria-label="Trading via Image Classification (opens in a new tab)" href="https://arxiv.org/pdf/1907.10046v1.pdf" target="_blank">Trading via Image Classification</a></p>



<p>The art of systematic financial trading evolved with an array of approaches, ranging from simple strategies to complex algorithms all relying, primary, on aspects of time-series analysis. Recently, after visiting the trading floor of a leading financial institution, AI researchers from J.P. Morgan noticed that traders always execute their trade orders while observing images of financial time-series on their screens. This work is built upon the success in image recognition and examine the value in transforming the traditional time-series analysis to that of image classification. The researchers created a large sample of financial time-series images encoded as candlestick (Box and Whisker) charts and label the samples following three algebraically-defined binary trade strategies. Using the images, the researchers trained over a dozen machine-learning classification models and find that the algorithms are very efficient in recovering the complicated, multiscale label-generating rules when the data is represented visually.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="624" height="403" src="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic3.png" alt="" class="wp-image-23168" srcset="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic3.png 624w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic3-150x97.png 150w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic3-300x194.png 300w" sizes="(max-width: 624px) 100vw, 624px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods (opens in a new tab)" href="https://arxiv.org/pdf/1907.09358v1.pdf" target="_blank">Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Integration of vision and language tasks has seen a significant growth in the recent times due to surge of interest from multi-disciplinary communities such as deep learning, computer vision, and natural language processing.  This survey focuses on ten different vision and language integration tasks in terms of their problem formulation, methods, existing datasets, evaluation measures, and comparison of results achieved with the corresponding state-of-the-art methods. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="815" height="409" src="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic4.png" alt="" class="wp-image-23169" srcset="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic4.png 815w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic4-150x75.png 150w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic4-300x151.png 300w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic4-768x385.png 768w" sizes="(max-width: 815px) 100vw, 815px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Where is the Information in a Deep Neural Network? (opens in a new tab)" href="https://arxiv.org/pdf/1905.12213.pdf" target="_blank">Where is the Information in a Deep Neural Network?</a></p>



<p>Whatever information a Deep Neural Network has gleaned from past data is encoded in its weights. How this information affects the response of the network to future data is largely an open question. In fact, even how to define and measure information in a network is still not settled. This paper introduces the notion of Information in the Weights as the optimal trade-off between accuracy of the network and complexity of the weights, relative to a prior. Depending on the prior, the definition reduces to known information measures such as Shannon Mutual Information and Fisher Information, but affords added flexibility that enables us to relate it to generalization, via the PAC-Bayes bound, and to invariance. This relation hinges not only on the architecture of the model, but surprisingly on how it is trained. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="630" height="323" src="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic5.png" alt="" class="wp-image-23171" srcset="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic5.png 630w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic5-150x77.png 150w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic5-300x154.png 300w" sizes="(max-width: 630px) 100vw, 630px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="A Selective Overview of Deep Learning (opens in a new tab)" href="https://arxiv.org/pdf/1904.05526.pdf" target="_blank">A Selective Overview of Deep Learning</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Deep learning has arguably achieved tremendous success in recent years. In simple words, deep learning uses the composition of many nonlinear functions to model the complex dependency between input features and labels. While neural networks have a long history, recent advances have greatly improved their performance in computer vision, natural language processing, etc. From the statistical and scientific perspective, it is natural to ask: What is deep learning? What are the new characteristics of deep learning, compared with classical methods? What are the theoretical foundations of deep learning? To answer these questions, this paper introduces common neural network models (e.g., convolutional neural nets, recurrent neural nets, generative adversarial nets) and training techniques (e.g., stochastic gradient descent, dropout, batch normalization) from a statistical point of view. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="767" height="426" src="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic6.png" alt="" class="wp-image-23173" srcset="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic6.png 767w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic6-150x83.png 150w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic6-300x167.png 300w" sizes="(max-width: 767px) 100vw, 767px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Augmenting Self-attention with Persistent Memory (opens in a new tab)" href="https://arxiv.org/pdf/1907.01470.pdf" target="_blank">Augmenting Self-attention with Persistent Memory</a></p>



<p>Transformer networks have lead to important progress in language modeling and machine translation. These models include two consecutive modules, a feed-forward layer and a self-attention layer. The latter allows the network to capture long term dependencies and are often regarded as the key ingredient in the success of Transformers. Building upon this intuition, this paper proposes a new model that solely consists of attention layers. More precisely, the Facebook AI researchers augment the self-attention layers with persistent memory vectors that play a similar role as the feed-forward layer. Thanks to these vectors, we can remove the feed-forward layer without degrading the performance of a transformer. The evaluation shows the benefits brought by our model on standard character and word level language modeling benchmarks. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="652" height="373" src="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic7.png" alt="" class="wp-image-23174" srcset="https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic7.png 652w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic7-150x86.png 150w, https://insidebigdata.com/wp-content/uploads/2019/08/arXiv_Jul19_pic7-300x172.png 300w" sizes="(max-width: 652px) 100vw, 652px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em>    </p>
</div></div><!-- end .after-post-ad --></div><div class="post-23280 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – August 2019</h1>
<div class="post-info"><span class="date published time" title="2019-09-18T08:30:52-07:00">September 18, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/09/18/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-august-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="227" height="192" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 227px) 100vw, 227px"></figure></div>







<p><a rel="noreferrer noopener" aria-label="A Probabilistic Representation of Deep Learning (opens in a new tab)" href="https://arxiv.org/pdf/1908.09772v1.pdf" target="_blank">A Probabilistic Representation of Deep Learning</a></p>



<p>This paper introduces a novel probabilistic representation of deep learning, which provides an explicit explanation for the Deep Neural Networks (DNNs) in three aspects: (i) neurons define the energy of a Gibbs distribution; (ii) the hidden layers of DNNs formulate Gibbs distributions; and (iii) the whole architecture of DNNs can be interpreted as a Bayesian neural network. Based on the proposed probabilistic representation, we investigate two fundamental properties of deep learning: hierarchy and generalization.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="579" height="312" src="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic1.png" alt="" class="wp-image-23284" srcset="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic1.png 579w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic1-150x81.png 150w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic1-300x162.png 300w" sizes="(max-width: 579px) 100vw, 579px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Inception-inspired LSTM for Next-frame Video Prediction (opens in a new tab)" href="https://arxiv.org/pdf/1909.05622.pdf" target="_blank">Inception-inspired LSTM for Next-frame Video Prediction</a>     </p>



<p>The problem of video frame prediction has received much interest  due to its relevance to many computer vision applications such as  autonomous vehicles or robotics. Supervised methods for video frame  prediction rely on labeled data, which may not always be available. This paper provides a novel unsupervised deep-learning method called  Inception-based LSTM for video frame prediction. The general idea of  inception networks is to implement wider networks instead of deeper  networks. This network design was shown to improve the performance of  image classification.    </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="608" height="443" src="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic2.png" alt="" class="wp-image-23286" srcset="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic2.png 608w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic2-150x109.png 150w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic2-300x219.png 300w" sizes="(max-width: 608px) 100vw, 608px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Meta-Learning with Warped Gradient Descent (opens in a new tab)" href="https://arxiv.org/pdf/1909.00025.pdf" target="_blank">Meta-Learning with Warped Gradient Descent</a>   </p>



<p>A versatile and effective approach to meta-learning is to infer a gradient-based up-date rule directly from data that promotes rapid  learning of new tasks from the same distribution. Current methods rely  on backpropagating through the learning process, limiting their scope to  few-shot learning. This paper introduces Warped Gradient Descent  (WarpGrad), a family of modular optimisers that can scale to arbitrary  adaptation processes. WarpGrad methods meta-learn to warp task loss  surfaces across the joint task-parameter distribution to facilitate  gradient descent, which is achieved by a reparametrisation of neural  networks that interleaves warp layers in the architecture.     </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="681" height="415" src="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic3.png" alt="" class="wp-image-23287" srcset="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic3.png 681w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic3-150x91.png 150w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic3-300x183.png 300w" sizes="(max-width: 681px) 100vw, 681px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Systematic Analysis of Image Generation using GANs (opens in a new tab)" href="https://arxiv.org/ftp/arxiv/papers/1908/1908.11863.pdf" target="_blank">Systematic Analysis of Image Generation using GANs</a>       </p>



<p>Generative Adversarial Networks have been crucial in the  developments made in unsupervised learning in recent times. Exemplars of  image synthesis from text or other images, these networks have shown  remarkable improvements over conventional methods in terms of  performance. Trained on the adversarial training philosophy, these  networks aim to estimate the potential distribution from the real data  and then use this as input to generate the synthetic data. Based on this  fundamental principle, several frameworks can be generated that are  paragon implementations in several real-life applications such as art  synthesis, generation of high resolution outputs and synthesis of images  from human drawn sketches, to name a few. While theoretically GANs  present better results and prove to be an improvement over conventional  methods in many factors, the implementation of these frameworks for  dedicated applications remains a challenge. This study explores and  presents a taxonomy of these frameworks and their use in various image  to image synthesis and text to image synthesis applications.       </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="530" height="272" src="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic4.png" alt="" class="wp-image-23289" srcset="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic4.png 530w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic4-150x77.png 150w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic4-300x154.png 300w" sizes="(max-width: 530px) 100vw, 530px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Dynamic Stale Synchronous Parallel Distributed Training for Deep Learning (opens in a new tab)" href="https://arxiv.org/pdf/1908.11848.pdf" target="_blank">Dynamic Stale Synchronous Parallel Distributed Training for Deep Learning</a></p>



<p>Deep learning is a popular machine learning technique and has  been applied to many real-world problems. However, training a deep  neural network is very time-consuming, especially on big data. It has  become difficult for a single machine to train a large model over large  datasets. A popular solution is to distribute and parallelize the  training process across multiple machines using the parameter server  framework. In this paper, we present a distributed paradigm on the  parameter server framework called Dynamic Stale Synchronous Parallel  (DSSP) which improves the state-of-the-art Stale Synchronous Parallel  (SSP) paradigm by dynamically determining the staleness threshold at the  run time.      </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="649" height="485" src="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic5.png" alt="" class="wp-image-23291" srcset="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic5.png 649w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic5-150x112.png 150w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic5-300x224.png 300w" sizes="(max-width: 649px) 100vw, 649px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Discovering Reliable Correlations in Categorical Data (opens in a new tab)" href="https://arxiv.org/pdf/1908.11682.pdf" target="_blank">Discovering Reliable Correlations in Categorical Data</a>      </p>



<p>In many scientific tasks we are interested in discovering  whether there exist any correlations in our data. This raises many  questions, such as how to reliably and interpretably measure correlation  between a multivariate set of attributes, how to do so without having  to make assumptions on distribution of the data or the type of  correlation, and, how to efficiently discover the top-most reliably  correlated attribute sets from data. This paper answers these  questions for discovery tasks in categorical data. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="497" height="512" src="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic6.png" alt="" class="wp-image-23293" srcset="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic6.png 497w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic6-146x150.png 146w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic6-291x300.png 291w" sizes="(max-width: 497px) 100vw, 497px"></figure></div>



<p>         </p>



<p><a rel="noreferrer noopener" aria-label="Smaller Models, Better Generalization (opens in a new tab)" href="https://arxiv.org/pdf/1908.11250.pdf" target="_blank">Smaller Models, Better Generalization</a>     </p>



<p>Reducing network complexity has been a major research focus in  recent years with the advent of mobile technology. Convolutional Neural  Networks that perform various vision tasks without memory overhaul is  the need of the hour. This paper focuses on qualitative and quantitative  analysis of reducing the network complexity using an upper bound on the  Vapnik-Chervonenkis dimension, pruning, and quantization. The researchers observe a  general trend in improvement of accuracies as the models are quantize. The paper proposes a novel loss function that helps in achieving considerable  sparsity at comparable accuracies to that of dense models. The paper compares various regularizations prevalent in the literature and shows the  superiority of the proposed method in achieving sparser models that generalize  well.  </p>



<p><a rel="noreferrer noopener" aria-label="An Auto-ML Framework Based on GBDT for Lifelong Learning (opens in a new tab)" href="https://arxiv.org/pdf/1908.11033.pdf" target="_blank">An Auto-ML Framework Based on GBDT for Lifelong Learning</a>       </p>



<p>Automatic  Machine Learning (Auto-ML) has attracted more and more attention in  recent years. This paper works to solve the problem of data drift, which  means that the distribution of data will gradually change with the  acquisition process, resulting in a worse performance of the auto-ML  model. The researchers construct their model based on GBDT. Incremental learning and  full learning are used to handle with drift problem. Experiments show  that the proposed method performs well on the five data sets. This shows that the method can effectively solve the problem of data drift and has  robust performance.       </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="868" height="416" src="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic7.png" alt="" class="wp-image-23295" srcset="https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic7.png 868w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic7-150x72.png 150w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic7-300x144.png 300w, https://insidebigdata.com/wp-content/uploads/2019/09/arXiv_2019_Aug_pic7-768x368.png 768w" sizes="(max-width: 868px) 100vw, 868px"></figure></div>



<p>       </p>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-23433 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-data-science-topics category-google-news-feed category-machine-learning category-main-feature category-news-analysis category-research-reports category-uncategorized tag-ai tag-artificial-intelligence tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – September 2019</h1>
<div class="post-info"><span class="date published time" title="2019-10-16T08:30:52-07:00">October 16, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/10/16/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-september-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="259" height="219" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 259px) 100vw, 259px"></figure></div>







<p><a rel="noreferrer noopener" aria-label="rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch (opens in a new tab)" href="https://arxiv.org/pdf/1909.01500.pdf" target="_blank">rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch</a></p>



<p>Since the recent advent of deep reinforcement learning for game play and simulated robotic control, a multitude of new algorithms have flourished. Most are model-free algorithms which can be categorized into three families: deep Q-learning, policy gradients, and Q-value policy gradients. These have developed along separate lines of research, such that few, if any, code bases incorporate all three kinds. Yet these algorithms share a great depth of common deep reinforcement learning machinery. We are pleased to share rlpyt, which implements all three algorithm families on top of a shared, optimized infrastructure, in a single repository. It contains modular implementations of many common deep RL algorithms in Python using PyTorch, a leading deep learning library. rlpyt is designed as a high-throughput code base for small- to medium-scale research in deep RL. This paper summarizes its features, algorithms implemented, and relation to prior work, and concludes with detailed implementation and usage notes. rlpyt can be found on <a rel="noreferrer noopener" aria-label="GitHub (opens in a new tab)" href="https://github.com/astooke/rlpyt" target="_blank">GitHub</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="697" height="306" src="https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic1.png" alt="" class="wp-image-23434" srcset="https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic1.png 697w, https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic1-150x66.png 150w, https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic1-300x132.png 300w" sizes="(max-width: 697px) 100vw, 697px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="SoftTriple Loss: Deep Metric Learning Without Triplet Sampling (opens in a new tab)" href="https://arxiv.org/pdf/1909.05235.pdf" target="_blank">SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</a></p>



<p>Distance metric learning (DML) is to learn the embeddings where examples from the same class are closer than examples from different classes. It can be cast as an optimization problem with triplet constraints. Due to the vast number of triplet constraints, a sampling strategy is essential for DML. With the tremendous success of deep learning in classifications, it has been applied for DML. When learning embeddings with deep neural networks (DNNs), only a mini-batch of data is available at each iteration. The set of triplet constraints has to be sampled within the mini-batch. Since a mini-batch cannot capture the neighbors in the original set well, it makes the learned embeddings sub-optimal. On the contrary, optimizing SoftMax loss, which is a classification loss, with DNN shows a superior performance in certain DML tasks. This paper investigates the formulation of SoftMax. The analysis shows that SoftMax loss is equivalent to a smoothed triplet loss where each class has a single center. In real-world data, one class can contain several local clusters rather than a single one, e.g., birds of different poses. Therefore, this paper proposes the SoftTriple loss to extend the SoftMax loss with multiple centers for each class.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="553" height="489" src="https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic2.png" alt="" class="wp-image-23435" srcset="https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic2.png 553w, https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic2-150x133.png 150w, https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic2-300x265.png 300w" sizes="(max-width: 553px) 100vw, 553px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Distributed Machine Learning on Mobile Devices: A Survey (opens in a new tab)" href="https://arxiv.org/pdf/1909.08329v1.pdf" target="_blank">Distributed Machine Learning on Mobile Devices: A Survey</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>In recent years, mobile devices have gained increasingly development with stronger computation capability and larger storage. Some of the computation-intensive machine learning and deep learning tasks can now be run on mobile devices. To take advantage of the resources available on mobile devices and preserve users’ privacy, the idea of mobile distributed machine learning is proposed. It uses local hardware resources and local data to solve machine learning sub-problems on mobile devices, and only uploads computation results instead of original data to contribute to the optimization of the global model. This architecture can not only relieve computation and storage burden on servers, but also protect the users’ sensitive information. Another benefit is the bandwidth reduction, as various kinds of local data can now participate in the training process without being uploaded to the server. This paper provides a comprehensive survey on recent studies of mobile distributed machine learning and surveys a number of widely-used mobile distributed machine learning methods.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="654" height="304" src="https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic3.png" alt="" class="wp-image-23436" srcset="https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic3.png 654w, https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic3-150x70.png 150w, https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic3-300x139.png 300w" sizes="(max-width: 654px) 100vw, 654px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Espresso: A Fast End-to-end Neural Speech Recognition Toolkit (opens in a new tab)" href="https://arxiv.org/pdf/1909.08723v2.pdf" target="_blank">Espresso: A Fast End-to-end Neural Speech Recognition Toolkit</a></p>



<p>This paper presents Espresso, an open-source, modular, extensible end-to-end  neural automatic speech recognition (ASR) toolkit based on the deep  learning library PyTorch and the popular neural machine translation  toolkit fairseq. Espresso supports distributed training across GPUs and  computing nodes, and features various decoding approaches commonly  employed in ASR, including look-ahead word-based language model fusion,  for which a fast, parallelized decoder is implemented. Espresso achieves  state-of-the-art ASR performance on the WSJ, LibriSpeech, and  Switchboard data sets among other end-to-end systems without data  augmentation, and is 4–11x faster for decoding than similar systems  (e.g. ESPnet). Espresso can be found on <a rel="noreferrer noopener" aria-label="GitHub (opens in a new tab)" href="https://github.com/freewym/espresso" target="_blank">GitHub</a>.  </p>



<p><a href="https://arxiv.org/pdf/1909.06654v1.pdf" target="_blank" rel="noreferrer noopener" aria-label="MUSICNN: Pre-trained Convolutional Neural Networks for Music Audio Tagging (opens in a new tab)">MUSICNN: Pre-trained Convolutional Neural Networks for Music Audio Tagging</a> </p>



<p>Pronounced as “musician”, the musicnn library contains a set of  pre-trained musically motivated convolutional neural networks for music  audio tagging.  The repository also includes some pre-trained vgg-like baselines. These models can  be used as out-of-the-box music audio taggers, as music feature  extractors, or as pre-trained models for transfer learning. musiccn can be found on <a rel="noreferrer noopener" aria-label="GitHub (opens in a new tab)" href="https://github.com/jordipons/musicnn" target="_blank">GitHub</a>.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="682" height="127" src="https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic5.png" alt="" class="wp-image-23437" srcset="https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic5.png 682w, https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic5-150x28.png 150w, https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic5-300x56.png 300w" sizes="(max-width: 682px) 100vw, 682px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="DeepPrivacy: A Generative Adversarial Network for Face Anonymization (opens in a new tab)" href="https://arxiv.org/pdf/1909.04538v1.pdf" target="_blank">DeepPrivacy: A Generative Adversarial Network for Face Anonymization</a></p>



<p>This paper proposes a novel architecture which is able to automatically anonymize  faces in images while retaining the original data distribution. We  ensure total anonymization of all faces in an image by generating images  exclusively on privacy-safe information. The model is  based on a conditional generative adversarial network, generating images  considering the original pose and image background. The conditional  information enables the method to generate highly realistic faces with a  seamless transition between the generated face and the existing  background. DeepPrivacy can be found on <a rel="noreferrer noopener" aria-label="GitHub (opens in a new tab)" href="https://github.com/hukkelas/DeepPrivacy" target="_blank">GitHub</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="683" height="287" src="https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic6.png" alt="" class="wp-image-23439" srcset="https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic6.png 683w, https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic6-150x63.png 150w, https://insidebigdata.com/wp-content/uploads/2019/10/arXiv_2019_Sept_pic6-300x126.png 300w" sizes="(max-width: 683px) 100vw, 683px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-23542 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – October 2019</h1>
<div class="post-info"><span class="date published time" title="2019-11-21T08:00:52-08:00">November 21, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/11/21/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-october-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="247" height="209" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 247px) 100vw, 247px"></figure></div>







<p><a rel="noreferrer noopener" aria-label="NGBoost: Natural Gradient Boosting for Probabilistic Prediction (opens in a new tab)" href="https://arxiv.org/pdf/1910.03225.pdf" target="_blank">NGBoost: Natural Gradient Boosting for Probabilistic Prediction</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>The Stanford ML Group presents the Natural Gradient Boosting (NGBoost), an algorithm which brings probabilistic prediction capability to gradient boosting in a generic way. Predictive uncertainty estimation is crucial in many applications such as healthcare and weather forecasting. Probabilistic prediction, which is the approach where the model outputs a full probability distribution over the entire outcome space, is a natural way to quantify those uncertainties. Gradient Boosting Machines have been widely successful in prediction tasks on structured input data, but a simple boosting solution for probabilistic prediction of real valued outputs is yet to be made. NGBoost is a gradient boosting approach which uses the <em>Natural Gradient </em>to address technical challenges that makes generic probabilistic prediction hard with existing gradient boosting methods. This approach is modular with respect to the choice of base learner, probability distribution, and scoring rule. The paper shows empirically on several regression data sets that NGBoost provides competitive predictive performance of both uncertainty estimates and traditional metrics. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="477" height="506" src="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct1.png" alt="" class="wp-image-23543" srcset="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct1.png 477w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct1-141x150.png 141w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct1-283x300.png 283w" sizes="(max-width: 477px) 100vw, 477px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Unsupervised Cross-lingual Representation Learning at Scale (opens in a new tab)" href="https://arxiv.org/pdf/1911.02116.pdf" target="_blank">Unsupervised Cross-lingual Representation Learning at Scale</a> </p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. The Facebook AI researchers trained a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. The model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="888" height="325" src="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct2.png" alt="" class="wp-image-23554" srcset="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct2.png 888w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct2-150x55.png 150w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct2-300x110.png 300w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct2-768x281.png 768w" sizes="(max-width: 888px) 100vw, 888px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Confident Learning: Estimating Uncertainty in Dataset Labels (opens in a new tab)" href="https://arxiv.org/pdf/1911.00068v1.pdf" target="_blank">Confident Learning: Estimating Uncertainty in Dataset Labels</a></p>



<p>Learning exists in the context of data, yet notions of confidence  typically focus on model predictions, not label quality. Confident  learning (CL) has emerged as an approach for characterizing,  identifying, and learning with noisy labels in data sets, based on the  principles of pruning noisy data, counting to estimate noise, and  ranking examples to train with confidence.              This paper generalizes CL, building on the assumption of a classification noise  process, to directly estimate the joint distribution between noisy  (given) labels and uncorrupted (unknown) labels. This generalized CL,  open-sourced as cleanlab,  is provably consistent under reasonable conditions, and experimentally  performant on ImageNet and CIFAR, outperforming recent approaches, e.g.  MentorNet, by 30% or more, when label noise is non-uniform. cleanlab also quantifies ontological class overlap, and can increase model accuracy (e.g. ResNet) by providing clean data for training. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="683" height="497" src="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct3.png" alt="" class="wp-image-23592" srcset="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct3.png 683w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct3-150x109.png 150w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct3-300x218.png 300w" sizes="(max-width: 683px) 100vw, 683px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Convolutional Character Networks (opens in a new tab)" href="https://arxiv.org/pdf/1910.07954v1.pdf" target="_blank">Convolutional Character Networks</a></p>



<p>Recent progress has been made on developing a unified framework for  joint text detection and recognition in natural images, but existing  joint models were mostly built on two-stage framework by involving ROI  pooling, which can degrade the performance on recognition task. This proposes convolutional character networks, referred as CharNet,  which is an one-stage model that can process two tasks simultaneously in  one pass. CharNet directly  outputs bounding boxes of words and characters, with corresponding  character labels. The technology utilizes character as basic element, allowing us to  overcome the main difficulty of existing approaches that attempted to  optimize text detection jointly with a RNN-based recognition branch. In  addition, the paper develops an iterative character detection approach able to  transform the ability of character detection learned from synthetic data  to real-world images. These technical improvements result in a simple,  compact, yet powerful one-stage model that works reliably on  multi-orientation and curved text. Code is available <a rel="noreferrer noopener" aria-label="HERE (opens in a new tab)" href="https://github.com/MalongTech/research-charnet" target="_blank">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="958" height="355" src="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct4.png" alt="" class="wp-image-23593" srcset="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct4.png 958w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct4-150x56.png 150w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct4-300x111.png 300w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct4-768x285.png 768w" sizes="(max-width: 958px) 100vw, 958px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="BoTorch: Programmable Bayesian Optimization in PyTorch (opens in a new tab)" href="https://arxiv.org/pdf/1910.06403v1.pdf" target="_blank">BoTorch: Programmable Bayesian Optimization in PyTorch</a></p>



<p>Bayesian optimization provides sample-efficient global optimization for a  broad range of applications, including automatic machine learning,  molecular chemistry, and experimental design. This paper introduces BoTorch, a  modern programming framework for Bayesian optimization.              Enabled by  Monte-Carlo (MC) acquisition functions and auto-differentiation,  BoTorch’s modular design facilitates flexible specification and  optimization of probabilistic models written in PyTorch, radically  simplifying implementation of novel acquisition functions. The MC  approach is made practical by a distinctive algorithmic foundation that  leverages fast predictive distributions and hardware acceleration.   Experiments demonstrates the improved sample efficiency of BoTorch  relative to other popular libraries. BoTorch is open source and  available at <a rel="noreferrer noopener" aria-label="HERE (opens in a new tab)" href="https://github.com/pytorch/botorch" target="_blank">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="956" height="208" src="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct5.png" alt="" class="wp-image-23594" srcset="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct5.png 956w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct5-150x33.png 150w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct5-300x65.png 300w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct5-768x167.png 768w" sizes="(max-width: 956px) 100vw, 956px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="RLCard: A Toolkit for Reinforcement Learning in Card Games (opens in a new tab)" href="https://arxiv.org/pdf/1910.04376v1.pdf" target="_blank">RLCard: A Toolkit for Reinforcement Learning in Card Games</a>     </p>



<p>RLCard is an open-source toolkit for reinforcement learning research in  card games. It supports various card environments with easy-to-use  interfaces, including Blackjack, Leduc Hold’em, Texas Hold’em, UNO, Dou  Dizhu and Mahjong. The goal of  RLCard is to bridge reinforcement learning and imperfect information  games, and push forward the research of reinforcement learning in  domains with multiple agents, large state and action space, and sparse  reward. In this paper, we provide an overview of the key components in  RLCard, a discussion of the design principles, a brief introduction of  the interfaces, and comprehensive evaluations of the environments. The  codes and documents are available <a rel="noreferrer noopener" aria-label="HERE (opens in a new tab)" href="https://github.com/datamllab/rlcard" target="_blank">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="527" height="382" src="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct6.png" alt="" class="wp-image-23595" srcset="https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct6.png 527w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct6-150x109.png 150w, https://insidebigdata.com/wp-content/uploads/2019/11/arXiv_2019_Oct6-300x217.png 300w" sizes="(max-width: 527px) 100vw, 527px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em>   </p>
</div></div><!-- end .after-post-ad --></div><div class="post-23707 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – November 2019</h1>
<div class="post-info"><span class="date published time" title="2019-12-18T08:00:38-08:00">December 18, 2019</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2019/12/18/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-november-2019/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="231" height="195" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 231px) 100vw, 231px"></figure></div>







<p><a rel="noreferrer noopener" aria-label="Hacking Neural Networks: A Short Introduction (opens in a new tab)" href="https://arxiv.org/pdf/1911.07658v2.pdf" target="_blank">Hacking Neural Networks: A Short Introduction</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>A large chunk of research on the security issues of neural networks is focused on adversarial attacks. However, there exists a vast sea of simpler attacks one can perform both against and with neural networks. This paper gives a quick introduction on how deep learning in security works and explore the basic methods of exploitation, but also look at the offensive capabilities deep learning enabled tools provide. All presented attacks, such as backdooring, GPU-based buffer overflows or automated bug hunting, are accompanied by short open-source exercises for anyone to try out. The TensorFlow code for this paper can be found <a href="https://github.com/Kayzaks/HackingNeuralNetworks" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="635" height="399" src="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov1.png" alt="" class="wp-image-23710" srcset="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov1.png 635w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov1-150x94.png 150w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov1-300x189.png 300w" sizes="(max-width: 635px) 100vw, 635px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research (opens in a new tab)" href="https://arxiv.org/pdf/1911.05063v2.pdf" target="_blank">Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research</a></p>



<p>This paper presents Kaolin, a PyTorch library aiming to accelerate 3D deep  learning research. Kaolin provides efficient implementations of  differentiable 3D modules for use in deep learning systems. With  functionality to load and preprocess several popular 3D data sets, and native functions to manipulate meshes, pointclouds, signed distance  functions, and voxel grids, Kaolin mitigates the need to write wasteful  boilerplate code. Kaolin packages together several differentiable graphics modules including rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate a comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to  serve as a starting point for future research endeavors. Kaolin is  available as open-source software <a rel="noreferrer noopener" aria-label="HERE (opens in a new tab)" href="https://github.com/NVIDIAGameWorks/kaolin/" target="_blank">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="587" height="293" src="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov2.png" alt="" class="wp-image-23711" srcset="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov2.png 587w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov2-150x75.png 150w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov2-300x150.png 300w" sizes="(max-width: 587px) 100vw, 587px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="End to End Learning for Self-Driving Cars (opens in a new tab)" href="https://arxiv.org/pdf/1604.07316v1.pdf" target="_blank">End to End Learning for Self-Driving Cars</a></p>



<p>The research detailed in this paper trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. The NVIDIA researchers never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. It’s argued that this will eventually lead to better performance and smaller systems. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="601" height="327" src="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov3.png" alt="" class="wp-image-23712" srcset="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov3.png 601w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov3-150x82.png 150w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov3-300x163.png 300w" sizes="(max-width: 601px) 100vw, 601px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game (opens in a new tab)" href="https://arxiv.org/pdf/1911.06997v1.pdf" target="_blank">Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game</a></p>



<p>Self-supervised (SS) learning is a powerful approach for representation  learning using unlabeled data. Recently, it has been applied to Generative Adversarial Networks (GAN) training. Specifically, SS  tasks were proposed to address the catastrophic forgetting issue in the  GAN discriminator. This paper performs an in-depth analysis to understand how SS tasks interact with learning of generator. From the analysis, there is an identification of issues of SS tasks which allow a severely  mode-collapsed generator to excel the SS tasks. To address the issues,  the researchers propose new SS tasks based on a multi-class minimax game. The  competition between proposed SS tasks in the game encourages the generator to learn the data distribution and generate diverse samples.  The paper provides both theoretical and empirical analysis to support that the proposed SS tasks have better convergence property. The TensorFlow code for this paper can be found <a href="https://github.com/tntrung/msgan" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>.</p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="672" height="407" src="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov4.png" alt="" class="wp-image-23713" srcset="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov4.png 672w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov4-150x91.png 150w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov4-300x182.png 300w" sizes="(max-width: 672px) 100vw, 672px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="On the Measure of Intelligence (opens in a new tab)" href="https://arxiv.org/pdf/1911.01547v2.pdf" target="_blank">On the Measure of Intelligence</a></p>



<p>To make deliberate progress towards more intelligent and more human-like  artificial systems, we need to be following an appropriate feedback  signal: we need to be able to define and evaluate intelligence in a way  that enables comparisons between two systems, as well as comparisons  with humans. Over the past hundred years, there has been an abundance of  attempts to define and measure intelligence, across both the fields of  psychology and AI. This paper summarizes and critically assesses these definitions and evaluation approaches, while  making apparent the two historical conceptions of intelligence that have  implicitly guided them. It’s noted that in practice, the contemporary AI  community still gravitates towards benchmarking intelligence by  comparing the skill exhibited by AIs and humans at specific tasks such  as board games and video games. The Google author argues that solely measuring skill at  any given task falls short of measuring intelligence, because skill is  heavily modulated by prior knowledge and experience: unlimited priors or  unlimited training data allow experimenters to “buy” arbitrary levels  of skills for a system, in a way that masks the system’s own  generalization power. He then articulates a new formal definition of  intelligence based on Algorithmic Information Theory, describing  intelligence as skill-acquisition efficiency and highlighting the  concepts of scope, generalization difficulty, priors, and experience.  Using this definition, a set of guidelines is proposed for what a general  AI benchmark should look like.  </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="741" height="340" src="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov5.png" alt="" class="wp-image-23714" srcset="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov5.png 741w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov5-150x69.png 150w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov5-300x138.png 300w" sizes="(max-width: 741px) 100vw, 741px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Auptimizer -- an Extensible, Open-Source Framework for Hyperparameter Tuning (opens in a new tab)" href="https://arxiv.org/pdf/1911.02522v1.pdf" target="_blank">Auptimizer — an Extensible, Open-Source Framework for Hyperparameter Tuning</a></p>



<p>Tuning machine learning models at scale, especially finding the right  hyperparameter values, can be difficult and time-consuming. In addition  to the computational effort required, this process also requires some  ancillary efforts including engineering tasks (e.g., job scheduling) as well as more mundane tasks (e.g., keeping track of the various  parameters and associated results). This paper presents Auptimizer, a general Hyperparameter Optimization (HPO) framework to  help data scientists speed up model tuning and bookkeeping. With  Auptimizer, users can use all available computing resources in  distributed settings for model training. The user-friendly system design  simplifies creating, controlling, and tracking of a typical machine  learning project. The code for the paper can be found <a href="https://github.com/LGE-ARC-AdvancedAI/auptimizer" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="606" height="349" src="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov6.png" alt="" class="wp-image-23715" srcset="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov6.png 606w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov6-150x86.png 150w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov6-300x173.png 300w" sizes="(max-width: 606px) 100vw, 606px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Putting An End to End-to-End:Gradient-Isolated Learning of Representations (opens in a new tab)" href="https://arxiv.org/pdf/1905.11786.pdf" target="_blank">Putting An End to End-to-End:Gradient-Isolated Learning of Representations</a></p>



<p>This paper proposes a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, the team split a deep neural network into a stack of gradient-isolated modules. Each module is trained to maximally preserve the information of its inputs using the InfoNCE bound from Oord et al. [2018]. Despite this greedy training, the results demonstrate that each module improves upon the output of its predecessor, and that the representations created by the top module yield highly competitive results on downstream classification tasks in the audio and visual domain. The proposal enables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled data sets. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="645" height="422" src="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov7.png" alt="" class="wp-image-23716" srcset="https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov7.png 645w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov7-150x98.png 150w, https://insidebigdata.com/wp-content/uploads/2019/12/arXiv_2019_Nov7-300x196.png 300w" sizes="(max-width: 645px) 100vw, 645px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em>   </p>
</div></div><!-- end .after-post-ad --></div><div class="post-23845 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – December 2019</h1>
<div class="post-info"><span class="date published time" title="2020-01-16T08:00:00-08:00">January 16, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/01/16/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-december-2019/#comments">1 Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="236" height="199" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 236px) 100vw, 236px"></figure></div>







<p><a rel="noreferrer noopener" aria-label="Deep Learning Model for Finding New Superconductors (opens in a new tab)" href="https://arxiv.org/pdf/1812.01995.pdf" target="_blank">Deep Learning Model for Finding New Superconductors</a></p>



<p>Superconductivity has been extensively studied since its discovery in 1911. However, the feasibility of room-temperature superconductivity is unknown. It is very difficult for both theory and computational methods to predict the superconducting transition temperatures Tc of superconductors for strongly correlated systems, in which high-temperature superconductivity emerges. Exploration of new superconductors still relies on the experience and intuition of experts, and is largely a process of experimental trial and error. In one study, only 3% of the candidate materials showed superconductivity. The paper reports the first deep learning model for finding new superconductors. The periodic table is represented in a way that allows a deep learning model to learn it. The paper obtained three remarkable results. The deep learning method can predict superconductivity for a material with a precision of 62%, which shows the usefulness of the model; it found the recently discovered superconductor CaBi2, which is not in the superconductor database; and it found Fe-based high-temperature superconductors (discovered in 2008) from the training data before 2008. These results open the way for the discovery of new high-temperature superconductor families. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="740" height="621" src="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec1.png" alt="" class="wp-image-23846" srcset="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec1.png 740w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec1-150x126.png 150w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec1-300x252.png 300w" sizes="(max-width: 740px) 100vw, 740px"></figure></div>



<p><a href="https://arxiv.org/pdf/1911.10500.pdf" target="_blank" rel="noreferrer noopener" aria-label="Causality for Machine Learning (opens in a new tab)">Causality for Machine Learning</a></p>



<p>Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This paper discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="731" height="240" src="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec2.png" alt="" class="wp-image-23847" srcset="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec2.png 731w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec2-150x49.png 150w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec2-300x98.png 300w" sizes="(max-width: 731px) 100vw, 731px"></figure></div>



<p><a href="https://arxiv.org/pdf/1908.09124v3.pdf" target="_blank" rel="noreferrer noopener" aria-label="SeesawFaceNets: sparse and robust face verification model for mobile platform (opens in a new tab)">SeesawFaceNets: sparse and robust face verification model for mobile platform</a></p>



<p>Deep Convolutional Neural Network (DCNNs) come to be the most widely  used solution for most computer vision related tasks, and one of the  most important application scenes is face verification. Due to its  high-accuracy performance, deep face verification models of which the  inference stage occurs on cloud platform through internet plays the key  role on most practical scenes. However, two  critical issues exist: (i) individual privacy may not be well  protected since they have to upload their personal photo and other  private information to the online cloud back-end, and (ii) either  training or inference stage is time-consuming and the latency may affect  customer experience, especially when the internet link speed is not so  stable or in remote areas where mobile reception is not so good, but  also in cities where building and other construction may block mobile  signals. Therefore, designing lightweight networks with low memory requirements and computational costs is one of the most practical  solutions for face verification on mobile platform. In this paper, a  novel mobile network named SeesawFaceNets, a simple but effective model,  is proposed for productively deploying face recognition for mobile  devices. The Pytorch code associated with the paper can be found <a href="https://github.com/cvtower/seesawfacenet_pytorch" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="471" height="317" src="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec3.png" alt="" class="wp-image-23848" srcset="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec3.png 471w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec3-150x101.png 150w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec3-300x202.png 300w" sizes="(max-width: 471px) 100vw, 471px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="VIBE: Video Inference for Human Body Pose and Shape Estimation (opens in a new tab)" href="https://arxiv.org/pdf/1912.05656v1.pdf" target="_blank">VIBE: Video Inference for Human Body Pose and Shape Estimation</a></p>



<p>Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methods fail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, this paper proposes Video Inference for Body Pose and Shape Estimation (VIBE), which makes use of an existing large-scale motion capture data set (AMASS) together with unpaired, in-the-wild, 2D key-point annotations. The key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape regression networks. The researchers define a temporal network architecture and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. Extensive experimentation is performed to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation data sets, achieving state-of-the-art performance.  The PyTorch code associated with this paper can be found <a href="https://github.com/mkocabas/VIBE" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="778" height="438" src="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec4.png" alt="" class="wp-image-23849" srcset="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec4.png 778w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec4-150x84.png 150w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec4-300x169.png 300w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec4-768x432.png 768w" sizes="(max-width: 778px) 100vw, 778px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Real-time Scene Text Detection with Differentiable Binarization (opens in a new tab)" href="https://arxiv.org/pdf/1911.08947v2.pdf" target="_blank">Real-time Scene Text Detection with Differentiable Binarization</a></p>



<p>Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. This paper proposes a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, the paper validates the performance improvements of DB on five benchmark data sets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. The PyTorch code associated with this paper can be found <a href="https://github.com/WenmuZhou/DBNet.pytorch" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="809" height="269" src="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec5.png" alt="" class="wp-image-23850" srcset="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec5.png 809w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec5-150x50.png 150w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec5-300x100.png 300w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec5-768x255.png 768w" sizes="(max-width: 809px) 100vw, 809px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="CNN-generated images are surprisingly easy to spot... for now (opens in a new tab)" href="https://arxiv.org/pdf/1912.11035v1.pdf" target="_blank">CNN-generated images are surprisingly easy to spot… for now</a></p>



<p>This work in this paper asks whether it is possible to create a “universal” detector for telling apart real images from these generated by a CNN, regardless of architecture or data set used. To test this, the researchers collect a data set consisting of fake images generated by 11 different CNN-based image generator models, chosen to span the space of commonly used architectures today (ProGAN, StyleGAN, BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks, implicit maximum likelihood estimation, second-order attention super-resolution, seeing-in-the-dark). The work demonstrates that, with careful pre- and post-processing and data augmentation, a standard image classifier trained on only one specific CNN generator (ProGAN) is able to generalize surprisingly well to unseen architectures, data sets, and training methods (including the just released StyleGAN2). The findings suggest the intriguing possibility that today’s CNN-generated images share some common systematic flaws, preventing them from achieving realistic image synthesis. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="768" height="235" src="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec6.png" alt="" class="wp-image-23851" srcset="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec6.png 768w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec6-150x46.png 150w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec6-300x92.png 300w" sizes="(max-width: 768px) 100vw, 768px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Characterizing the Decision Boundary of Deep Neural Networks (opens in a new tab)" href="https://arxiv.org/pdf/1912.11460v2.pdf" target="_blank">Characterizing the Decision Boundary of Deep Neural Networks</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Deep neural networks and in particular, deep neural classifiers have become an integral part of many modern applications. Despite their practical success, we still have limited knowledge of how they work and the demand for such an understanding is ever growing. In this regard, one crucial aspect of deep neural network classifiers that can help us deepen our knowledge about their decision-making behavior is to investigate their decision boundaries. Nevertheless, this is contingent upon having access to samples populating the areas near the decision boundary. To achieve this, this paper proposes a novel approach called Deep Decision boundary Instance Generation (DeepDIG). DeepDIG utilizes a method based on adversarial example generation as an effective way of generating samples near the decision boundary of any deep neural network model. Then, a set of important principled characteristics are introduced that take advantage of the generated instances near the decision boundary to provide multifaceted understandings of deep neural networks. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="480" height="422" src="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec7.png" alt="" class="wp-image-23852" srcset="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec7.png 480w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec7-150x132.png 150w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec7-300x264.png 300w" sizes="(max-width: 480px) 100vw, 480px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="BackPACK: Packing more into backprop (opens in a new tab)" href="https://arxiv.org/pdf/1912.10985v1.pdf" target="_blank">BackPACK: Packing more into backprop</a></p>



<div class="wp-block-image"><figure class="alignleft"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Automatic differentiation frameworks are optimized for exactly one thing: computing the average mini-batch gradient. Yet, other quantities such as the variance of the mini-batch gradients or many approximations to the Hessian can, in theory, be computed efficiently, and at the same time as the gradient. While these quantities are of great interest to researchers and practitioners, current deep-learning software does not support their automatic calculation. Manually implementing them is burdensome, inefficient if done naively, and the resulting code is rarely shared. This hampers progress in deep learning, and unnecessarily narrows research to focus on gradient descent and its variants; it also complicates replication studies and comparisons between newly developed methods that require those quantities, to the point of impossibility. To address this problem, this paper introduces BackPACK, an efficient framework built on top of PyTorch, that extends the backpropagation algorithm to extract additional information from first- and second-order derivatives. Its capabilities are illustrated by benchmark reports for computing additional quantities on deep neural networks, and an example application by testing several recent curvature approximations for optimization. The PyTorch code associated with this paper can be found <a href="https://f-dangel.github.io/backpack/" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter"><img loading="lazy" width="518" height="328" src="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec8.png" alt="" class="wp-image-23853" srcset="https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec8.png 518w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec8-150x95.png 150w, https://insidebigdata.com/wp-content/uploads/2020/01/arXiv_2019_Dec8-300x190.png 300w" sizes="(max-width: 518px) 100vw, 518px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-23966 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-artificial-intelligence tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – January 2020</h1>
<div class="post-info"><span class="date published time" title="2020-02-11T08:00:00-08:00">February 11, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/02/11/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-january-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="216" height="182" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 216px) 100vw, 216px"></figure></div>







<p><a rel="noreferrer noopener" aria-label="FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence (opens in a new tab)" href="https://arxiv.org/pdf/2001.07685v1.pdf" target="_blank">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</a></p>



<p>Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model’s performance. This paper from Google Research demonstrates the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. The new algorithm, FixMatch, first generates pseudo-labels using the model’s predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, it’s shown that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 — just 4 labels per class. The code for this paper is available <a href="https://github.com/google-research/fixmatch" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="294" src="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_2.png" alt="" class="wp-image-23967" srcset="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_2.png 700w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_2-300x126.png 300w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_2-150x63.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2001.06249.pdf" target="_blank">Markov Chain Monte Carlo Methods, a survey with some frequent misunderstandings</a> </p>



<p>This paper reviews some of the most standard MCMC tools used in Bayesian computation, along with vignettes on standard misunderstandings of these approaches. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="744" height="352" src="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_1-1.png" alt="" class="wp-image-23968" srcset="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_1-1.png 744w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_1-1-300x142.png 300w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_1-1-150x71.png 150w" sizes="(max-width: 744px) 100vw, 744px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="batchboost: regularization for stabilizing training with resistance to underfitting &amp; overfitting (opens in a new tab)" href="https://arxiv.org/pdf/2001.07627v1.pdf" target="_blank">batchboost: regularization for stabilizing training with resistance to underfitting &amp; overfitting</a> </p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Overfitting &amp; underfitting and stable training are an important challenges in machine learning. Current approaches for these issues are mixup, SamplePairing and BC learning. This paper states the hypothesis that mixing many images together can be more effective than just two. Batchboost pipeline has three stages: (a) pairing: method of selecting two samples. (b) mixing: how to create a new one from two samples. (c) feeding: combining mixed samples with new ones from dataset into batch (with ratio <em>γ</em>). The batchboost method is slightly better than SamplePairing technique on small datasets (up to 5%). Batchboost provides stable training on not tuned parameters (like weight decay), thus its a good method to test performance of different architectures. The code for this paper is available <a rel="noreferrer noopener" aria-label="HERE (opens in a new tab)" href="https://github.com/maciejczyzewski/batchboost" target="_blank">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="432" src="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_3.png" alt="" class="wp-image-23969" srcset="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_3.png 700w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_3-300x185.png 300w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_3-150x93.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="https://arxiv.org/pdf/2001.05574v3.pdf" target="_blank" rel="noreferrer noopener" aria-label="Advbox: a toolbox to generate adversarial examples that fool neural networks (opens in a new tab)">Advbox: a toolbox to generate adversarial examples that fool neural networks</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>In recent years, neural networks have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. Recent studies have shown that they are all vulnerable to the attack of adversarial examples. Small and often imperceptible perturbations to the input images are sufficient to fool the most powerful neural networks. Advbox is a toolbox to generate adversarial examples that fool neural networks in PaddlePaddle, PyTorch, Caffe2, MxNet, Keras, TensorFlow and it can benchmark the robustness of machine learning models. Compared to previous work, this platform supports black box attacks on Machine-Learning-as-a-service, as well as more attack scenarios, such as Face Recognition Attack, Stealth T-shirt, and Deepfake Face Detect. The code for this paper is available <a href="https://github.com/advboxes/AdvBox" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>.  </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="334" src="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_4.png" alt="" class="wp-image-23971" srcset="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_4.png 700w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_4-300x143.png 300w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_4-150x72.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network (opens in a new tab)" href="https://arxiv.org/pdf/2001.06268v1.pdf" target="_blank">Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network</a></p>



<p>Recent studies in image classification have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. This paper carries out extensive experiments to validate that carefully assembling these techniques and applying them to a basic CNN model in combination can improve the accuracy and robustness of the model while minimizing the loss of throughput. For example, a proposed ResNet-50 shows an improvement in top-1 accuracy from 76.3% to 82.78%, and an mCE improvement from 76.0% to 48.9%, on the ImageNet ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. The resulting model significantly outperforms state-of-the-art models with similar accuracy in terms of mCE and inference throughput. To verify the performance improvement in transfer learning, fine grained classification and image retrieval tasks were tested on several open datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly. The code for this paper is available <a href="https://github.com/clovaai/assembled-cnn" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="486" height="731" src="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_5.png" alt="" class="wp-image-23973" srcset="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_5.png 486w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_5-199x300.png 199w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_5-100x150.png 100w" sizes="(max-width: 486px) 100vw, 486px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Deep Learning for Person Re-identification: A Survey and Outlook (opens in a new tab)" href="https://arxiv.org/pdf/2001.04193v1.pdf" target="_blank">Deep Learning for Person Re-identification: A Survey and Outlook</a></p>



<p>Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained significantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, it is possible to categorize it into the closed-world and open-world settings. The widely studied closed-world setting is usually applied under various research-oriented assumptions, and has achieved inspiring success using deep learning techniques on a number of data sets. This paper first conducts a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under specific scenarios. The paper summarizes the open-world Re-ID in terms of five different aspects. By analyzing the advantages of existing methods, the paper designs a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on both single- and cross-modality Re-ID tasks. Meanwhile, the paper introduces a new evaluation metric (mINP) for person Re-ID, indicating the cost for finding all the correct matches, which provides an additional criteria to evaluate the Re-ID system for real applications. The code for this paper is available <a href="https://github.com/mangye16/ReID-Survey" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="175" src="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_6.png" alt="" class="wp-image-23976" srcset="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_6.png 700w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_6-300x75.png 300w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_6-150x38.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection (opens in a new tab)" href="https://arxiv.org/pdf/2001.03024v1.pdf" target="_blank">DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection</a></p>



<p>This paper presents an on-going effort of constructing a large-scale benchmark, DeeperForensics-1.0, for face forgery detection. The benchmark represents the largest face forgery detection data set by far, with 60, 000 videos constituted by a total of 17.6 million frames, 10 times larger than existing data sets of the same kind. Extensive real-world perturbations are applied to obtain a more challenging benchmark of larger scale and higher diversity. All source videos in DeeperForensics-1.0 are carefully collected, and fake videos are generated by a newly proposed end-to-end face swapping framework. The quality of generated videos outperforms those in existing datasets, validated by user studies. The benchmark features a hidden test set, which contains manipulated videos achieving high deceptive scores in human evaluations. The code for this paper is available <a rel="noreferrer noopener" aria-label="HERE (opens in a new tab)" href="https://github.com/EndlessSora/DeeperForensics-1.0" target="_blank">HERE</a>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="285" src="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_7.png" alt="" class="wp-image-23978" srcset="https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_7.png 700w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_7-300x122.png 300w, https://insidebigdata.com/wp-content/uploads/2020/02/arXiv_2020_Jan_7-150x61.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em>   </p>
</div></div><!-- end .after-post-ad --></div><div class="post-24075 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-artificial-intelligence tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – February 2020</h1>
<div class="post-info"><span class="date published time" title="2020-03-09T08:00:00-07:00">March 9, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/03/09/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-february-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="266" height="225" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 266px) 100vw, 266px"></figure></div>







<p><a href="https://arxiv.org/pdf/2002.12327.pdf" target="_blank" rel="noreferrer noopener" aria-label="A Primer in BERTology: What we know about how BERT works (opens in a new tab)">A Primer in BERTology: What we know about how BERT works</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Transformer-based models are now widely used in NLP, but we still do not understand a lot about their inner workings. This paper describes what is known to date about the famous BERT model (Devlin et al. 2019), synthesizing over 40 analysis studies. Also provided is an overview of the proposed modifications to the model and its training regime. Also included is an outline of the directions for further research. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="444" height="453" src="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_1.png" alt="" class="wp-image-24076" srcset="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_1.png 444w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_1-294x300.png 294w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_1-147x150.png 147w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_1-50x50.png 50w" sizes="(max-width: 444px) 100vw, 444px"></figure></div>



<p><a href="https://arxiv.org/pdf/2002.07971v1.pdf" target="_blank" rel="noreferrer noopener" aria-label="Gradient Boosting Neural Networks: GrowNet (opens in a new tab)">Gradient Boosting Neural Networks: GrowNet</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>A novel gradient boosting framework is proposed where shallow neural networks are employed as “weak learners.” General loss functions are considered under this unified framework with specific examples presented for classification, regression and learning to rank. A fully corrective step is incorporated to remedy the pitfall of greedy function approximation of classic gradient boosting decision tree. The proposed model rendered state-of-the-art results in all three tasks on multiple data sets. An ablation study is performed to shed light on the effect of each model components and model hyperparameters. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="310" src="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_2.png" alt="" class="wp-image-24077" srcset="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_2.png 700w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_2-300x133.png 300w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_2-150x66.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="The Deep Learning Compiler: A Comprehensive Survey (opens in a new tab)" href="https://arxiv.org/pdf/2002.03794.pdf" target="_blank">The Deep Learning Compiler: A Comprehensive Survey</a></p>



<p>The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing surveys has analyzed the unique design of the DL compilers comprehensively. This paper performs a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. This is the first survey paper focusing on the unique design of DL compiler, and it is hoped this can pave the road for future research towards the DL compiler. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="189" src="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_3.png" alt="" class="wp-image-24078" srcset="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_3.png 700w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_3-300x81.png 300w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_3-150x41.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks (opens in a new tab)" href="https://arxiv.org/pdf/1905.11286v3.pdf" target="_blank">Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks</a></p>



<p>This paper proposes NovoGrad, an adaptive stochastic gradient descent method with layer-wise gradient normalization and decoupled weight decay. In experiments on neural networks for image classification, speech recognition, machine translation, and language modeling, it performs on par or better than well tuned SGD with momentum and Adam or AdamW. Additionally, NovoGrad (1) is robust to the choice of learning rate and weight initialization, (2) works well in a large batch setting, and (3) has two times smaller memory footprint than Adam. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="482" height="551" src="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_4.png" alt="" class="wp-image-24079" srcset="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_4.png 482w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_4-262x300.png 262w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_4-131x150.png 131w" sizes="(max-width: 482px) 100vw, 482px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="GANILLA: Generative Adversarial Networks for Image to Illustration Translation (opens in a new tab)" href="https://arxiv.org/pdf/2002.05638v2.pdf" target="_blank">GANILLA: Generative Adversarial Networks for Image to Illustration Translation</a></p>



<p>This paper explores illustrations in children’s books as a new domain in unpaired image-to-image translation. It’s shown that although the current state-of-the-art image-to-image translation models successfully transfer either the style or the content, they fail to transfer both at the same time. A new generator network is proposed to address this issue and show that the resulting network strikes a better balance between style and content. The PyTorch code for this paper can be found <a href="https://github.com/giddyyupp/ganilla" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="702" height="391" src="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_5.png" alt="" class="wp-image-24080" srcset="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_5.png 702w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_5-300x167.png 300w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_5-150x84.png 150w" sizes="(max-width: 702px) 100vw, 702px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Cross-Iteration Batch Normalization (opens in a new tab)" href="https://arxiv.org/pdf/2002.05712v2.pdf" target="_blank">Cross-Iteration Batch Normalization</a></p>



<p>A well-known issue of Batch Normalization is its significantly reduced effectiveness in the case of small mini-batch sizes. When a mini-batch contains few examples, the statistics upon which the normalization is defined cannot be reliably estimated from it during a training iteration. To address this problem, this paper presents <em>Cross-Iteration Batch Normalization (CBN)</em>, in which examples from multiple recent iterations are jointly utilized to enhance estimation quality. On object detection and image classification with small mini-batch sizes, CBN is found to outperform the original batch normalization and a direct calculation of statistics over previous iterations without the proposed compensation technique. The PyTorch code for this paper can be found<a href="https://github.com/Howal/Cross-iterationBatchNorm" target="_blank" rel="noreferrer noopener" aria-label=" HERE (opens in a new tab)"> HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="431" src="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_6.png" alt="" class="wp-image-24081" srcset="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_6.png 700w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_6-300x185.png 300w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_6-150x92.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning (opens in a new tab)" href="https://arxiv.org/pdf/2002.05651v1.pdf" target="_blank">Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning</a></p>



<p>Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. This paper introduces a framework that makes this easier by providing a simple interface for tracking real-time energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, the paper creates a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using the framework, strategies for mitigation of carbon emissions and reduction of energy consumption are proposed. By making accounting easier, the hope is to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms. The code for this paper can be found <a href="https://github.com/Breakend/experiment-impact-tracker" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="915" height="314" src="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_7.png" alt="" class="wp-image-24082" srcset="https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_7.png 915w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_7-300x103.png 300w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_7-150x51.png 150w, https://insidebigdata.com/wp-content/uploads/2020/03/arXiv_2020_Feb_7-768x264.png 768w" sizes="(max-width: 915px) 100vw, 915px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em> </p>
</div></div><!-- end .after-post-ad --></div><div class="post-24213 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-artificial-intelligence tag-data-science tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – March 2020</h1>
<div class="post-info"><span class="date published time" title="2020-04-16T07:00:00-07:00">April 16, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/04/16/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-march-2020/#comments">1 Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="234" height="198" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 234px) 100vw, 234px"></figure></div>







<p><a rel="noreferrer noopener" aria-label="Julia Language in Machine Learning: Algorithms, Applications, and Open Issues (opens in a new tab)" href="https://arxiv.org/pdf/2003.10146.pdf" target="_blank">Julia Language in Machine Learning: Algorithms, Applications, and Open Issues</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Machine learning is driving development across many fields in science and engineering. A simple and efficient programming language could accelerate applications of machine learning in various fields. Currently, the programming languages most commonly used to develop machine learning algorithms include Python, MATLAB, and C/C ++. However, none of these languages well balance both efficiency and simplicity. The Julia language is a fast, easy-to-use, and open-source programming language that was originally designed for high-performance computing, which can well balance the efficiency and simplicity. This paper summarizes the related research work and developments in the application of the Julia language in machine learning. It first surveys the popular machine learning algorithms that are developed in the Julia language. Then, it investigates applications of the machine learning algorithms implemented with the Julia language. Finally, it discusses the open issues and the potential future directions that arise in the use of the Julia language in machine learning. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="616" src="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_1.png" alt="" class="wp-image-24214" srcset="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_1.png 700w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_1-300x264.png 300w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_1-150x132.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="COVID-CT-Dataset: A CT Scan Dataset about COVID-19 (opens in a new tab)" href="https://arxiv.org/pdf/2003.13865v1.pdf" target="_blank">COVID-CT-Dataset: A CT Scan Dataset about COVID-19</a></p>



<p>CT scans are promising in providing accurate, fast, and cheap screening and testing of COVID-19. This paper builds a publicly available COVID-CT data set, containing 275 CT scans that are positive for COVID-19, to foster the research and development of deep learning methods which predict whether a person is affected with COVID-19 by analyzing his/her CTs. The authors train a deep convolutional neural network (CNN) on this data set and achieve an F1 of 0.85 which is a promising performance but yet to be further improved.  The data and PyTorch code for this paper can be found <a href="https://github.com/UCSD-AI4H/COVID-CT" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">HERE</a>.  </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="395" src="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_2.png" alt="" class="wp-image-24273" srcset="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_2.png 700w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_2-300x169.png 300w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_2-150x85.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="COVID-19 Image Data Collection (opens in a new tab)" href="https://arxiv.org/pdf/2003.11597v1.pdf" target="_blank">COVID-19 Image Data Collection</a></p>



<p>This paper describes the initial COVID-19 open image data collection. It was created by assembling medical images from websites and publications and currently contains 123 frontal view X-rays. The GitHub site associated with this paper can be found <a href="https://github.com/ieee8023/covid-chestxray-dataset" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="217" src="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_3.png" alt="" class="wp-image-24274" srcset="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_3.png 700w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_3-300x93.png 300w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_3-150x47.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Masked Face Recognition Dataset and Application (opens in a new tab)" href="https://arxiv.org/pdf/2003.09093v2.pdf" target="_blank">Masked Face Recognition Dataset and Application</a></p>



<p>In order to effectively prevent the spread of COVID-19 virus, almost everyone wears a mask during coronavirus epidemic. This almost makes conventional facial recognition technology ineffective in many cases, such as community access control, face access control, facial attendance, facial security checks at train stations, etc. Therefore, it is very urgent to improve the recognition performance of the existing face recognition technology on the masked faces. Most current advanced face recognition approaches are designed based on deep learning, which depend on a large number of face samples. However, at present, there are no publicly available masked face recognition datasets. To this end, this paper proposes three types of masked face datasets, including Masked Face Detection Dataset (MFDD), Real-world Masked Face Recognition Dataset (RMFRD) and Simulated Masked Face Recognition Dataset (SMFRD). The GitHub site associated with this paper can be found <a href="https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="458" height="194" src="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_4.png" alt="" class="wp-image-24275" srcset="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_4.png 458w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_4-300x127.png 300w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_4-150x64.png 150w" sizes="(max-width: 458px) 100vw, 458px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="Stanza: A Python Natural Language Processing Toolkit for Many Human Languages (opens in a new tab)" href="https://arxiv.org/pdf/2003.07082v1.pdf" target="_blank">Stanza: A Python Natural Language Processing Toolkit for Many Human Languages</a></p>



<p>This paper introduces Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. The Authors have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionalities to cover other tasks such as coreference resolution and relation extraction. The PyTorch source code, documentation, and pretrained models for 66 languages are available <a rel="noreferrer noopener" aria-label="HERE (opens in a new tab)" href="https://github.com/stanfordnlp/stanza" target="_blank">HERE</a>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="575" height="550" src="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_5.png" alt="" class="wp-image-24276" srcset="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_5.png 575w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_5-300x287.png 300w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_5-150x143.png 150w" sizes="(max-width: 575px) 100vw, 575px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="TensorFlow Quantum: A Software Framework for Quantum Machine Learning (opens in a new tab)" href="https://arxiv.org/pdf/2003.02989v1.pdf" target="_blank">TensorFlow Quantum: A Software Framework for Quantum Machine Learning</a></p>



<p>This paper introduces TensorFlow Quantum (TFQ), an open source library for the rapid prototyping of hybrid quantum-classical models for classical or quantum data. This framework offers high-level abstractions for the design and training of both discriminative and generative quantum models under TensorFlow and supports high-performance quantum circuit simulators. The paper provides an overview of the software architecture and building blocks through several examples and review the theory of hybrid quantum-classical neural networks. The TFQ functionalities are illustrated via several basic applications including supervised learning for quantum classification, quantum control, and quantum approximate optimization. Moreover, the authors demonstrate how one can apply TFQ to tackle advanced quantum learning tasks including meta-learning, Hamiltonian learning, and sampling thermal states. The authors hope this framework provides the necessary tools for the quantum computing and machine learning research communities to explore models of both natural and artificial quantum systems, and ultimately discover new quantum algorithms which could potentially yield a quantum advantage. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="513" height="659" src="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_6.png" alt="" class="wp-image-24277" srcset="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_6.png 513w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_6-234x300.png 234w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_6-117x150.png 117w" sizes="(max-width: 513px) 100vw, 513px"></figure></div>



<p><a rel="noreferrer noopener" aria-label="What is the State of Neural Network Pruning? (opens in a new tab)" href="https://arxiv.org/pdf/2003.03033v1.pdf" target="_blank">What is the State of Neural Network Pruning?</a></p>



<p>Neural network pruning—the task of reducing the size of a network by removing parameters—has been the subject of a great deal of work in recent years. This paper provides a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, the clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, the authors identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. ShrinkBench is used to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods. The PyTorch code for this paper is available <a href="https://github.com/jjgo/shrinkbench" target="_blank" rel="noreferrer noopener" aria-label="HERE (opens in a new tab)">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="465" height="279" src="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_7.png" alt="" class="wp-image-24278" srcset="https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_7.png 465w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_7-300x180.png 300w, https://insidebigdata.com/wp-content/uploads/2020/04/arXiv_2020_Mar_7-150x90.png 150w" sizes="(max-width: 465px) 100vw, 465px"></figure></div>



<p> </p>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em> </p>
</div></div><!-- end .after-post-ad --></div><div class="post-24385 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – April 2020</h1>
<div class="post-info"><span class="date published time" title="2020-05-13T06:00:00-07:00">May 13, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/05/13/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-april-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="227" height="192" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 227px) 100vw, 227px"></figure></div>







<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2004.15004v2.pdf" target="_blank">CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Deep learning’s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. This paper presents CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. The tool addresses key challenges that novices face while learning about CNNs, which are identified from interviews with instructors and a survey with past students. Users can interactively visualize and inspect the data transformation and flow of intermediate results in a CNN. CNN Explainer tightly integrates a model overview that summarizes a CNN’s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, the tool enables users to inspect the interplay between low-level operations (e.g., mathematical computations) and high-level outcomes (e.g., class predictions). Developed using modern web technologies, CNN Explainer runs locally in users’ web browsers without the need for installation or specialized hardware, broadening the public’s education access to modern deep learning techniques. The TensorFlow code associated with this paper can be found <a href="https://github.com/poloclub/cnn-explainer" target="_blank" rel="noreferrer noopener">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="484" src="https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr1.png" alt="" class="wp-image-24386" srcset="https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr1.png 700w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr1-300x207.png 300w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr1-150x104.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2004.14990v2.pdf" target="_blank">Reinforcement Learning with Augmented Data</a></p>



<p>Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, this paper presents RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. It is shown that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. It is found that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. The PyTorch code associated with this paper can be found <a href="https://github.com/MishaLaskin/rad" target="_blank" rel="noreferrer noopener">HERE</a>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="695" height="446" src="https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr2.png" alt="" class="wp-image-24387" srcset="https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr2.png 695w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr2-300x193.png 300w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr2-150x96.png 150w" sizes="(max-width: 695px) 100vw, 695px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2004.15011v2.pdf" target="_blank">TLDR: Extreme Summarization of Scientific Documents</a></p>



<p>This paper introduces TLDR generation for scientific papers, a new automatic summarization task with high source compression, requiring expert background knowledge and complex language understanding. To facilitate research on this task, the paper introduces SciTLDR, a data set of 3.9K TLDRs. Furthermore, the paper introduces a novel annotation protocol for scalably curating additional gold summaries by rewriting peer review comments. This protocol is used to augment the test set, yielding multiple gold TLDRs for evaluation, which is unlike most recent summarization data sets that assume only one valid gold summary. The paper presents a training strategy for adapting pre-trained language models that exploits similarities between TLDR generation and the related task of title generation, which outperforms strong extractive and abstractive summarization baselines. The GitHub repo associated with this paper can be found <a href="https://github.com/allenai/scitldr" target="_blank" rel="noreferrer noopener">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="524" height="623" src="https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr3.png" alt="" class="wp-image-24388" srcset="https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr3.png 524w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr3-252x300.png 252w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr3-126x150.png 126w" sizes="(max-width: 524px) 100vw, 524px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2004.10934v1.pdf" target="_blank">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></p>



<p>There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large data sets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and data sets. This paper’s authors assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. They use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO data set at a realtime speed of ~65 FPS on Tesla V100. The PyTorch code associate with this paper can be found <a href="https://github.com/Tianxiaomo/pytorch-YOLOv4" target="_blank" rel="noreferrer noopener">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="562" height="548" src="https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr4.png" alt="" class="wp-image-24389" srcset="https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr4.png 562w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr4-300x293.png 300w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr4-150x146.png 150w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr4-50x50.png 50w" sizes="(max-width: 562px) 100vw, 562px"></figure></div>



<p><a href="https://arxiv.org/pdf/2004.00221v1.pdf" target="_blank" rel="noreferrer noopener">NBDT: Neural-Backed Decision Trees</a></p>



<p>Deep learning is being adopted in settings where accurate and justifiable predictions are required, ranging from finance to medical imaging. While there has been recent work providing post-hoc explanations for model predictions, there has been relatively little work exploring more directly interpretable models that can match state-of-the-art accuracy. Historically, decision trees have been the gold standard in balancing interpretability and accuracy. However, recent attempts to combine decision trees with deep learning have resulted in models that (1) achieve accuracies far lower than that of modern neural networks (e.g. ResNet) even on small datasets (e.g. MNIST), and (2) require significantly different architectures, forcing practitioners pick between accuracy and interpretability. This paper forgoes this dilemma by creating Neural-Backed Decision Trees (NBDTs) that (1) achieve neural network accuracy and (2) require no architectural changes to a neural network. NBDTs achieve accuracy within 1% of the base neural network on CIFAR10, CIFAR100, TinyImageNet, using recently state-of-the-art WideResNet; and within 2% of EfficientNet on ImageNet. This yields state-of-the-art explainable models on ImageNet, with NBDTs improving the baseline by ~14% to 75.30% top-1 accuracy. Furthermore, it is shown interpretability of the model’s decisions both qualitatively and quantitatively via a semi-automatic process. The PyTorch code associated with this paper can be found <a href="https://github.com/alvinwan/neural-backed-decision-trees" target="_blank" rel="noreferrer noopener">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="812" height="483" src="https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr5.png" alt="" class="wp-image-24390" srcset="https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr5.png 812w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr5-300x178.png 300w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr5-150x89.png 150w, https://insidebigdata.com/wp-content/uploads/2020/05/arXiv_2020_Apr5-768x457.png 768w" sizes="(max-width: 812px) 100vw, 812px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-24604 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – May 2020</h1>
<div class="post-info"><span class="date published time" title="2020-06-17T06:00:00-07:00">June 17, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/06/17/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-may-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="271" height="229" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 271px) 100vw, 271px"></figure></div>







<p><a href="https://arxiv.org/pdf/2005.12872.pdf" target="_blank" rel="noreferrer noopener">End-to-End Object Detection with Transformers</a></p>



<p>This paper presents a new method that views object detection as a direct set prediction problem. The approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. PyTorch training code and pretrained models are available at <a rel="noreferrer noopener" href="https://github.com/facebookresearch/detr" target="_blank">HERE</a>.  </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="688" height="245" src="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_1.png" alt="" class="wp-image-24605" srcset="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_1.png 688w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_1-300x107.png 300w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_1-150x53.png 150w" sizes="(max-width: 688px) 100vw, 688px"></figure></div>



<p><a href="https://arxiv.org/pdf/2005.10200v1.pdf" target="_blank" rel="noreferrer noopener">BERTweet: A pre-trained language model for English Tweets</a></p>



<p>This paper presents BERTweet, the first public large-scale pre-trained language model for English Tweets. BERTweet is trained using the RoBERTa pre-training procedure (Liu et al., 2019), with the same model configuration as BERT-base (Devlin et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. BERTweet is released to facilitate future research and downstream applications on Tweet data. PyTorch BERTweet code is available <a rel="noreferrer noopener" href="https://github.com/VinAIResearch/BERTweet" target="_blank">HERE</a>. </p>



<p><a rel="noreferrer noopener" href="https://arxiv.org/abs/2005.10999v1" target="_blank">Spoof Face Detection Via Semi-Supervised Adversarial Training</a></p>



<p>Face spoofing causes severe security threats in face recognition systems. Previous anti-spoofing works focused on supervised techniques, typically with either binary or auxiliary supervision. Most of them suffer from limited robustness and generalization, especially in the cross-dataset setting. This paper proposes a semi-supervised adversarial learning framework for spoof face detection, which largely relaxes the supervision condition. To capture the underlying structure of live faces data in latent representation space, the proposal it to train the live face data only, with a convolutional Encoder-Decoder network acting as a Generator. Meanwhile, a second convolutional network is added serving as a Discriminator. The generator and discriminator are trained by competing with each other while collaborating to understand the underlying concept in the normal class(live faces). Since the spoof face detection is video based (i.e., temporal information), the optical flow maps are converted from consecutive video frames as input. The approach is free of the spoof faces, thus being robust and general to different types of spoof, even unknown spoof. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="203" src="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_3.png" alt="" class="wp-image-24606" srcset="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_3.png 700w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_3-300x87.png 300w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_3-150x44.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="https://arxiv.org/vc/arxiv/papers/2005/2005.11524v1.pdf" target="_blank" rel="noreferrer noopener">Coronavirus: Comparing COVID-19, SARS and MERS in the eyes of AI</a></p>



<p>Novel Coronavirus disease (COVID-19) is an extremely contagious and quickly spreading Coronavirus disease. Severe Acute Respiratory Syndrome (SARS)-CoV, Middle East Respiratory Syndrome (MERS)-CoV outbreak in 2002 and 2011 and current COVID-19 pandemic all from the same family of Coronavirus. The fatality rate due to SARS and MERS were higher than COVID-19 however, the spread of those were limited to few countries while COVID-19 affected more than two-hundred countries of the world. This paper uses deep machine learning algorithms along with innovative image pre-processing techniques to distinguish COVID-19 images from SARS and MERS images. Several deep learning algorithms were trained, and tested and four outperforming algorithms were reported: SqueezeNet, ResNet18, Inceptionv3 and DenseNet201. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="1024" height="396" src="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_4-1024x396.png" alt="" class="wp-image-24607" srcset="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_4-1024x396.png 1024w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_4-300x116.png 300w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_4-150x58.png 150w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_4-768x297.png 768w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_4.png 1065w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2005.12155v1.pdf" target="_blank">AGVNet: Attention Guided Velocity Learning for 3D Human Motion Prediction</a></p>



<p>Human motion prediction plays a vital role in human-robot interaction with various applications such as family service robot. Most of the existing works did not explicitly model velocities of skeletal motion that carried rich motion dynamics, which is critical to predict future poses. This paper proposes a novel feedforward network, AGVNet (Attention Guided Velocity Learning Network), to predict future poses, which explicitly models the velocities at both Encoder and Decoder. Specifically, a novel two-stream Encoder is proposed to encode the skeletal motion in both velocity space and position space. Then, a new feedforward Decoder is presented to predict future velocities instead of position poses, which enables the network to predict multiple future velocities recursively like RNN based Decoder. Finally, a novel loss, ATPL (Attention Temporal Prediction Loss), is designed to pay more attention to the early predictions, which can efficiently guide the recursive model to achieve more accurate predictions. Extensive experiments show the method achieves state-of-the-art performance on two benchmark datasets (i.e. Human3.6M and 3DPW) for human motion prediction.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="509" height="403" src="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_5.png" alt="" class="wp-image-24608" srcset="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_5.png 509w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_5-300x238.png 300w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_5-150x119.png 150w" sizes="(max-width: 509px) 100vw, 509px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/ftp/arxiv/papers/2005/2005.13178.pdf" target="_blank">Generative Adversarial Networks (GANs): An Overview of Theoretical Model, Evaluation Metrics, and Recent Developments</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>One of the most significant challenges in statistical signal processing and machine learning is how to obtain a generative model that can produce samples of large-scale data distribution, such as images and speeches. Generative Adversarial Network (GAN) is an effective method to address this problem. The GANs provide an appropriate way to learn deep representations without widespread use of labeled training data. This approach has attracted the attention of many researchers in computer vision since it can generate a large amount of data without precise modeling of the probability density function (PDF). In GANs, the generative model is estimated via a competitive process where the generator and discriminator networks are trained simultaneously. The generator learns to generate plausible data, and the discriminator learns to distinguish fake data created by the generator from real data samples. Given the rapid growth of GANs over the last few years and their application in various fields, it is necessary to investigate these networks accurately. This paper, after introducing the main concepts and the theory of GAN, compares two new deep generative models, and the evaluation metrics utilized in the literature and challenges of GANs are also explained. Moreover, the most remarkable GAN architectures are categorized and discussed. Finally, the essential applications in computer vision are examined.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="530" height="274" src="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_6.png" alt="" class="wp-image-24609" srcset="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_6.png 530w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_6-300x155.png 300w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_6-150x78.png 150w" sizes="(max-width: 530px) 100vw, 530px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/ftp/arxiv/papers/2005/2005.13997.pdf" target="_blank">Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI)</a></p>



<p>Recently, a groundswell of research has identified the use of counterfactual explanations as a potentially significant solution to the Explainable AI (XAI) problem. It is argued that (a) technically, these counterfactual cases can be generated by permuting problem-features until a class change is found, (b) psychologically, they are much more causally informative than factual explanations, (c) legally, they are GDPR-compliant. However, there are issues around the finding of good counterfactuals using current techniques (e.g. sparsity and plausibility). This paper shows that many commonly-used datasets appear to have few good counterfactuals for explanation purposes. So, it is proposed a new case based approach for generating counterfactuals using novel ideas about the counterfactual potential and explanatory coverage of a case-base. The new technique reuses patterns of good counterfactuals, present in a case-base, to generate analogous counterfactuals that can explain new problems and their solutions. Several experiments show how this technique can improve the counterfactual potential and explanatory coverage of case-bases that were previously found wanting.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="413" src="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_7.png" alt="" class="wp-image-24610" srcset="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_7.png 700w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_7-300x177.png 300w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_7-150x89.png 150w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_7-280x165.png 280w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2005.13885v1.pdf" target="_blank">Hyperbolic Manifold Regression</a></p>



<p>Geometric representation learning has recently shown great promise in several machine learning settings, ranging from relational learning to language processing and generative models. This paper considers the problem of performing manifold-valued regression onto an hyperbolic space as an intermediate component for a number of relevant machine learning applications. In particular, by formulating the problem of predicting nodes of a tree as a manifold regression task in the hyperbolic space, a novel perspective is proposed on two challenging tasks: 1) hierarchical classification via label embeddings and 2) taxonomy extension of hyperbolic representations. To address the regression problem the researchers consider previous methods as well as proposing two novel approaches that are computationally more advantageous: a parametric deep learning model that is informed by the geodesics of the target space and a non-parametric kernel-method for which we also prove excess risk bounds. Our experiments show that the strategy of leveraging the hyperbolic geometry is promising. In particular, in the taxonomy expansion setting, the work finds that the hyperbolic-based estimators significantly outperform methods performing regression in the ambient Euclidean space.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="676" height="340" src="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_8.png" alt="" class="wp-image-24611" srcset="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_8.png 676w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_8-300x151.png 300w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_8-150x75.png 150w" sizes="(max-width: 676px) 100vw, 676px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2005.13012v1.pdf" target="_blank">Comparing BERT against traditional machine learning text classification</a></p>



<p>The BERT model has arisen as a popular state-of-the-art machine learning model in the recent years that is able to cope with multiple NLP tasks such as supervised text classification without human supervision. Its flexibility to cope with any type of corpus delivering great results has make this approach very popular not only in academia but also in the industry. Although, there are lots of different approaches that have been used throughout the years with success. This paper first presents BERT and includes a little review on classical NLP approaches. Then, the researchers empirically test with a suite of experiments dealing different scenarios the behaviour of BERT against the traditional TF-IDF vocabulary fed to machine learning algorithms. The purpose of this work is to add empirical evidence to support or refuse the use of BERT as a default on NLP tasks. Experiments show the superiority of BERT and its independence of features of the NLP problem such as the language of the text adding empirical evidence to use BERT as a default technique to be used in NLP problems.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="477" height="507" src="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_9.png" alt="" class="wp-image-24612" srcset="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_9.png 477w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_9-282x300.png 282w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_May_9-141x150.png 141w" sizes="(max-width: 477px) 100vw, 477px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-24656 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-education category-research-reports category-uncategorized tag-ai tag-artificial-intelligence tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – June 2020</h1>
<div class="post-info"><span class="date published time" title="2020-07-16T06:00:00-07:00">July 16, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/07/16/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-june-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="226" height="191" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 226px) 100vw, 226px"></figure></div>







<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2006.11287.pdf" target="_blank">Discovering Symbolic Models from Deep Learning with Inductive Biases</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>The authors of this paper develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. The focus is on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. The approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.</p>



<figure class="wp-block-image size-large"><img loading="lazy" width="700" height="499" src="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_Jun_1.png" alt="" class="wp-image-24657" srcset="https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_Jun_1.png 700w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_Jun_1-300x214.png 300w, https://insidebigdata.com/wp-content/uploads/2020/06/arXiv_2020_Jun_1-150x107.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure>



<p><a href="https://arxiv.org/pdf/2003.03384.pdf" target="_blank" rel="noreferrer noopener">AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks—or similarly restrictive search spaces. The goal of this paper is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. This is demonstrated by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="468" height="603" src="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_2.png" alt="" class="wp-image-24735" srcset="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_2.png 468w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_2-233x300.png 233w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_2-116x150.png 116w" sizes="(max-width: 468px) 100vw, 468px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2006.12681v1.pdf" target="_blank">Contrastive Generative Adversarial Networks</a></p>



<p>Conditional image synthesis is the task to generate high-fidelity diverse images using class label information. Although many studies have shown realistic results, there is room for improvement if the number of classes increases. This paper proposes a novel conditional contrastive loss to maximize a lower bound on mutual information between samples from the same class. The framework, called Contrastive Generative Adversarial Networks (ContraGAN), learns to synthesize images using class information and data-to-data relations of training examples. The discriminator in ContraGAN discriminates the authenticity of given samples and maximizes the mutual information between embeddings of real images from the same class. Simultaneously, the generator attempts to synthesize images to fool the discriminator and to maximize the mutual information of fake images from the same class prior. The experimental results show that ContraGAN is robust to network architecture selection and outperforms state-of-the-art-models by 3.7% and 11.2% on CIFAR10 and Tiny ImageNet datasets, respectively, without any data augmentation. The software package that can re-produce all experiments is available at <a rel="noreferrer noopener" href="https://github.com/POSTECH-CVLab/PyTorch-StudioGAN" target="_blank">https://github.com/POSTECH-CVLab/PyTorch-StudioGAN</a>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="647" height="344" src="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_3.png" alt="" class="wp-image-24757" srcset="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_3.png 647w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_3-300x160.png 300w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_3-150x80.png 150w" sizes="(max-width: 647px) 100vw, 647px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2006.11538v1.pdf" target="_blank">Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition</a></p>



<p>This work introduces pyramidal convolution (PyConv), which is capable of processing the input at multiple filter scales. PyConv contains a pyramid of kernels, where each level involves different types of filters with varying size and depth, which are able to capture different levels of details in the scene. On top of these improved recognition capabilities, PyConv is also efficient and, with our formulation, it does not increase the computational cost and parameters compared to standard convolution. Moreover, it is very flexible and extensible, providing a large space of potential network architectures for different applications. PyConv has the potential to impact nearly every computer vision task and this work presents different architectures based on PyConv for four main tasks on visual recognition: image classification, video action classification/recognition, object detection and semantic image segmentation/parsing. The approach shows significant improvements over all these core tasks in comparison with the baselines. For instance, on image recognition, our 50-layers network outperforms in terms of recognition performance on ImageNet dataset its counterpart baseline ResNet with 152 layers, while having 2.39 times less parameters, 2.52 times lower computational complexity and more than 3 times less layers. On image segmentation, the novel framework sets a new state-of-the-art on the challenging ADE20K benchmark for scene parsing. Code is available at: <a rel="noreferrer noopener" href="https://github.com/iduta/pyconv" target="_blank">https://github.com/iduta/pyconv</a></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="499" height="450" src="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_4.png" alt="" class="wp-image-24758" srcset="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_4.png 499w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_4-300x271.png 300w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_4-150x135.png 150w" sizes="(max-width: 499px) 100vw, 499px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2006.12567v2.pdf" target="_blank">A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence</a></p>



<p>Deep learning based localization and mapping has recently attracted significant attention. Instead of creating hand-designed algorithms through exploitation of physical models or geometric theories, deep learning based solutions provide an alternative to solve the problem in a data-driven way. Benefiting from ever-increasing volumes of data and computational power, these methods are fast evolving into a new area that offers accurate and robust systems to track motion and estimate scenes and their structure for real-world applications. This papper provides a comprehensive survey, and propose a new taxonomy for localization and mapping using deep learning. Also discussed are the limitations of current models, and indicate possible future directions. A wide range of topics are covered, from learning odometry estimation, mapping, to global localization and simultaneous localization and mapping (SLAM). Revisited is the problem of perceiving self-motion and scene understanding with on-board sensors, and also shown is how to solve it by integrating these modules into a prospective spatial machine intelligence system (SMIS). The hope is that this work can connect emerging works from robotics, computer vision and machine learning communities, and serve as a guide for future researchers to apply deep learning to tackle localization and mapping problems. TensorFlow code associated with this paper can be found here: <a rel="noreferrer noopener" href="https://github.com/changhao-chen/deep-learning-localization-mapping" target="_blank">https://github.com/changhao-chen/deep-learning-localization-mapping</a></p>



<figure class="wp-block-image size-large"><img loading="lazy" width="770" height="414" src="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_8.png" alt="" class="wp-image-24762" srcset="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_8.png 770w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_8-300x161.png 300w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_8-150x81.png 150w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_8-768x413.png 768w" sizes="(max-width: 770px) 100vw, 770px"></figure>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2006.06500v1.pdf" target="_blank">Rethinking the Truly Unsupervised Image-to-Image Translation</a></p>



<p>Every recent image-to-image translation model uses either image-level (i.e. input-output pairs) or set-level (i.e. domain labels) supervision at minimum. However, even the set-level supervision can be a serious bottleneck for data collection in practice. This paper tackles image-to-image translation in a fully unsupervised setting, i.e., neither paired images nor domain labels. It is proposed the truly unsupervised image-to-image translation method (TUNIT) that simultaneously learns to separate image domains via an information-theoretic approach and generate corresponding images using the estimated domain labels. Experimental results on various datasets show that the proposed method successfully separates domains and translates images across those domains. In addition, the model outperforms existing set-level supervised methods under a semi-supervised setting, where a subset of domain labels is provided. The source code is available at <a rel="noreferrer noopener" href="https://github.com/clovaai/tunit" target="_blank">https://github.com/clovaai/tunit</a></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="600" height="287" src="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_5.png" alt="" class="wp-image-24759" srcset="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_5.png 600w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_5-300x144.png 300w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_5-150x72.png 150w" sizes="(max-width: 600px) 100vw, 600px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2006.00979v1.pdf" target="_blank">Acme: A Research Framework for Distributed Reinforcement Learning</a></p>



<p>Deep reinforcement learning has led to many recent-and groundbreaking-advancements. However, these advances have often come at the cost of both the scale and complexity of the underlying RL algorithms. Increases in complexity have in turn made it more difficult for researchers to reproduce published RL algorithms or rapidly prototype ideas. To address this, this paper introduces Acme, a tool to simplify the development of novel RL algorithms that is specifically designed to enable simple agent implementations that can be run at various scales of execution. Our aim is also to make the results of various RL algorithms developed in academia and industrial labs easier to reproduce and extend. To this end baseline implementations of various algorithms are being released, created using the framework. This work also introduces the major design decisions behind Acme and show how these are used to construct these baselines. Software associated with this research can be found here: <a href="https://github.com/deepmind/acme" target="_blank" rel="noreferrer noopener">https://github.com/deepmind/acme</a></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="600" height="256" src="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_6.png" alt="" class="wp-image-24760" srcset="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_6.png 600w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_6-300x128.png 300w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_6-150x64.png 150w" sizes="(max-width: 600px) 100vw, 600px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2006.01112v1.pdf" target="_blank">Cascaded Text Generation with Markov Transformers</a></p>



<p>The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autoregressive model with sub-linear parallel time generation. Noting that conditional random fields with bounded context can be decoded in parallel, the paper proposes an efficient cascaded decoding approach for generating high-quality output. To parameterize this cascade, a Markov transformer is introduced, a variant of the popular fully autoregressive model that allows us to simultaneously decode with specific autoregressive context cutoffs. This approach requires only a small modification from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on five machine translation datasets.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="600" height="302" src="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_7.png" alt="" class="wp-image-24761" srcset="https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_7.png 600w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_7-300x151.png 300w, https://insidebigdata.com/wp-content/uploads/2020/07/arXiv_2020_Jun_7-150x76.png 150w" sizes="(max-width: 600px) 100vw, 600px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-24862 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-analytics category-big-data category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-big-data tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – July 2020</h1>
<div class="post-info"><span class="date published time" title="2020-08-11T06:00:00-07:00">August 11, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/08/11/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-july-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="240" height="203" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 240px) 100vw, 240px"></figure></div>







<p><a href="https://arxiv.org/pdf/2007.15745v1.pdf" target="_blank" rel="noreferrer noopener">On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model’s performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. Several state-of-the-art optimization techniques are introduced in terms of how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="600" height="484" src="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul1.png" alt="" class="wp-image-24863" srcset="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul1.png 600w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul1-300x242.png 300w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul1-150x121.png 150w" sizes="(max-width: 600px) 100vw, 600px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2007.12099v3.pdf" target="_blank">PP-YOLO: An Effective and Efficient Implementation of Object Detector</a></p>



<p>Object detection is one of the most important areas in computer vision, which plays a key role in various practical scenarios. Due to limitation of hardware, it is often necessary to sacrifice accuracy to ensure the infer speed of the detector in practice. Therefore, the balance between effectiveness and efficiency of object detector must be considered. The goal of this paper is to implement an object detector with relatively balanced effectiveness and efficiency that can be directly applied in actual application scenarios, rather than propose a novel detection model. Considering that YOLOv3 has been widely used in practice, this paper describes the development of a new object detector based on YOLOv3. The Baidu, Inc. researchers mainly try to combine various existing tricks that do not increase the number of model parameters and FLOPs, to achieve the goal of improving the accuracy of detector as much as possible while ensuring that the speed is almost unchanged. Since all experiments in this paper are conducted based on PaddlePaddle, the new technique is PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better balance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), surpassing the existing state-of-the-art detectors such as EfficientDet and YOLOv4. Source code associated with this paper can be found <a rel="noreferrer noopener" href="https://github.com/PaddlePaddle/PaddleDetection" target="_blank">HERE</a>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="510" height="419" src="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul2.png" alt="" class="wp-image-24864" srcset="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul2.png 510w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul2-300x246.png 300w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul2-150x123.png 150w" sizes="(max-width: 510px) 100vw, 510px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/abs/2007.12568v1" target="_blank">The Surprising Effectiveness of Linear Unsupervised Image-to-Image Translation</a></p>



<p>Unsupervised image-to-image translation is an inherently ill-posed problem. Recent methods based on deep encoder-decoder architectures have shown impressive results, but this paper shows that they only succeed due to a strong locality bias, and they fail to learn very simple nonlocal transformations (e.g. mapping upside down faces to upright faces). When the locality bias is removed, the methods are too powerful and may fail to learn simple local transformations. This paper introduces linear encoder-decoder architectures for unsupervised image to image translation. The paper shows that learning is much easier and faster with these architectures and yet the results are surprisingly effective. In particular, the paper shows a number of local problems for which the results of the linear methods are comparable to those of state-of-the-art architectures but with a fraction of the training time, and a number of nonlocal problems for which the state-of-the-art fails while linear methods succeed.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="515" height="538" src="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul3.png" alt="" class="wp-image-24865" srcset="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul3.png 515w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul3-287x300.png 287w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul3-144x150.png 144w" sizes="(max-width: 515px) 100vw, 515px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2007.08450v1.pdf" target="_blank">Learning perturbation sets for robust machine learning</a></p>



<p>Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. This paper aims to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, the researchers use a conditional generator that defines the perturbation set over a constrained region of the latent space. The authors formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, the approach can generate a variety of perturbations at different complexities and scales, ranging from baseline digit transformations, through common image corruptions, to lighting variations. The quality of learned perturbation sets were measured both quantitatively and qualitatively, finding that the models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, the learned perturbation sets were leveraged to learn models which have improved generalization performance and are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found <a rel="noreferrer noopener" href="https://github.com/locuslab/perturbation_learning" target="_blank">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="600" height="252" src="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul4.png" alt="" class="wp-image-24866" srcset="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul4.png 600w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul4-300x126.png 300w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul4-150x63.png 150w" sizes="(max-width: 600px) 100vw, 600px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2007.14495v1.pdf" target="_blank">Visualizing classification results</a></p>



<p>Classification is a major tool of statistics and machine learning. A classification method first processes a training set of objects with given classes (labels), with the goal of afterward assigning new objects to one of these classes. When running the resulting prediction method on the training data or on test data, it can happen that an object is predicted to lie in a class that differs from its given label. This is sometimes called label bias, and raises the question whether the object was mislabeled.Our goal is to visualize aspects of the data classification to obtain insight. The proposed display reflects to what extent each object’s label is (dis)similar to its prediction, how far each object lies from the other objects in its class, and whether some objects lie far from all classes. The display is constructed for discriminant analysis, the k-nearest neighbor classifier, support vector machines, logistic regression, and majority voting. It is illustrated on several benchmark datasets containing images and texts.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="600" height="395" src="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul5.png" alt="" class="wp-image-24867" srcset="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul5.png 600w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul5-300x198.png 300w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul5-150x99.png 150w" sizes="(max-width: 600px) 100vw, 600px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2007.13101v1.pdf" target="_blank">Regularized Flexible Activation Function Combinations for Deep Neural Networks</a></p>



<p>Activation in deep neural networks is fundamental to achieving non-linear mappings. Traditional studies mainly focus on finding fixed activations for a particular set of learning tasks or model architectures. The research on flexible activation is quite limited in both designing philosophy and application scenarios. This study looks at three principles of choosing flexible activation components are proposed and a general combined form of flexible activation functions is implemented. Based on this, a novel family of flexible activation functions that can replace sigmoid or tanh in LSTM cells are implemented, as well as a new family by combining ReLU and ELUs. Also, two new regularisation terms based on assumptions as prior knowledge are introduced. It has been shown that LSTM models with proposed flexible activations P-Sig-Ramp provide significant improvements in time series forecasting, while the proposed P-E2-ReLU achieves better and more stable performance on lossy image compression tasks with convolutional auto-encoders. In addition, the proposed regularization terms improve the convergence, performance and stability of the models with flexible activation functions.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="511" height="494" src="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul6.png" alt="" class="wp-image-24868" srcset="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul6.png 511w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul6-300x290.png 300w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul6-150x145.png 150w" sizes="(max-width: 511px) 100vw, 511px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2007.13241v1.pdf" target="_blank">Beyond the Worst-Case Analysis of Algorithms (Introduction)</a></p>



<p>One of the primary goals of the mathematical analysis of algorithms is to provide guidance about which algorithm is the “best” for solving a given computational problem. Worst-case analysis summarizes the performance profile of an algorithm by its worst performance on any input of a given size, implicitly advocating for the algorithm with the best-possible worst-case performance. Strong worst-case guarantees are the holy grail of algorithm design, providing an application-agnostic certification of an algorithm’s robustly good performance. However, for many fundamental problems and performance measures, such guarantees are impossible and a more nuanced analysis approach is called for. This paper surveys several alternatives to worst-case analysis.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="605" height="392" src="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul7.png" alt="" class="wp-image-24869" srcset="https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul7.png 605w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul7-300x194.png 300w, https://insidebigdata.com/wp-content/uploads/2020/08/arXiv_2020_Jul7-150x97.png 150w" sizes="(max-width: 605px) 100vw, 605px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-24975 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-analytics category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-big-data tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – August 2020</h1>
<div class="post-info"><span class="date published time" title="2020-09-10T06:00:00-07:00">September 10, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/09/10/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-august-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="224" height="189" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 224px) 100vw, 224px"></figure></div>







<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2008.01951v1.pdf" target="_blank">MusPy: A Toolkit for Symbolic Music Generation</a></p>



<p>This paper presents MusPy, an open source Python library for symbolic music generation. MusPy provides easy-to-use tools for essential components in a music generation system, including data set management, data I/O, data pre-processing and model evaluation. In order to showcase its potential, statistical analysis of the eleven data sets currently supported by MusPy is presented. Moreover, a cross-data set generalizability experiment is conducted by training an autoregressive model on each data set and measuring held-out likelihood on the others—a process which is made easier by MusPy’s data set management system. The results provide a map of domain overlap between various commonly used data sets and show that some data sets contain more representative cross-genre samples than others. Along with the data set analysis, these results might serve as a guide for choosing data sets in future research. Source code and documentation are available <a rel="noreferrer noopener" href="https://github.com/salu133445/muspy" target="_blank">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="505" height="304" src="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug1.png" alt="" class="wp-image-24976" srcset="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug1.png 505w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug1-300x181.png 300w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug1-150x90.png 150w" sizes="(max-width: 505px) 100vw, 505px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2008.06893v1.pdf" target="_blank">Context-aware Feature Generation for Zero-shot Semantic Segmentation</a></p>



<p>Existing semantic segmentation models heavily rely on dense pixel-wise annotations. To reduce the annotation pressure, this paper focuses on a challenging task named zero-shot semantic segmentation, which aims to segment unseen objects with zero annotations. This task can be accomplished by transferring knowledge across categories via semantic word embeddings. This paper proposes a novel context-aware feature generation method for zero-shot segmentation named CaGNet. In particular, with the observation that a pixel-wise feature highly depends on its contextual information, the technique inserts a contextual module in a segmentation network to capture the pixel-wise contextual information, which guides the process of generating more diverse and context-aware features from semantic word embeddings. The method achieves state-of-the-art results on three benchmark data sets for zero-shot segmentation. Code associated with the paper is available <a rel="noreferrer noopener" href="https://github.com/bcmi/CaGNet-Zero-Shot-Semantic-Segmentation" target="_blank">HERE</a>.  </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="316" src="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug2.png" alt="" class="wp-image-24977" srcset="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug2.png 700w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug2-300x135.png 300w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug2-150x68.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2008.12350v1.pdf" target="_blank">A Federated Approach for Fine-Grained Classification of Fashion Apparel</a></p>



<p>As online retail services proliferate and are pervasive in modern lives, applications for classifying fashion apparel features from image data are becoming more indispensable. Online retailers, from leading companies to start-ups, can leverage such applications in order to increase profit margin and enhance the consumer experience. Many notable schemes have been proposed to classify fashion items, however, the majority of which focused upon classifying basic-level categories, such as T-shirts, pants, skirts, shoes, bags, and so forth. In contrast to most prior efforts, this paper aims to enable an in-depth classification of fashion item attributes within the same category. Beginning with a single dress, the goal is to classify the type of dress hem, the hem length, and the sleeve length. The proposed scheme is comprised of three major stages: (a) localization of a target item from an input image using semantic segmentation, (b) detection of human key points (e.g., point of shoulder) using a pre-trained CNN and a bounding box, and (c) three phases to classify the attributes using a combination of algorithmic approaches and deep neural networks. The experimental results demonstrate that the proposed scheme is highly effective, with all categories having average precision of above 93.02 and outperforms existing Convolutional Neural Networks (CNNs)-based schemes.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="256" src="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug3.png" alt="" class="wp-image-24978" srcset="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug3.png 700w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug3-300x110.png 300w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug3-150x55.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2008.09305v1.pdf" target="_blank">ATG-PVD: Ticketing Parking Violations on A Drone</a></p>



<p>This paper introduces a novel suspect-and-investigate framework, which can be easily embedded in a drone for automated parking violation detection (PVD). The proposed framework consists of: 1) SwiftFlow, an efficient and accurate convolutional neural network (CNN) for unsupervised optical flow estimation; 2) Flow-RCNN, a flow-guided CNN for car detection and classification; and 3) an illegally parked car (IPC) candidate investigation module developed based on visual SLAM. The proposed framework was successfully embedded in a drone from ATG Robotics. The experimental results demonstrate that, firstly, the proposed SwiftFlow outperforms all other state-of-the-art unsupervised optical flow estimation approaches in terms of both speed and accuracy; secondly, IPC candidates can be effectively and efficiently detected by the proposed Flow-RCNN, with a better performance than our baseline network, Faster-RCNN; finally, the actual IPCs can be successfully verified by the investigation module after drone re-localization.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="699" height="662" src="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug4.png" alt="" class="wp-image-24979" srcset="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug4.png 699w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug4-300x284.png 300w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug4-150x142.png 150w" sizes="(max-width: 699px) 100vw, 699px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2008.09293v1.pdf" target="_blank">A Composable Specification Language for Reinforcement Learning Tasks</a></p>



<p>Reinforcement learning is a promising approach for learning control policies for robot tasks. However, specifying complex tasks (e.g., with multiple objectives and safety constraints) can be challenging, since the user must design a reward function that encodes the entire task. Furthermore, the user often needs to manually shape the reward to ensure convergence of the learning algorithm. This paper proposes a language for specifying complex control tasks, along with an algorithm that compiles specifications in our language into a reward function and automatically performs reward shaping. The approach is implemented in a tool called SPECTRL, which is shown to outperform several state-of-the-art baselines.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="706" height="234" src="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug5.png" alt="" class="wp-image-24980" srcset="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug5.png 706w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug5-300x99.png 300w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug5-150x50.png 150w" sizes="(max-width: 706px) 100vw, 706px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2008.10634v1.pdf" target="_blank">DiverseNet: When One Right Answer is not Enough</a></p>



<p>Many structured prediction tasks in machine vision have a collection of acceptable answers, instead of one definitive ground truth answer. Segmentation of images, for example, is subject to human labeling bias. Similarly, there are multiple possible pixel values that could plausibly complete occluded image regions. State-of-the art supervised learning methods are typically optimized to make a single test-time prediction for each query, failing to find other modes in the output space. Existing methods that allow for sampling often sacrifice speed or accuracy. This paper introduces a simple method for training a neural network, which enables diverse structured predictions to be made for each test-time query. For a single input, we learn to predict a range of possible answers. We compare favorably to methods that seek diversity through an ensemble of networks. Such stochastic multiple choice learning faces mode collapse, where one or more ensemble members fail to receive any training signal. The best performing solution can be deployed for various tasks, and just involves small modifications to the existing single-mode architecture, loss function, and training regime. The method results in quantitative improvements across three challenging tasks: 2D image completion, 3D volume estimation, and flow prediction.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="425" height="666" src="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug6.png" alt="" class="wp-image-24981" srcset="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug6.png 425w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug6-191x300.png 191w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug6-96x150.png 96w" sizes="(max-width: 425px) 100vw, 425px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2008.11104v1.pdf" target="_blank">Masked Face Recognition for Secure Authentication</a></p>



<p>With the recent world-wide COVID-19 pandemic, using face masks have become an important part of our lives. People are encouraged to cover their faces when in public area to avoid the spread of infection. The use of these face masks has raised a serious question on the accuracy of the facial recognition system used for tracking school/office attendance and to unlock phones. Many organizations use facial recognition as a means of authentication and have already developed the necessary datasets in-house to be able to deploy such a system. Unfortunately, masked faces make it difficult to be detected and recognized, thereby threatening to make the in-house datasets invalid and making such facial recognition systems inoperable. This paper addresses a methodology to use the current facial datasets by augmenting it with tools that enable masked faces to be recognized with low false-positive rates and high overall accuracy, without requiring the user data set to be recreated by taking new pictures for authentication. An open-source tool is presented, MaskTheFace to mask faces effectively creating a large data set of masked faces. </p>



<figure class="wp-block-image size-large"><img loading="lazy" width="700" height="261" src="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug7.png" alt="" class="wp-image-24982" srcset="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug7.png 700w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug7-300x112.png 300w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug7-150x56.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2008.10435v1.pdf" target="_blank">Periodic Stochastic Gradient Descent with Momentum for Decentralized Training</a></p>



<p>Decentralized training has been actively studied in recent years. Although a wide variety of methods have been proposed, yet the decentralized momentum SGD method is still underexplored. This paper proposes a novel periodic decentralized momentum SGD method, which employs the momentum schema and periodic communication for decentralized training. With these two strategies, as well as the topology of the decentralized training system, the theoretical convergence analysis of our proposed method is difficult. The research addresses this challenging problem and provides the condition under which our proposed method can achieve the linear speedup regarding the number of workers. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="747" height="303" src="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug8.png" alt="" class="wp-image-24983" srcset="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug8.png 747w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug8-300x122.png 300w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug8-150x61.png 150w" sizes="(max-width: 747px) 100vw, 747px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2008.10422v1.pdf" target="_blank">Adaptive Serverless Learning</a></p>



<p>With the emergence of distributed data, training machine learning models in the serverless manner has attracted increasing attention in recent years. Numerous training approaches have been proposed in this regime, such as decentralized SGD. However, all existing decentralized algorithms only focus on standard SGD. It might not be suitable for some applications, such as deep factorization machine in which the feature is highly sparse and categorical so that the adaptive training algorithm is needed. This paper proposes a novel adaptive decentralized training approach, which can compute the learning rate from data dynamically. To the best of the researcher’s knowledge, this is the first adaptive decentralized training approach. The theoretical results reveal that the proposed algorithm can achieve linear speedup with respect to the number of workers. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="749" height="245" src="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug9.png" alt="" class="wp-image-24984" srcset="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug9.png 749w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug9-300x98.png 300w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug9-150x49.png 150w" sizes="(max-width: 749px) 100vw, 749px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2008.11363v1.pdf" target="_blank">How Do the Hearts of Deep Fakes Beat? Deep Fake Source Detection via Interpreting Residuals with Biological Signals</a></p>



<p>Fake portrait video generation techniques have been posing a new threat to the society with photorealistic deep fakes for political propaganda, celebrity imitation, forged evidences, and other identity related manipulations. Following these generation techniques, some detection approaches have also been proved useful due to their high classification accuracy. Nevertheless, almost no effort was spent to track down the source of deep fakes. This paper proposes an approach not only to separate deep fakes from real videos, but also to discover the specific generative model behind a deep fake. Some pure deep learning based approaches try to classify deep fakes using CNNs where they actually learn the residuals of the generator. It is believed that these residuals contain more information and it can reveal these manipulation artifacts by disentangling them with biological signals. The key observation yields that the spatiotemporal patterns in biological signals can be conceived as a representative projection of residuals. To justify this observation, PPG cells are extracted from real and fake videos and fed to a state-of-the-art classification network for detecting the generative model per video. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="408" src="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug10.png" alt="" class="wp-image-24985" srcset="https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug10.png 700w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug10-300x175.png 300w, https://insidebigdata.com/wp-content/uploads/2020/09/arXiv_2020_Aug10-150x87.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-25131 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – September 2020</h1>
<div class="post-info"><span class="date published time" title="2020-10-22T06:00:00-07:00">October 22, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/10/22/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-september-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="247" height="209" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 247px) 100vw, 247px"></figure></div>







<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2009.11564.pdf" target="_blank">Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Equipping machines with comprehensive knowledge of the world’s entities and their relationships has been a long-standing goal of AI. Over the last decade, large-scale knowledge bases, also known as knowledge graphs, have been automatically constructed from web contents and text sources, and have become a key asset for search engines. This machine knowledge can be harnessed to semantically interpret textual phrases in news, social media and web tables, and contributes to question answering, natural language processing and data analytics. This 261 page paper surveys fundamental concepts and practical methods for creating and curating large knowledge bases. It covers models and methods for discovering and canonicalizing entities and their semantic types and organizing them into clean taxonomies. On top of this, the article discusses the automatic extraction of entity-centric properties. To support the long-term life-cycle and the quality assurance of machine knowledge, the article presents methods for constructing open schemas and for knowledge curation. Case studies on academic projects and industrial knowledge graphs complement the survey of concepts and methods.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="451" src="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep1.png" alt="" class="wp-image-25132" srcset="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep1.png 700w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep1-300x193.png 300w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep1-150x97.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2009.13664v1.pdf" target="_blank">Breaking the Memory Wall for AI Chip with a NewDimension</a></p>



<p>Recent advancements in deep learning have led to the widespread adoption of artificial intelligence (AI) in applications such as computer vision and natural language processing. As neural networks become deeper and larger, AI modeling demands outstrip the capabilities of conventional chip architectures. Memory bandwidth falls behind processing power. Energy consumption comes to dominate the total cost of ownership. Currently, memory capacity is insufficient to support the most advanced NLP models. This paper presents a 3D AI chip, called Sunrise, with near-memory computing architecture to address these three challenges. This distributed, near-memory computing architecture allows us to tear down the performance-limiting memory wall with an abundance of data bandwidth. The same level of energy efficiency is achieved on 40nm technology as competing chips on 7nm technology. By moving to similar technologies as other AI chips, we project to achieve more than ten times the energy efficiency, seven times the performance of the current state-of-the-art chips, and twenty times of memory capacity as compared with the best chip in each benchmark.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="495" height="315" src="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep2.png" alt="" class="wp-image-25133" srcset="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep2.png 495w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep2-300x191.png 300w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep2-150x95.png 150w" sizes="(max-width: 495px) 100vw, 495px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2009.12547v1.pdf" target="_blank">Causal Intervention for Weakly-Supervised Semantic Segmentation</a></p>



<p>This paperpresents a causal inference framework to improve Weakly-Supervised Semantic Segmentation (WSSS). Specifically, the aim is to generate better pixel-level pseudo-masks by using only image-level labels – the most crucial step in WSSS. The researchers attribute the cause of the ambiguous boundaries of pseudo-masks to the confounding context, e.g., the correct image-level classification of “horse” and “person” may be not only due to the recognition of each instance, but also their co-occurrence context, making the model inspection (e.g., CAM) hard to distinguish between the boundaries. Inspired by this, it is proposed a structural causal model to analyze the causalities among images, contexts, and class labels. Based on it, a new method was developed: Context Adjustment (CONTA), to remove the confounding bias in image-level classification and thus provide better pseudo-masks as ground-truth for the subsequent segmentation model. On PASCAL VOC 2012 and MS-COCO, it is shown that CONTA boosts various popular WSSS methods to new state-of-the-arts.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="537" height="283" src="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep3.png" alt="" class="wp-image-25134" srcset="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep3.png 537w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep3-300x158.png 300w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep3-150x79.png 150w" sizes="(max-width: 537px) 100vw, 537px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2009.12981v2.pdf" target="_blank">Parametric UMAP: learning embeddings with deep neural networks for representation and semi-supervised learning</a></p>



<p>This paper proposes Parametric UMAP, a parametric variation of the UMAP (Uniform Manifold Approximation and Projection) algorithm. UMAP is a non-parametric graph-based dimensionality reduction algorithm using applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data. The UMAP algorithm consists of two steps: (1) Compute a graphical representation of a data set (fuzzy simplicial complex), and (2) Through stochastic gradient descent, optimize a low-dimensional embedding of the graph. Here, the second step of UMAP is replaced with a deep neural network that learns a parametric relationship between data and embedding. It is demonstrated that the method performs similarly to its non-parametric counterpart while conferring the benefit of a learned parametric mapping (e.g. fast online embeddings for new data). It is  then shown that UMAP loss can be extended to arbitrary deep learning applications, for example constraining the latent distribution of autoencoders, and improving classifier accuracy for semi-supervised learning by capturing structure in unlabeled data. The code associated with the paper is available <a rel="noreferrer noopener" href="https://github.com/timsainb/ParametricUMAP_paper" target="_blank">HERE</a>.  </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="694" height="521" src="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep4.png" alt="" class="wp-image-25135" srcset="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep4.png 694w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep4-300x225.png 300w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep4-150x113.png 150w" sizes="(max-width: 694px) 100vw, 694px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2009.10639v1.pdf" target="_blank">What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors</a></p>



<p>EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the parts of the inputs deemed important to arrive a decision at a specific target. However, it remains challenging to quantify correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. This paper proposes backdoor trigger patterns–hidden malicious functionalities that cause misclassification–to automate the evaluation of saliency explanations. The key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. Three complementary metrics are introduced for systematic evaluation of explanations that an XAI method generates and evaluate seven state-of-the-art model-free and model-specific posthoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="459" height="294" src="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep5.png" alt="" class="wp-image-25136" srcset="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep5.png 459w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep5-300x192.png 300w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep5-150x96.png 150w" sizes="(max-width: 459px) 100vw, 459px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2009.10750v1.pdf" target="_blank">Lifelong Learning Dialogue Systems: Chatbots that Self-Learn On the Job</a></p>



<p>Dialogue systems, also called chatbots, are now used in a wide range of applications. However, they still have some major weaknesses. One key weakness is that they are typically trained from manually-labeled data and/or written with handcrafted rules, and their knowledge bases (KBs) are also compiled by human experts. Due to the huge amount of manual effort involved, they are difficult to scale and also tend to produce many errors ought to their limited ability to understand natural language and the limited knowledge in their KBs. Thus, the level of user satisfactory is often low. This paper proposes to dramatically improve this situation by endowing the system the ability to continually learn (1) new world knowledge, (2) new language expressions to ground them to actions, and (3) new conversational skills, during conversation or “on the job” by themselves so that as the systems chat more and more with users, they become more and more knowledgeable and are better and better able to understand diverse natural language expressions and improve their conversational skills. A key approach to achieving these is to exploit the multi-user environment of such systems to self-learn through interactions with users via verb and non-verb means. The paper discusses not only key challenges and promising directions to learn from users during conversation but also how to ensure the correctness of the learned knowledge.</p>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2009.11116v1.pdf" target="_blank">Phishing Detection Using Machine LearningTechniques</a></p>



<p>The Internet has become an indispensable part of our life, However, It also has provided opportunities to anonymously perform malicious activities like Phishing. Phishers try to deceive their victims by social engineering or creating mock-up websites to steal information such as account ID, username, password from individuals and organizations. Although many methods have been proposed to detect phishing websites, Phishers have evolved their methods to escape from these detection methods. One of the most successful methods for detecting these malicious activities is Machine Learning. This is because most Phishing attacks have some common characteristics which can be identified by machine learning methods. This paper compares the results of multiple machine learning methods for predicting phishing websites.</p>



<figure class="wp-block-image size-large"><img loading="lazy" width="464" height="460" src="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep7.png" alt="" class="wp-image-25137" srcset="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep7.png 464w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep7-300x297.png 300w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep7-150x150.png 150w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep7-110x110.png 110w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep7-50x50.png 50w" sizes="(max-width: 464px) 100vw, 464px"></figure>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2009.10292v1.pdf" target="_blank">PennSyn2Real: Training Object Recognition Models without Human Labeling</a></p>



<p>Scalability is a critical problem in generating training images for deep learning models. This paper proposes PennSyn2Real – a photo-realistic synthetic data set with more than 100, 000 4K images of more than 20 types of micro aerial vehicles (MAV) that can be used to generate an arbitrary number of training images for MAV detection and classification. The data generation framework bootstraps chroma-keying, a matured cinematography technique with a motion tracking system, providing artifact-free and curated annotated images where object orientations and lighting are controlled. This framework is easy to set up and can be applied to a broad range of objects, reducing the gap between synthetic and real-world data. The paper demonstrates that CNNs trained on the synthetic data have on par performance with those trained on real-world data in both semantic segmentation and object detection setups.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="438" height="316" src="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep8.png" alt="" class="wp-image-25138" srcset="https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep8.png 438w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep8-300x216.png 300w, https://insidebigdata.com/wp-content/uploads/2020/10/arXiv_2020_Sep8-150x108.png 150w" sizes="(max-width: 438px) 100vw, 438px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-25252 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – October 2020</h1>
<div class="post-info"><span class="date published time" title="2020-11-18T06:00:00-08:00">November 18, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/11/18/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-october-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="211" height="178" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 211px) 100vw, 211px"></figure></div>







<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2010.11929v1.pdf" target="_blank">An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. This paper shows that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. The PyTorch code associated with this paper can be found <a rel="noreferrer noopener" href="https://github.com/jiupinjia/SkyAR" target="_blank">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="445" src="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct1.png" alt="" class="wp-image-25254" srcset="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct1.png 700w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct1-300x191.png 300w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct1-150x95.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="http://mT5: A massively multilingual pre-trained text-to-text transformer" target="_blank">mT5: A massively multilingual pre-trained text-to-text transformer</a></p>



<p>The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. This paper introduces mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based data set covering 101 languages. Also described is the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. All of the TensorFlow code and model checkpoints used in this work are publicly available <a rel="noreferrer noopener" href="https://github.com/google-research/multilingual-t5" target="_blank">HERE</a>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="237" src="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct2.png" alt="" class="wp-image-25256" srcset="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct2.png 700w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct2-300x102.png 300w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct2-150x51.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2010.14925v1.pdf" target="_blank">MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis</a></p>



<p>This paper presents MedMNIST, a collection of 10 pre-processed medical open datasets. MedMNIST is standardized to perform classification tasks on lightweight 28×28 images, which requires no background knowledge. Covering the primary data modalities in medical image analysis, it is diverse on data scale (from 100 to 100,000) and tasks (binary/multi-class, ordinal regression and multi-label). MedMNIST could be used for educational purpose, rapid prototyping, multi-modal machine learning or AutoML in medical image analysis. Moreover, MedMNIST Classification Decathlon is designed to benchmark AutoML algorithms on all 10 datasets; The paper compares several baseline methods, including open-source or commercial AutoML tools. The data sets, evaluation PyTorch code and baseline methods for MedMNIST are publicly available <a rel="noreferrer noopener" href="https://medmnist.github.io/" target="_blank">HERE</a>.  </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="249" src="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct3.png" alt="" class="wp-image-25258" srcset="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct3.png 700w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct3-300x107.png 300w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct3-150x53.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2010.15703v1.pdf" target="_blank">Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks</a></p>



<p>Compressing large neural networks is an important step for their deployment in resource-constrained computational platforms. In this context, vector quantization is an appealing framework that expresses multiple parameters using a single code, and has recently achieved state-of-the-art network compression on a range of core vision and natural language processing tasks. Key to the success of vector quantization is deciding which parameter groups should be compressed together. Previous work has relied on heuristics that group the spatial dimension of individual convolutional filters, but a general solution remains unaddressed. This is desirable for pointwise convolutions (which dominate modern architectures), linear layers (which have no notion of spatial dimension), and convolutions (when more than one filter is compressed to the same codeword). This paper makes the observation that the weights of two adjacent layers can be permuted while expressing the same function. A connection is then established to rate-distortion theory and search for permutations that result in networks that are easier to compress. Finally, an annealed quantization algorithm is used to better compress the network and achieve higher final accuracy. Results are shown on image classification, object detection, and segmentation, reducing the gap with the uncompressed model by 40 to 70% with respect to the current state of the art. The PyTorch code associated with this paper is available <a rel="noreferrer noopener" href="https://github.com/uber-research/permute-quantize-finetune" target="_blank">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="186" src="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct4.png" alt="" class="wp-image-25260" srcset="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct4.png 700w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct4-300x80.png 300w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct4-150x40.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2010.14501v2.pdf" target="_blank">Memory Optimization for Deep Networks</a></p>



<p>Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by 32x over the last five years, the total available memory only grew by 2.5x. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. This paper presents MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by 3x for various PyTorch models, with a 9-16% overhead in computation. For the same computation cost, MONeT requires 1.2-1.8x less memory than current state-of-the-art automated checkpointing frameworks. The PyTorch code associated with this paper is available <a href="https://github.com/utsaslab/MONeT">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="692" height="314" src="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct5.png" alt="" class="wp-image-25262" srcset="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct5.png 692w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct5-300x136.png 300w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct5-150x68.png 150w" sizes="(max-width: 692px) 100vw, 692px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2010.15639v1.pdf" target="_blank">Teaching a GAN What Not to Learn</a></p>



<p>Generative adversarial networks (GANs) were originally envisioned as unsupervised generative models that learn to follow a target distribution. Variants such as conditional GANs, auxiliary-classifier GANs (ACGANs) project GANs on to supervised and semi-supervised learning frameworks by providing labelled data and using multi-class discriminators. This paper approaches the supervised GAN problem from a different perspective, one that is motivated by the philosophy of the famous Persian poet Rumi who said, “The art of knowing is knowing what to ignore.” </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="381" src="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct6.png" alt="" class="wp-image-25264" srcset="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct6.png 700w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct6-300x163.png 300w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct6-150x82.png 150w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a rel="noreferrer noopener" href="https://arxiv.org/pdf/2010.13880v1.pdf" target="_blank">Versatile Verification of Tree Ensembles</a></p>



<p>Machine learned models often must abide by certain requirements (e.g., fairness or legal). This has spurred interested in developing approaches that can provably verify whether a model satisfies certain properties. This paper introduces a generic algorithm called <strong>Veritas</strong> that enables tackling multiple different verification tasks for tree ensemble models like random forests (RFs) and gradient boosting <a href="https://deepai.org/machine-learning-glossary-and-terms/decision-tree">decision trees</a> (GBDTs). This generality contrasts with previous work, which has focused exclusively on either adversarial example generation or robustness checking. Veritas formulates the verification task as a generic optimization problem and introduces a novel search space representation. Veritas offers two key advantages. First, it provides anytime lower and upper bounds when the optimization problem cannot be solved exactly. In contrast, many existing methods have focused on exact solutions and are thus limited by the verification problem being NP-complete. Second, Veritas produces full (bounded suboptimal) solutions that can be used to generate concrete examples. Experimentally, Veritas is shown to outperform the previous state of the art by (a) generating exact solutions more frequently, (b) producing tighter bounds when (a) is not possible, and (c) offering orders of magnitude speed ups. Subsequently, Veritas enables tackling more and larger real-world verification scenarios.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="441" height="512" src="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct7.png" alt="" class="wp-image-25265" srcset="https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct7.png 441w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct7-258x300.png 258w, https://insidebigdata.com/wp-content/uploads/2020/11/arXiv_2020_Oct7-129x150.png 129w" sizes="(max-width: 441px) 100vw, 441px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-25405 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-big-data tag-data-science tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – November 2020</h1>
<div class="post-info"><span class="date published time" title="2020-12-23T06:00:00-08:00">December 23, 2020</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2020/12/23/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-november-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="210" height="177" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 210px) 100vw, 210px"></figure></div>







<p><a href="http://Scaled-YOLOv4: Scaling Cross Stage Partial Network" target="_blank" rel="noreferrer noopener">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></p>



<p>This paper shows that the YOLOv4 object detection neural network based on the CSP approach, scales both up and down and is applicable to small and large networks while maintaining optimal speed and accuracy. It is  proposed a network scaling approach that modifies not only the depth, width, resolution, but also structure of the network. YOLOv4-large model achieves state-of-the-art results: 55.4% AP (73.3% AP50) for the MS COCO data set at a speed of 15 FPS on Tesla V100, while with the test time augmentation, YOLOv4-large achieves 55.8% AP (73.2 AP50). To the best of our knowledge, this is currently the highest accuracy on the COCO data set among any published work. The YOLOv4-tiny model achieves 22.0% AP (42.0% AP50) at a speed of 443 FPS on RTX 2080Ti, while by using TensorRT, batch size = 4 and FP16-precision the YOLOv4-tiny achieves 1774 FPS. The cod associated with this paper can be found <a href="https://github.com/WongKinYiu/ScaledYOLOv4" target="_blank" rel="noreferrer noopener">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="490" height="494" src="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov1.png" alt="" class="wp-image-25409" srcset="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov1.png 490w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov1-150x150.png 150w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov1-298x300.png 298w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov1-110x110.png 110w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov1-50x50.png 50w" sizes="(max-width: 490px) 100vw, 490px"></figure></div>



<p><a href="https://arxiv.org/pdf/2011.07027v2.pdf" target="_blank" rel="noreferrer noopener">DeepMind Lab2D</a></p>



<p>This paper presents DeepMind Lab2D, a scalable environment simulator for artificial intelligence research that facilitates researcher-led experimentation with environment design. DeepMind Lab2D was built with the specific needs of multi-agent deep reinforcement learning researchers in mind, but it may also be useful beyond that particular subfield. The code associated with this paper can be found <a href="https://github.com/deepmind/lab2d" target="_blank" rel="noreferrer noopener">HERE</a>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="481" height="391" src="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov2.png" alt="" class="wp-image-25411" srcset="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov2.png 481w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov2-150x122.png 150w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov2-300x244.png 300w" sizes="(max-width: 481px) 100vw, 481px"></figure></div>



<p><a href="a PyTorch library and evaluation platform for end-to-end compression research" target="_blank" rel="noreferrer noopener">CompressAI: a PyTorch library and evaluation platform for end-to-end compression research</a></p>



<p>This paper presents CompressAI, a platform that provides custom operations, layers, models and tools to research, develop and evaluate end-to-end image and video compression codecs. In particular, CompressAI includes pre-trained models and evaluation tools to compare learned methods with traditional codecs. Multiple models from the state-of-the-art on learned end-to-end compression have thus been reimplemented in PyTorch and trained from scratch. The paper also reports objective comparison results using PSNR and MS-SSIM metrics vs. bit-rate, using the Kodak image data set as test set. Although this framework currently implements models for still-picture compression, it is intended to be soon extended to the video compression domain.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="721" height="225" src="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov3.png" alt="" class="wp-image-25413" srcset="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov3.png 721w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov3-150x47.png 150w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov3-300x94.png 300w" sizes="(max-width: 721px) 100vw, 721px"></figure></div>



<p><a href="https://arxiv.org/pdf/2011.13253v1.pdf" target="_blank" rel="noreferrer noopener">Two Stage Transformer Model for COVID-19 Fake News Detection andFact Checking</a></p>



<p>The rapid advancement of technology in online communication via social media platforms has led to a prolific rise in the spread of misinformation and fake news. Fake news is especially rampant in the current COVID-19 pandemic, leading to people believing in false and potentially harmful claims and stories. Detecting fake news quickly can alleviate the spread of panic, chaos and potential health hazards. This paper describes the development of a two stage automated pipeline for COVID-19 fake news detection using state of the art machine learning models for natural language processing. The first model leverages a novel fact checking algorithm that retrieves the most relevant facts concerning user claims about particular COVID-19 claims. The second model verifies the level of truth in the claim by computing the textual entailment between the claim and the true facts retrieved from a manually curated COVID-19 data set. The data set is based on a publicly available knowledge source consisting of more than 5000 COVID-19 false claims and verified explanations, a subset of which was internally annotated and cross-validated to train and evaluate our models. A series of models are developed based on classical text-based features to more contextual Transformer based models and observe that a model pipeline based on BERT and ALBERT for the two stages respectively yields the best results.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="789" height="442" src="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov4.png" alt="" class="wp-image-25414" srcset="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov4.png 789w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov4-150x84.png 150w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov4-300x168.png 300w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov4-768x430.png 768w" sizes="(max-width: 789px) 100vw, 789px"></figure></div>



<p><a href="https://arxiv.org/pdf/2011.12919v2.pdf" target="_blank" rel="noreferrer noopener">Analyzing the Machine Learning Conference Review Process</a></p>



<p>Mainstream machine learning conferences have seen a dramatic increase in the number of participants, along with a growing range of perspectives, in recent years. Members of the machine learning community are likely to overhear allegations ranging from randomness of acceptance decisions to institutional bias. This paper critically analyzes the review process through a comprehensive study of papers submitted to ICLR between 2017 and 2020. The paper quantifies reproducibility/randomness in review scores and acceptance decisions, and examine whether scores correlate with paper impact. The findings suggest strong institutional bias in accept/reject decisions, even after controlling for paper quality. Furthermore, evidence is found for a gender gap, with female authors receiving lower scores, lower acceptance rates, and fewer citations per paper than their male counterparts. The paper concludes with recommendations for future conference organizers.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="691" height="310" src="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov5.png" alt="" class="wp-image-25415" srcset="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov5.png 691w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov5-150x67.png 150w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov5-300x135.png 300w" sizes="(max-width: 691px) 100vw, 691px"></figure></div>



<p><a href="https://arxiv.org/pdf/2011.11819v1.pdf" target="_blank" rel="noreferrer noopener">When Machine Learning Meets Privacy: A Survey and Outlook</a></p>



<div class="wp-block-image"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>The newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This paper surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on in-depth analysis of the area of privacy and machine learning, the paper points out future research directions in this field.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="629" height="278" src="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov6.png" alt="" class="wp-image-25418" srcset="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov6.png 629w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov6-150x66.png 150w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov6-300x133.png 300w" sizes="(max-width: 629px) 100vw, 629px"></figure></div>



<p><a href="https://arxiv.org/pdf/2011.10829v1.pdf" target="_blank" rel="noreferrer noopener">On the Convergence of Reinforcement Learning</a></p>



<p>This paper considers the problem of Reinforcement Learning for nonlinear stochastic dynamical systems. It’s shown that in the RL setting, there is an inherent “Curse of Variance” in addition to Bellman’s infamous “Curse of Dimensionality”, in particular, it’s shown that the variance in the solution grows factorial-exponentially in the order of the approximation. A fundamental consequence is that this precludes the search for anything other than “local” feedback solutions in RL, in order to control the explosive variance growth, and thus, ensure accuracy. It’s further shown that the deterministic optimal control has a perturbation structure, in that the higher order terms do not affect the calculation of lower order terms, which can be utilized in RL to get accurate local solutions.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img loading="lazy" width="417" height="308" src="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov7.png" alt="" class="wp-image-25420" srcset="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov7.png 417w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov7-150x111.png 150w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Nov7-300x222.png 300w" sizes="(max-width: 417px) 100vw, 417px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>



<p>Join us on Twitter:&nbsp;@InsideBigData1 – <a href="https://twitter.com/search?q=insidebigdata1&amp;src=recent_search_click" target="_blank" rel="noreferrer noopener">https://twitter.com/InsideBigData1</a></p>
</div></div><!-- end .after-post-ad --></div><div class="post-25449 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – December 2020</h1>
<div class="post-info"><span class="date published time" title="2021-01-06T06:00:00-08:00">January 6, 2021</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2021/01/06/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-december-2020/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image is-style-default"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="208" height="176" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 208px) 100vw, 208px"></figure></div>







<p><a href="https://arxiv.org/pdf/2012.12474.pdf" target="_blank" rel="noreferrer noopener">Self-supervised self-supervision by combining deep learning and probabilistic logic</a></p>



<div class="wp-block-image is-style-default"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Labeling training examples at scale is a perennial challenge in machine learning. Self-supervision methods compensate for the lack of direct supervision by leveraging prior knowledge to automatically generate noisy labeled examples. Deep probabilistic logic (DPL) is a unifying framework for self-supervised learning that represents unknown labels as latent variables and incorporates diverse self-supervision using probabilistic logic to train a deep neural network end-to-end using variational EM. While DPL is successful at combining pre-specified self-supervision, manually crafting self-supervision to attain high accuracy may still be tedious and challenging. This paper proposes <em>Self-Supervised Self-Supervision (S4)</em>, which adds to DPL the capability to learn new self-supervision automatically. Starting from an initial “seed,” S4 iteratively uses the deep neural network to propose new self supervision. These are either added directly (a form of structured self-training) or verified by a human expert (as in feature-based active learning). Experiments show that S4 is able to automatically propose accurate self-supervision and can often nearly match the accuracy of supervised methods with a tiny fraction of the human effort.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="453" src="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Dec1.png" alt="" class="wp-image-25450" srcset="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Dec1.png 700w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Dec1-150x97.png 150w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Dec1-300x194.png 300w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="https://arxiv.org/pdf/2012.07805.pdf" target="_blank" rel="noreferrer noopener">Extracting Training Data from Large Language Models</a></p>



<p>It has become common to publish large (billion parameter) language models that have been trained on private data sets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. The team of researchers demonstrate an attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model’s training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. The team comprehensively evaluates the extraction attack to understand the factors that contribute to its success. For example, it’s found that larger models are more vulnerable than smaller models. The paper concludes by drawing lessons and discussing possible safeguards for training large language models.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="456" height="481" src="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Dec2.png" alt="" class="wp-image-25451" srcset="https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Dec2.png 456w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Dec2-142x150.png 142w, https://insidebigdata.com/wp-content/uploads/2020/12/arXiv_2020_Dec2-284x300.png 284w" sizes="(max-width: 456px) 100vw, 456px"></figure></div>



<p><a href="https://arxiv.org/pdf/2012.12516v1.pdf" target="_blank" rel="noreferrer noopener">Analyzing Representations inside Convolutional Neural Networks</a></p>



<p>How can we discover and succinctly summarize the concepts that a neural network has learned? Such a task is of great importance in applications of networks in areas of inference that involve classification, like medical diagnosis based on fMRI/x-ray etc. This paper proposes a framework to categorize the concepts a network learns based on the way it clusters a set of input examples, clusters neurons based on the examples they activate for, and input features all in the same latent space. This framework is unsupervised and can work without any labels for input features, it only needs access to internal activations of the network for each input example, thereby making it widely applicable. The proposed method is evaluated extensively and demonstrates that it produces human-understandable and coherent concepts that a ResNet-18 has learned on the CIFAR-100 data set.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="699" height="428" src="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec3.png" alt="" class="wp-image-25463" srcset="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec3.png 699w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec3-150x92.png 150w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec3-300x184.png 300w" sizes="(max-width: 699px) 100vw, 699px"></figure></div>



<p><a href="https://arxiv.org/pdf/2012.14271v2.pdf" target="_blank" rel="noreferrer noopener">Towards Fully Automated Manga Translation</a></p>



<p>This paper tackles the problem of machine translation of manga, Japanese comics. Manga translation involves two important problems in machine translation: context-aware and multimodal translation. Since text and images are mixed up in an unstructured fashion in Manga, obtaining context from the image is essential for manga translation. However, it is still an open problem how to extract context from image and integrate into MT models. In addition, corpus and benchmarks to train and evaluate such model is currently unavailable. This paper makes four contributions that establishes the foundation of manga translation research. </p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="550" height="503" src="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec4.png" alt="" class="wp-image-25465" srcset="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec4.png 550w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec4-150x137.png 150w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec4-300x274.png 300w" sizes="(max-width: 550px) 100vw, 550px"></figure></div>



<p><a href="https://arxiv.org/pdf/2012.13872v1.pdf" target="_blank" rel="noreferrer noopener">My Teacher Thinks The World Is Flat! Interpreting Automatic Essay Scoring Mechanism</a></p>



<p>Significant progress has been made in deep-learning based Automatic Essay Scoring (AES) systems in the past two decades. However, little research has been put to understand and interpret the black-box nature of these deep-learning based scoring models. Recent work shows that automated scoring systems are prone to even common-sense adversarial samples. Their lack of natural language understanding capability raises questions on the models being actively used by millions of candidates for life-changing decisions. With scoring being a highly multi-modal task, it becomes imperative for scoring models to be validated and tested on all these modalities. This paper utilizes recent advances in interpretability to find the extent to which features such as coherence, content and relevance are important for automated scoring mechanisms and why they are susceptible to adversarial samples. It’s found that the systems tested consider essays not as a piece of prose having the characteristics of natural flow of speech and grammatical structure, but as `word-soups’ where a few words are much more important than the other words. Removing the context surrounding those few important words causes the prose to lose the flow of speech and grammar, however has little impact on the predicted score. It’s also found that since the models are not semantically grounded with world-knowledge and common sense, adding false facts such as “the world is flat” actually increases the score instead of decreasing it.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="677" height="233" src="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec5.png" alt="" class="wp-image-25467" srcset="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec5.png 677w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec5-150x52.png 150w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec5-300x103.png 300w" sizes="(max-width: 677px) 100vw, 677px"></figure></div>



<p><a href="https://arxiv.org/abs/2012.14136v1" target="_blank" rel="noreferrer noopener">On Generating Extended Summaries of Long Documents</a></p>



<p>Prior work in document summarization has mainly focused on generating short summaries of a document. While this type of summary helps get a high-level view of a given document, it is desirable in some cases to know more detailed information about its salient points that can’t fit in a short summary. This is typically the case for longer documents such as a research paper, legal document, or a book. This paper presents a new method for generating extended summaries of long papers. The method exploits hierarchical structure of the documents and incorporates it into an extractive summarization model through a multi-task learning approach. The results are presented on three long summarization data sets, arXiv-Long, PubMed-Long, and Longsumm. The method outperforms or matches the performance of strong baselines. The data sets, and codes are publicly available <a href="https://github.com/Georgetown-IR-Lab/ExtendedSumm" target="_blank" rel="noreferrer noopener">HERE</a>. </p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="359" src="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec6.png" alt="" class="wp-image-25469" srcset="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec6.png 700w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec6-150x77.png 150w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec6-300x154.png 300w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="Everything you ever wanted to know about neural backdoors (but were afraid to ask)" target="_blank" rel="noreferrer noopener">TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask)</a></p>



<p>Neural backdoors represent one primary threat to the security of deep learning systems. The intensive research on this subject has produced a plethora of attacks/defenses, resulting in a constant arms race. However, due to the lack of evaluation benchmarks, many critical questions remain largely unexplored: (i) How effective, evasive, or transferable are different attacks? (ii) How robust, utility-preserving, or generic are different defenses? (iii) How do various factors (e.g., model architectures) impact their performance? (iv) What are the best practices (e.g., optimization strategies) to operate such attacks/defenses? (v) How can the existing attacks/defenses be further improved? To bridge the gap, this paper describes the design and implementation of TROJANZOO, the first open-source platform for evaluating neural backdoor attacks/defenses in a unified, holistic, and practical manner.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="523" height="302" src="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec7.png" alt="" class="wp-image-25470" srcset="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec7.png 523w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec7-150x87.png 150w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec7-300x173.png 300w" sizes="(max-width: 523px) 100vw, 523px"></figure></div>



<p><a href="https://arxiv.org/pdf/2012.12253v1.pdf" target="_blank" rel="noreferrer noopener">Improving Sample and Feature Selection with Principal Covariates Regression</a></p>



<p>Selecting the most relevant features and samples out of a large set of candidates is a task that occurs very often in the context of automated data analysis, where it can be used to improve the computational performance, and also often the transferability, of a model. This paper focuses on two popular sub-selection schemes which have been applied to this end: CUR decomposition, that is based on a low-rank approximation of the feature matrix and Farthest Point Sampling, that relies on the iterative identification of the most diverse samples and discriminating features. These unsupervised approaches are modified, incorporating a supervised component following the same spirit as the Principal Covariates Regression (PCovR) method. It’s shown that incorporating target information provides selections that perform better in supervised tasks, demonstrated with ridge regression, kernel ridge regression, and sparse kernel regression. It’s also shown that incorporating aspects of simple supervised learning models can improve the accuracy of more complex models, such as feed-forward neural networks. </p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="447" height="457" src="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec8.png" alt="" class="wp-image-25472" srcset="https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec8.png 447w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec8-147x150.png 147w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec8-293x300.png 293w, https://insidebigdata.com/wp-content/uploads/2021/01/arXiv_2020_Dec8-50x50.png 50w" sizes="(max-width: 447px) 100vw, 447px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>



<p>Join us on Twitter:&nbsp;@InsideBigData1 – <a href="https://twitter.com/search?q=insidebigdata1&amp;src=recent_search_click" target="_blank" rel="noreferrer noopener">https://twitter.com/InsideBigData1</a></p>
</div></div><!-- end .after-post-ad --></div><div class="post-25642 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wna entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – January 2021</h1>
<div class="post-info"><span class="date published time" title="2021-02-18T06:00:00-08:00">February 18, 2021</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2021/02/18/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-january-2021/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image is-style-default"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="207" height="175" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 207px) 100vw, 207px"></figure></div>







<p><a href="https://arxiv.org/pdf/2101.03961v1.pdf" target="_blank" rel="noreferrer noopener">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></p>



<p>In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model — with outrageous numbers of parameters — but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability — this paper addresses these with the <em>Switch Transformer</em>. The Google Brain researchers simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. The proposed training techniques help wrangle the instabilities and it is shown that large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. They design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings to measure gains over the mT5-Base version across all 101 languages. Finally, the paper advances the current scale of language models by pre-training up to trillion parameter models on the “Colossal Clean Crawled Corpus” and achieve a 4x speedup over the T5-XXL model. The PyTorch code associated with this paper can be found <a href="https://github.com/lab-ml/nn/tree/master/labml_nn/transformers/switch" target="_blank" rel="noreferrer noopener">HERE</a>. </p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="701" height="475" src="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan1.png" alt="" class="wp-image-25643" srcset="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan1.png 701w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan1-150x102.png 150w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan1-300x203.png 300w" sizes="(max-width: 701px) 100vw, 701px"></figure></div>



<p><a href="Making VGG-style ConvNets Great Again" target="_blank" rel="noreferrer noopener">RepVGG: Making VGG-style ConvNets Great Again</a></p>



<p>VGG-style ConvNets, although now considered a classic architecture, were attractive due to their simplicity. In contrast, ResNets have become popular due to their high accuracy but are more difficult to customize and display undesired inference drawbacks. To address these issues, Ding et al. propose RepVGG – the return of the VGG!&nbsp;</p>



<p>RepVGG is an efficient and simple architecture using plain VGG-style ConvNets. It decouples the inference-time and training-time architecture through a structural re-parameterization technique. The researchers report favorable speed-accuracy tradeoff&nbsp;compared to state-of-the-art models, such as&nbsp;EfficientNet&nbsp;and RegNet.&nbsp;RepVGG achieves 80% top-1 accuracy&nbsp;on&nbsp;ImageNet&nbsp;and is benchmarked as being&nbsp;83% faster than ResNet-50.&nbsp;This research is part of a broader effort to build more efficient models using simpler architectures and operations. The PyTorch code associated with this paper can be found <a href="https://github.com/DingXiaoH/RepVGG" target="_blank" rel="noreferrer noopener">HERE</a>.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="559" height="545" src="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan2.png" alt="" class="wp-image-25644" srcset="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan2.png 559w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan2-150x146.png 150w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan2-300x292.png 300w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan2-50x50.png 50w" sizes="(max-width: 559px) 100vw, 559px"></figure></div>



<p><a href="https://arxiv.org/pdf/2101.08543v1.pdf" target="_blank" rel="noreferrer noopener">Boost then Convolve: Gradient Boosting Meets Graph Neural Networks</a></p>



<div class="wp-block-image is-style-default"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>Graph neural networks (GNNs) are powerful models that have been successful in various graph representation learning tasks. Whereas gradient boosted decision trees (GBDT) often outperform other machine learning methods when faced with heterogeneous tabular data. But what approach should be used for graphs with tabular node features? Previous GNN models have mostly focused on networks with homogeneous sparse features and are suboptimal in the heterogeneous setting. This paper proposes a novel architecture that trains GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. The model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. With an extensive experimental comparison to the leading GBDT and GNN models, the researchers demonstrate a significant increase in performance on a variety of graphs with tabular features. The code associated with this paper can be found <a href="https://github.com/nd7141/bgnn" target="_blank" rel="noreferrer noopener">HERE</a>. </p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="657" height="381" src="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan3.png" alt="" class="wp-image-25646" srcset="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan3.png 657w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan3-150x87.png 150w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan3-300x174.png 300w" sizes="(max-width: 657px) 100vw, 657px"></figure></div>



<p><a href="https://arxiv.org/pdf/2101.03958v1.pdf" target="_blank" rel="noreferrer noopener">Evolving Reinforcement Learning Algorithms</a></p>



<p>This paper proposes a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. The method can both learn from scratch and bootstrap off known existing algorithms, like DQN, enabling interpretable modifications which improve performance. Learning from scratch on simple classical control and gridworld tasks, the method rediscovers the temporal-difference (TD) algorithm. Bootstrapped from DQN, two learned algorithms are highlighted which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that address overestimation in value-based methods.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="685" height="317" src="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan4.png" alt="" class="wp-image-25647" srcset="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan4.png 685w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan4-150x69.png 150w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan4-300x139.png 300w" sizes="(max-width: 685px) 100vw, 685px"></figure></div>



<p><a href="https://arxiv.org/pdf/2101.10710v1.pdf" target="_blank" rel="noreferrer noopener">Introducing and assessing the explainable AI (XAI)method: SIDU</a></p>



<p>Explainable Artificial Intelligence (XAI) has in recent years become a well-suited framework to generate human understandable explanations of black box models. This paper presents a novel XAI visual explanation algorithm denoted SIDU that can effectively localize entire object regions responsible for prediction in a full extend. The paper analyzes its robustness and effectiveness through various computational and human subject experiments. In particular, the SIDU algorithm is assessed using three different types of evaluations (Application, Human and Functionally-Grounded) to demonstrate its superior performance. The robustness of SIDU is further studied in presence of adversarial attack on black box models to better understand its performance.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="637" height="413" src="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan5.png" alt="" class="wp-image-25649" srcset="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan5.png 637w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan5-150x97.png 150w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan5-300x195.png 300w" sizes="(max-width: 637px) 100vw, 637px"></figure></div>



<p><a href="http://tf.data: A Machine Learning Data ProcessingFramework" target="_blank" rel="noreferrer noopener">tf.data: A Machine Learning Data Processing Framework</a></p>



<p>Training machine learning models requires feeding input data for models to ingest. Input pipelines for machine learning jobs are often challenging to implement efficiently as they require reading large volumes of data, applying complex transformations, and transferring data to hardware accelerators while overlapping computation and communication to achieve optimal performance. This paper presents tf.data, a framework for building and executing efficient input pipelines for machine learning jobs. The tf.data API provides operators which can be parameterized with user-defined computation, composed, and reused across different machine learning domains. These abstractions allow users to focus on the application logic of data processing, while tf.data’s runtime ensures that pipelines run efficiently. The paper demonstrates that input pipeline performance is critical to the end-to-end training time of state-of-the-art machine learning models. tf.data delivers the high performance required, while avoiding the need for manual tuning of performance knobs. </p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="495" height="338" src="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan6.png" alt="" class="wp-image-25651" srcset="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan6.png 495w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan6-150x102.png 150w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan6-300x205.png 300w" sizes="(max-width: 495px) 100vw, 495px"></figure></div>



<p><a href="https://arxiv.org/pdf/2101.10112v1.pdf" target="_blank" rel="noreferrer noopener">Fringe News Networks: Dynamics of US News Viewership following the 2020 Presidential Election</a></p>



<p>The growing political polarization of the American electorate over the last several decades has been widely studied and documented. During the administration of President Donald Trump, charges of “fake news” made social and news media not only the means but, to an unprecedented extent, the topic of political communication. Using data from before the November 3rd, 2020 US Presidential election, recent work has demonstrated the viability of using YouTube’s social media ecosystem to obtain insights into the extent of US political polarization as well as the relationship between this polarization and the nature of the content and commentary provided by different US news networks. With that work as background, this paper looks at the sharp transformation of the relationship between news consumers and here-to-fore “fringe” news media channels in the 64 days between the US presidential election and the violence that took place at US Capitol on January 6th. This paper makes two distinct types of contributions. The first is to introduce a novel methodology to analyze large social media data to study the dynamics of social political news networks and their viewers. The second is to provide insights into what actually happened regarding US political social media channels and their viewerships during this volatile 64 day period.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="198" src="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan7.png" alt="" class="wp-image-25652" srcset="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan7.png 700w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan7-150x42.png 150w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan7-300x85.png 300w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="https://arxiv.org/pdf/2101.10531v1.pdf" target="_blank" rel="noreferrer noopener">Deep Learning for Scene Classification: A Survey</a></p>



<p>Scene classification, aiming at classifying a scene image to one of the predefined scene categories by comprehending the entire image, is a longstanding, fundamental and challenging problem in computer vision. The rise of large-scale datasets, which constitute a dense sampling of diverse real-world scenes, and the renaissance of deep learning techniques, which learn powerful feature representations directly from big raw data, have been bringing remarkable progress in the field of scene representation and classification. To help researchers master needed advances in this field, the goal of this paper is to provide a comprehensive survey of recent achievements in scene classification using deep learning. More than 260 major publications are included in this survey covering different aspects of scene classification, including challenges, benchmark datasets, taxonomy, and quantitative performance comparisons of the reviewed methods. In retrospect of what has been achieved so far, this paper is concluded with a list of promising research opportunities.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="335" src="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan8.png" alt="" class="wp-image-25654" srcset="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan8.png 700w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan8-150x72.png 150w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan8-300x144.png 300w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="https://arxiv.org/pdf/2101.09429v1.pdf" target="_blank" rel="noreferrer noopener">Explainable Artificial Intelligence Approaches: A Survey</a></p>



<div class="wp-block-image is-style-default"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>The lack of explainability of a decision from an Artificial Intelligence (AI) based “black box” system/model, despite its superiority in many real-world applications, is a key stumbling block for adopting AI in many high stakes applications of different domain or industry. While many popular Explainable Artificial Intelligence (XAI) methods or approaches are available to facilitate a human-friendly explanation of the decision, each has its own merits and demerits, with a plethora of open challenges. This paper demonstrates popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or human-centered AI using XAI as a medium. Practitioners can use this work as a catalog to understand, compare, and correlate competitive advantages of popular XAI methods. In addition, this survey elicits future research directions towards responsible or human-centric AI systems, which is crucial to adopt AI in high stakes applications.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="477" height="472" src="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan9.png" alt="" class="wp-image-25656" srcset="https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan9.png 477w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan9-150x148.png 150w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan9-300x297.png 300w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan9-110x110.png 110w, https://insidebigdata.com/wp-content/uploads/2021/02/arXiv_2021_Jan9-50x50.png 50w" sizes="(max-width: 477px) 100vw, 477px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>



<p><em>Join us on Twitter:&nbsp;@InsideBigData1 – <a href="https://twitter.com/InsideBigData1" target="_blank" rel="noreferrer noopener">https://twitter.com/InsideBigData1</a></em></p>
</div></div><!-- end .after-post-ad --></div><div class="post-25789 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-deep-learning category-analytics category-big-data category-data-science-topics category-featured category-google-news-feed category-machine-learning category-news-analysis category-research-reports category-uncategorized tag-ai tag-artificial-intelligence tag-arxiv tag-data-science tag-deep-learning tag-machine-learning tag-wfnp entry"><h1 class="entry-title">Best of arXiv.org for AI, Machine Learning, and Deep Learning – February 2021</h1>
<div class="post-info"><span class="date published time" title="2021-03-17T06:00:00-07:00">March 17, 2021</span>  by <span class="author vcard"><span class="fn"><a href="https://insidebigdata.com/author/dangutierrez/" rel="author">Daniel Gutierrez</a></span></span> <span class="post-comments"><a href="https://insidebigdata.com/2021/03/17/best-of-arxiv-org-for-ai-machine-learning-and-deep-learning-february-2021/#respond">Leave a Comment</a></span> <div class="printfriendly"><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://insidebigdata.com/wp-content/themes/news-bigdata/images/print-icon.gif" alt="Print Friendly, PDF &amp; Email" style="display: inline; vertical-align:text-bottom; margin:0; padding:0; border:none; -webkit-box-shadow:none; -moz-box-shadow:none; box-shadow: none;"></a></div></div><div class="entry-content"><div class="pf-content">
<div class="wp-block-image is-style-default"><figure class="alignright size-large is-resized"><img loading="lazy" src="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg" alt="" class="wp-image-6361" width="213" height="180" srcset="https://insidebigdata.com/wp-content/uploads/2013/12/arxiv.jpg 450w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-150x127.jpg 150w, https://insidebigdata.com/wp-content/uploads/2013/12/arxiv-300x253.jpg 300w" sizes="(max-width: 213px) 100vw, 213px"></figure></div>







<p><a href="Two Transformers Can Make One Strong GAN" target="_blank" rel="noreferrer noopener">TransGAN: Two Transformers Can Make One Strong GAN</a></p>



<p>The recent explosive interest with transformers has suggested their potential to become powerful “universal” models for computer vision tasks, such as classification, detection, and segmentation. An important question is how much further transformers can go – are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, this paper conducts the first pilot study in building a GAN completely free of convolutions, using only pure transformer-based architectures. The proposed vanilla GAN architecture, dubbed TransGAN, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformer-based. We see TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Specifically, our the best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. The code associated with this paper is available <a href="https://github.com/VITA-Group/TransGAN" target="_blank" rel="noreferrer noopener">HERE</a>.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="334" src="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb1.png" alt="" class="wp-image-25790" srcset="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb1.png 700w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb1-150x72.png 150w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb1-300x143.png 300w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="https://arxiv.org/pdf/2102.11447v1.pdf" target="_blank" rel="noreferrer noopener">Data Engineering for Everyone</a></p>



<p>Data engineering is one of the fastest-growing fields within machine learning (ML). As ML becomes more common, the appetite for data grows more ravenous. But ML requires more data than individual teams of data engineers can readily produce, which presents a severe challenge to ML deployment at scale. Much like the software-engineering revolution, where mass adoption of open-source software replaced the closed, in-house development model for infrastructure code, there is a growing need to enable rapid development and open contribution to massive machine learning data sets. This paper shows that open-source data sets are the rocket fuel for research and innovation at even some of the largest AI organizations. The analysis of nearly 2000 research publications from Facebook, Google and Microsoft over the past five years shows the widespread use and adoption of open data sets. Open data sets that are easily accessible to the public are vital to accelerating ML innovation for everyone. But such open resources are scarce in the wild. So, what if we are able to accelerate data-set creation via automatic data set generation tools?</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="332" src="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb2.png" alt="" class="wp-image-25791" srcset="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb2.png 700w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb2-150x71.png 150w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb2-300x142.png 300w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="https://arxiv.org/ftp/arxiv/papers/2102/2102.11855.pdf" target="_blank" rel="noreferrer noopener">Deep Convolutional Neural Networks with Unitary Weights</a></p>



<p>While normalizations aim to fix the exploding and vanishing gradient problem in deep neural networks, they have drawbacks in speed or accuracy because of their dependency on the data set statistics. This paper is a comprehensive study of a novel method based on unitary synaptic weights derived from Lie Group to construct intrinsically stable neural systems. Here it’s shown that unitary convolutional neural networks deliver up to 32% faster inference speeds while maintaining competitive prediction accuracy. Unlike prior arts restricted to square synaptic weights, the paper expands the unitary networks to weights of any size and dimension.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="365" src="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb3.png" alt="" class="wp-image-25792" srcset="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb3.png 700w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb3-150x78.png 150w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb3-300x156.png 300w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="Quantized Personalization with Applications to Federated Learning" target="_blank" rel="noreferrer noopener">QuPeL: Quantized Personalization with Applications to Federated Learning</a></p>



<p>Traditionally, federated learning (FL) aims to train a single global model while collaboratively using multiple clients and a server. Two natural challenges that FL algorithms face are heterogeneity in data across clients and collaboration of clients with diverse resources. This paper introduces a quantized and personalized FL algorithm QuPeL that facilitates collective training with heterogeneous clients while respecting resource diversity. For personalization, clients are allowed to learn compressed personalized models with different quantization parameters depending on their resources. Towards this, an algorithm is proposed for learning quantized models through a relaxed optimization problem, where quantization values are also optimized over. When each client participating in the (federated) learning process has different requirements of the quantized model (both in value and precision), a quantized personalization framework is formulated by introducing a penalty term for local client objectives against a globally trained model to encourage collaboration. </p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="202" src="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb4.png" alt="" class="wp-image-25794" srcset="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb4.png 700w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb4-150x43.png 150w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb4-300x87.png 300w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="https://arxiv.org/pdf/2102.12736v1.pdf" target="_blank" rel="noreferrer noopener">Time-Series Imputation with Wasserstein Interpolation for Optimal Look-Ahead-Bias and Variance Tradeoff</a></p>



<p>Missing time-series data is a prevalent practical problem. Imputation methods in time-series data often are applied to the full panel data with the purpose of training a model for a downstream out-of-sample task. For example, in finance, imputation of missing returns may be applied prior to training a portfolio optimization model. Unfortunately, this practice may result in a look-ahead-bias in the future performance on the downstream task. There is an inherent trade-off between the look-ahead-bias of using the full data set for imputation and the larger variance in the imputation from using only the training data. By connecting layers of information revealed in time, this paper proposes a Bayesian posterior consensus distribution which optimally controls the variance and look-ahead-bias trade-off in the imputation. The benefit of the methodology both in synthetic and real financial data is demonstrated. </p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="706" height="375" src="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb5.png" alt="" class="wp-image-25796" srcset="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb5.png 706w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb5-150x80.png 150w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb5-300x159.png 300w" sizes="(max-width: 706px) 100vw, 706px"></figure></div>



<p><a href="https://arxiv.org/pdf/2102.12634v1.pdf" target="_blank" rel="noreferrer noopener">Automatic Story Generation: Challenges and Attempts</a></p>



<div class="wp-block-image is-style-default"><figure class="alignleft size-large"><img loading="lazy" width="100" height="100" src="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg" alt="" class="wp-image-22440" srcset="https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782.jpg 100w, https://insidebigdata.com/wp-content/uploads/2019/04/ThumbsUP_shutterstock_325452782-50x50.jpg 50w" sizes="(max-width: 100px) 100vw, 100px"></figure></div>



<p>The scope of this survey paper is to explore the challenges in automatic story generation. The goal is to contribute in the following ways: 1. Explore how previous research in story generation addressed those challenges. 2. Discuss future research directions and new technologies that may aid more advancements. 3. Shed light on emerging and often overlooked challenges such as creativity and discourse.</p>



<p><a href="https://arxiv.org/pdf/2102.12894v1.pdf" target="_blank" rel="noreferrer noopener">Constrained Optimization for Training Deep Neural Networks Under Class Imbalance</a></p>



<p>Deep neural networks (DNNs) are notorious for making more mistakes for the classes that have substantially fewer samples than the others during training. Such class imbalance is ubiquitous in clinical applications and very crucial to handle because the classes with fewer samples most often correspond to critical cases (e.g., cancer) where misclassifications can have severe consequences. Not to miss such cases, binary classifiers need to be operated at high True Positive Rates (TPR) by setting a higher threshold but this comes at the cost of very high False Positive Rates (FPR) for problems with class imbalance. Existing methods for learning under class imbalance most often do not take this into account. This paper argues that prediction accuracy should be improved by emphasizing reducing FPRs at high TPRs for problems where misclassification of the positive samples are associated with higher cost. To this end, it’s posed the training of a DNN for binary classification as a constrained optimization problem and introduce a novel constraint that can be used with existing loss functions to enforce maximal area under the ROC curve (AUC). The resulting constrained optimization problem is solved using an Augmented Lagrangian method (ALM), where the constraint emphasizes reduction of FPR at high TPR. Results demonstrate that the proposed method almost always improves the loss functions it is used with by attaining lower FPR at high TPR and higher or equal AUC.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="420" height="565" src="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb6.png" alt="" class="wp-image-25799" srcset="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb6.png 420w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb6-112x150.png 112w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb6-223x300.png 223w" sizes="(max-width: 420px) 100vw, 420px"></figure></div>



<p><a href="https://arxiv.org/pdf/2102.10788v1.pdf" target="_blank" rel="noreferrer noopener">Attention Models for Point Clouds in Deep Learning: A Survey</a></p>



<p>Recently, the advancement of 3D point clouds in deep learning has attracted intensive research in different application domains such as computer vision and robotic tasks. However, creating feature representation of robust, discriminative from unordered and irregular point clouds is challenging. The goal of this paper is to provide a comprehensive overview of the point clouds feature representation which uses attention models. More than 75+ key contributions in the recent three years are summarized in this survey, including the 3D objective detection, 3D semantic segmentation, 3D pose estimation, point clouds completion etc. Also provided are: a detailed characterization (1) the role of attention mechanisms, (2) the usability of attention models into different tasks, (3) the development trend of key technology.</p>



<p><a href="https://arxiv.org/pdf/2102.11107v1.pdf" target="_blank" rel="noreferrer noopener">Towards Causal Representation Learning</a></p>



<p>The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. This paper reviews fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, the paper delineates some implications of causality for machine learning and propose key research areas at the intersection of both communities.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="700" height="319" src="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb7.png" alt="" class="wp-image-25802" srcset="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb7.png 700w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb7-150x68.png 150w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb7-300x137.png 300w" sizes="(max-width: 700px) 100vw, 700px"></figure></div>



<p><a href="https://arxiv.org/pdf/2102.10423v1.pdf" target="_blank" rel="noreferrer noopener">An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks</a></p>



<p>Edge TPUs are a domain of accelerators for low-power, edge devices and are widely used in various Google products such as Coral and Pixel devices. This paper first discusses the major microarchitectural details of Edge TPUs. This is followed by an extensive evaluation of three classes of Edge TPUs, covering different computing ecosystems, that are either currently deployed in Google products or are the product pipeline, across 423K unique convolutional neural networks. Building upon this extensive study, the paper discusses critical and interpretable microarchitectural insights about the studied classes of Edge TPUs. Mainly discussed is how Edge TPU accelerators perform across convolutional neural networks with different structures. Finally, the paper presents ongoing efforts in developing high-accuracy learned machine learning models to estimate the major performance metrics of accelerators such as latency and energy consumption. These learned models enable significantly faster (in the order of milliseconds) evaluations of accelerators as an alternative to time-consuming cycle-accurate simulators and establish an exciting opportunity for rapid hard-ware/software co-design.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="572" height="353" src="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb8.png" alt="" class="wp-image-25804" srcset="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb8.png 572w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb8-150x93.png 150w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb8-300x185.png 300w" sizes="(max-width: 572px) 100vw, 572px"></figure></div>



<p><a href="https://arxiv.org/pdf/2102.09603v1.pdf" target="_blank" rel="noreferrer noopener">Improving DeepFake Detection Using Dynamic Face Augmentation</a></p>



<p>The creation of altered and manipulated faces has become more common due to the improvement of DeepFake generation methods. Simultaneously, we have seen detection models’ development for differentiating between a manipulated and original face from image or video content. We have observed that most publicly available DeepFake detection datasets have limited variations, where a single face is used in many videos, resulting in an oversampled training dataset. Due to this, deep neural networks tend to overfit to the facial features instead of learning to detect manipulation features of DeepFake content. As a result, most detection architectures perform poorly when tested on unseen data. This paper provides a quantitative analysis to investigate this problem and present a solution to prevent model overfitting due to the high volume of samples generated from a small number of actors.</p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large"><img loading="lazy" width="485" height="453" src="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb9.png" alt="" class="wp-image-25806" srcset="https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb9.png 485w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb9-150x140.png 150w, https://insidebigdata.com/wp-content/uploads/2021/03/arXiv_2021_Feb9-300x280.png 300w" sizes="(max-width: 485px) 100vw, 485px"></figure></div>



<p><em>Sign up for the free insideBIGDATA&nbsp;<a rel="noreferrer noopener" href="http://inside-bigdata.com/newsletter/" target="_blank">newsletter</a>.</em></p>



<p><em>Join us on Twitter:&nbsp;@InsideBigData1 – <a href="https://twitter.com/InsideBigData1" target="_blank" rel="noreferrer noopener">https://twitter.com/InsideBigData1</a></em></p>
</div></div><!-- end .after-post-ad --></div></body></html>
